#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# vocal_smart_splitter/core/quality_controller.py
# AI-SUMMARY: è´¨é‡æ§åˆ¶æ ¸å¿ƒæ¨¡å—ï¼Œç¡®ä¿åˆ†å‰²ç»“æœçš„è´¨é‡å’Œå®Œæ•´æ€§

import numpy as np
import logging
from typing import List, Tuple, Dict, Optional

from vocal_smart_splitter.utils.config_manager import get_config
from vocal_smart_splitter.utils.audio_processor import AudioProcessor
from vocal_smart_splitter.utils.adaptive_parameter_calculator import AdaptiveParameterCalculator

logger = logging.getLogger(__name__)

class QualityController:
    """BPMæ„ŸçŸ¥çš„è´¨é‡æ§åˆ¶å™¨ï¼Œç¡®ä¿åˆ†å‰²ç»“æœçš„è´¨é‡"""
    
    def __init__(self, sample_rate: int = 22050):
        """åˆå§‹åŒ–è´¨é‡æ§åˆ¶å™¨
        
        Args:
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        self.audio_processor = AudioProcessor(sample_rate)
        self.adaptive_calculator = AdaptiveParameterCalculator()
        
        # ä»é…ç½®åŠ è½½å‚æ•°
        self.validate_split_points = get_config('quality_control.validate_split_points', True)
        
        # åŠ¨æ€å‚æ•°ï¼ˆå°†è¢«BPMè‡ªé€‚åº”ç³»ç»Ÿè¦†ç›–ï¼‰
        self.current_adaptive_params = None
        self.bpm_info = None
        
        # ğŸ”„ ä»¥ä¸‹å‚æ•°å°†è¢«BPMè‡ªé€‚åº”ç³»ç»ŸåŠ¨æ€è¦†ç›–
        self.min_pause_at_split = get_config('quality_control.min_pause_at_split', 1.0)
        self.max_vocal_at_split = get_config('quality_control.max_vocal_at_split', 0.10)
        self.min_split_gap = get_config('quality_control.min_split_gap', 2.5)
        
        self.min_vocal_content_ratio = get_config('quality_control.min_vocal_content_ratio', 0.4)
        self.max_silence_ratio = get_config('quality_control.max_silence_ratio', 0.3)
        
        self.fade_in_duration = get_config('quality_control.fade_in_duration', 0.02)
        self.fade_out_duration = get_config('quality_control.fade_out_duration', 0.02)
        self.normalize_audio = get_config('quality_control.normalize_audio', True)
        
        self.remove_click_noise = get_config('quality_control.remove_click_noise', True)
        self.smooth_transitions = get_config('quality_control.smooth_transitions', True)
        
        logger.info("BPMæ„ŸçŸ¥è´¨é‡æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def apply_bpm_adaptive_parameters(self, bpm: float, complexity: float, 
                                     instrument_count: int) -> None:
        """åº”ç”¨BPMè‡ªé€‚åº”å‚æ•°
        
        Args:
            bpm: æ£€æµ‹åˆ°çš„BPMå€¼
            complexity: ç¼–æ›²å¤æ‚åº¦ (0-1)
            instrument_count: ä¹å™¨æ•°é‡
        """
        try:
            # è®¡ç®—è‡ªé€‚åº”å‚æ•°
            self.current_adaptive_params = self.adaptive_calculator.calculate_all_parameters(
                bpm=bpm, complexity=complexity, instrument_count=instrument_count
            )
            
            self.bpm_info = {
                'bpm': bpm,
                'complexity': complexity,
                'instrument_count': instrument_count,
                'category': self.current_adaptive_params.category,
                'compensation_factor': self.current_adaptive_params.compensation_factor
            }
            
            # åŠ¨æ€è¦†ç›–è´¨é‡æ§åˆ¶å‚æ•°
            self.min_pause_at_split = self.current_adaptive_params.min_pause_duration
            self.min_split_gap = self.current_adaptive_params.min_split_gap
            self.max_vocal_at_split = min(0.15, 0.10 + complexity * 0.05)  # åŸºäºå¤æ‚åº¦è°ƒæ•´
            
            logger.info(f"BPMè‡ªé€‚åº”å‚æ•°å·²åº”ç”¨: {self.bpm_info['category']}æ­Œæ›² "
                       f"BPM={float(bpm):.1f}, åœé¡¿è¦æ±‚={self.min_pause_at_split:.3f}s, "
                       f"åˆ†å‰²é—´éš™={self.min_split_gap:.3f}s")
                       
        except Exception as e:
            logger.error(f"åº”ç”¨BPMè‡ªé€‚åº”å‚æ•°å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å‚æ•°ç»§ç»­
            self.current_adaptive_params = None
    
    def get_current_quality_parameters(self) -> Dict:
        """è·å–å½“å‰è´¨é‡æ§åˆ¶å‚æ•°ä¿¡æ¯"""
        if self.current_adaptive_params:
            return {
                'min_pause_at_split': self.min_pause_at_split,
                'min_split_gap': self.min_split_gap,
                'max_vocal_at_split': self.max_vocal_at_split,
                'adaptive_mode': True,
                'bpm_info': self.bpm_info
            }
        else:
            return {
                'min_pause_at_split': self.min_pause_at_split,
                'min_split_gap': self.min_split_gap,
                'max_vocal_at_split': self.max_vocal_at_split,
                'adaptive_mode': False,
                'bpm_info': None
            }
    
    def validate_and_process_segments(self, audio: np.ndarray,
                                    vocal_track: np.ndarray,
                                    split_points: List[Dict]) -> List[Dict]:
        """éªŒè¯å’Œå¤„ç†éŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio: åŸå§‹éŸ³é¢‘
            vocal_track: äººå£°è½¨é“
            split_points: åˆ†å‰²ç‚¹åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„ç‰‡æ®µä¿¡æ¯åˆ—è¡¨
        """
        logger.info("å¼€å§‹éªŒè¯å’Œå¤„ç†éŸ³é¢‘ç‰‡æ®µ...")
        
        try:
            # 1. åˆ›å»ºéŸ³é¢‘ç‰‡æ®µ
            raw_segments = self._create_audio_segments(audio, split_points)
            
            # 2. éªŒè¯ç‰‡æ®µè´¨é‡
            validated_segments = self._validate_segments(raw_segments, vocal_track)
            
            # 3. å¤„ç†éŸ³é¢‘è´¨é‡
            processed_segments = self._process_audio_quality(validated_segments)
            
            # 4. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            quality_report = self._generate_quality_report(processed_segments, vocal_track)
            
            logger.info(f"ç‰‡æ®µå¤„ç†å®Œæˆï¼Œå…± {len(processed_segments)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")
            
            return {
                'segments': processed_segments,
                'quality_report': quality_report
            }
            
        except Exception as e:
            logger.error(f"ç‰‡æ®µéªŒè¯å’Œå¤„ç†å¤±è´¥: {e}")
            raise
    
    def _create_audio_segments(self, audio: np.ndarray, 
                             split_points: List[Dict]) -> List[Dict]:
        """åˆ›å»ºéŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio: åŸå§‹éŸ³é¢‘
            split_points: åˆ†å‰²ç‚¹åˆ—è¡¨
            
        Returns:
            éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨
        """
        segments = []
        audio_duration = len(audio) / self.sample_rate
        
        # æ·»åŠ å¼€å§‹å’Œç»“æŸæ—¶é—´ç‚¹
        time_points = [0.0] + [point['split_time'] for point in split_points] + [audio_duration]
        
        for i in range(len(time_points) - 1):
            start_time = time_points[i]
            end_time = time_points[i + 1]
            
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            
            # ç¡®ä¿ä¸è¶…å‡ºéŸ³é¢‘èŒƒå›´
            end_sample = min(end_sample, len(audio))
            
            if end_sample > start_sample:
                segment_audio = audio[start_sample:end_sample]
                
                segment_info = {
                    'index': i,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': end_time - start_time,
                    'start_sample': start_sample,
                    'end_sample': end_sample,
                    'audio_data': segment_audio,
                    'original_length': len(segment_audio)
                }
                
                segments.append(segment_info)
        
        logger.debug(f"åˆ›å»ºäº† {len(segments)} ä¸ªåŸå§‹ç‰‡æ®µ")
        return segments
    
    def _validate_segments(self, segments: List[Dict], 
                         vocal_track: np.ndarray) -> List[Dict]:
        """éªŒè¯ç‰‡æ®µè´¨é‡
        
        Args:
            segments: åŸå§‹ç‰‡æ®µåˆ—è¡¨
            vocal_track: äººå£°è½¨é“
            
        Returns:
            éªŒè¯é€šè¿‡çš„ç‰‡æ®µåˆ—è¡¨
        """
        validated_segments = []
        
        for segment in segments:
            validation_result = self._validate_single_segment(segment, vocal_track)
            
            if validation_result['is_valid']:
                segment.update(validation_result)
                validated_segments.append(segment)
            else:
                logger.debug(f"ç‰‡æ®µ {segment['index']} éªŒè¯å¤±è´¥: {validation_result['failure_reason']}")
        
        return validated_segments
    
    def _validate_single_segment(self, segment: Dict, 
                               vocal_track: np.ndarray) -> Dict:
        """éªŒè¯å•ä¸ªç‰‡æ®µ - BPMæ„ŸçŸ¥ç‰ˆæœ¬ï¼šåŸºäºéŸ³ä¹ç†è®ºçš„è´¨é‡è¯„ä¼°
        
        Args:
            segment: ç‰‡æ®µä¿¡æ¯
            vocal_track: äººå£°è½¨é“
            
        Returns:
            éªŒè¯ç»“æœ
        """
        audio_data = segment['audio_data']
        start_sample = segment['start_sample']
        end_sample = segment['end_sample']
        
        # æå–å¯¹åº”çš„äººå£°ç‰‡æ®µ
        vocal_segment = vocal_track[start_sample:min(end_sample, len(vocal_track))]
        
        validation_result = {
            'is_valid': True,
            'failure_reason': None,
            'quality_metrics': {},
            'musical_assessment': {}
        }
        
        # BPMæ„ŸçŸ¥éªŒè¯ç­–ç•¥ï¼šåŸºäºéŸ³ä¹ç†è®ºçš„è´¨é‡è¯„ä¼°
        
        # 1. æ£€æŸ¥ç‰‡æ®µé•¿åº¦ - BPMè‡ªé€‚åº”é•¿åº¦åˆ¤æ–­
        duration = segment['duration']
        
        # ä½¿ç”¨BPMè‡ªé€‚åº”é•¿åº¦æ ‡å‡†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.current_adaptive_params:
            # åŸºäºéŸ³ä¹èŠ‚æ‹çš„åˆç†é•¿åº¦èŒƒå›´
            beat_interval = self.current_adaptive_params.beat_interval
            min_beats = 8  # è‡³å°‘2ä¸ªä¹å¥ï¼ˆæ¯ä¹å¥4æ‹ï¼‰
            max_beats = 32 # æœ€å¤š8ä¸ªä¹å¥
            
            adaptive_min_len = min_beats * beat_interval
            adaptive_max_len = max_beats * beat_interval
            
            # éŸ³ä¹ç†è®ºéªŒè¯
            validation_result['musical_assessment']['expected_duration_range'] = (adaptive_min_len, adaptive_max_len)
            validation_result['musical_assessment']['beat_count'] = duration / beat_interval
            
            # åªåœ¨æç«¯æƒ…å†µä¸‹æ‹’ç»
            if duration < adaptive_min_len * 0.5:  # å°‘äº1ä¸ªä¹å¥
                validation_result['is_valid'] = False
                validation_result['failure_reason'] = f"éŸ³ä¹é•¿åº¦è¿‡çŸ­: {duration:.2f}s (å°‘äº{adaptive_min_len*0.5:.1f}sæœ€å°éŸ³ä¹å•ä½)"
                return validation_result
            if duration > adaptive_max_len * 2:  # è¶…è¿‡16ä¸ªä¹å¥
                validation_result['is_valid'] = False
                validation_result['failure_reason'] = f"éŸ³ä¹é•¿åº¦è¿‡é•¿: {duration:.2f}s (è¶…è¿‡{adaptive_max_len*2:.1f}sæœ€å¤§éŸ³ä¹å•ä½)"
                return validation_result
        else:
            # å›é€€åˆ°é…ç½®æ–‡ä»¶è®¾ç½®
            min_len = get_config('quality_control.min_segment_duration', None)
            max_len = get_config('quality_control.max_segment_duration', None)
            if min_len is not None and duration < float(min_len):
                validation_result['is_valid'] = False
                validation_result['failure_reason'] = f"ç‰‡æ®µè¿‡çŸ­: {duration:.2f}s (<{float(min_len):.2f}s)"
                return validation_result
            if max_len is not None and duration > float(max_len):
                validation_result['is_valid'] = False
                validation_result['failure_reason'] = f"ç‰‡æ®µè¿‡é•¿: {duration:.2f}s (>{float(max_len):.2f}s)"
                return validation_result
        
        # 2. æ£€æŸ¥éŸ³é¢‘æ•°æ®å®Œæ•´æ€§
        if len(audio_data) == 0:
            validation_result['is_valid'] = False
            validation_result['failure_reason'] = "éŸ³é¢‘æ•°æ®ä¸ºç©º"
            return validation_result
        
        # 3. è®¡ç®—è´¨é‡æŒ‡æ ‡ä½†ä¸ç”¨äºè¿‡æ»¤ - åªç”¨äºä¿¡æ¯è®°å½•
        vocal_content_ratio = self._calculate_vocal_content_ratio(vocal_segment)
        validation_result['quality_metrics']['vocal_content_ratio'] = vocal_content_ratio
        
        # 4. è®¡ç®—éŸ³é¢‘è´¨é‡æŒ‡æ ‡
        audio_quality_dict = self._calculate_audio_quality(audio_data)
        validation_result['quality_metrics'].update(audio_quality_dict)
        
        # è®¡ç®—ç»¼åˆéŸ³é¢‘è´¨é‡åˆ†æ•°
        audio_quality_score = (audio_quality_dict['rms_energy'] * 0.4 + 
                              min(audio_quality_dict['dynamic_range'] / 20.0, 1.0) * 0.3 +
                              audio_quality_dict['peak_level'] * 0.3)
        validation_result['quality_metrics']['audio_quality'] = audio_quality_score
        
        # 5. è®¡ç®—é™éŸ³æ¯”ä¾‹
        silence_ratio = self._calculate_silence_ratio(audio_data)
        validation_result['quality_metrics']['silence_ratio'] = silence_ratio
        
        # 6. è®¾ç½®åˆ†å‰²è´¨é‡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        validation_result['quality_metrics']['split_quality'] = 0.8  # å‡è®¾åˆ†å‰²è´¨é‡è‰¯å¥½
        
        # 7. è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•° - å¿…éœ€çš„ï¼Œä½†ä¸ç”¨äºè¿‡æ»¤
        validation_result['quality_metrics']['overall_quality'] = self._calculate_overall_quality(
            validation_result['quality_metrics']
        )
        
        # ğŸ†• åŸºäºéŸ³ä¹ç†è®ºçš„é¢å¤–è´¨é‡è¯„ä¼°
        if self.current_adaptive_params:
            musical_quality = self._assess_musical_quality(segment, validation_result['musical_assessment'])
            validation_result['quality_metrics']['musical_quality'] = musical_quality
        
        # è®°å½•ä½†ä¸è¿‡æ»¤ - è®©æ‰€æœ‰åœ¨è‡ªç„¶åœé¡¿å¤„çš„åˆ†å‰²éƒ½ä¿ç•™
        logger.debug(f"ç‰‡æ®µ {segment['index']}: {duration:.2f}s, äººå£°æ¯”ä¾‹: {vocal_content_ratio:.2f}")
        
        return validation_result
    
    def _assess_musical_quality(self, segment: Dict, musical_assessment: Dict) -> float:
        """åŸºäºéŸ³ä¹ç†è®ºè¯„ä¼°ç‰‡æ®µè´¨é‡
        
        Args:
            segment: ç‰‡æ®µä¿¡æ¯
            musical_assessment: éŸ³ä¹è¯„ä¼°æ•°æ®
            
        Returns:
            éŸ³ä¹è´¨é‡åˆ†æ•° (0-1)
        """
        if not self.current_adaptive_params:
            return 0.5  # æ— BPMä¿¡æ¯æ—¶çš„é»˜è®¤åˆ†æ•°
            
        quality_score = 0.0
        total_weight = 0.0
        
        # 1. èŠ‚æ‹å¯¹é½è´¨é‡ (30%)
        if 'beat_count' in musical_assessment:
            beat_count = musical_assessment['beat_count']
            # æ›´æ¥è¿‘æ•´æ•°æ‹æ•°çš„ç‰‡æ®µè´¨é‡æ›´é«˜
            beat_alignment_quality = 1.0 - (abs(beat_count - round(beat_count)) / 0.5)
            beat_alignment_quality = max(0.0, min(1.0, beat_alignment_quality))
            quality_score += beat_alignment_quality * 0.3
            total_weight += 0.3
        
        # 2. éŸ³ä¹é•¿åº¦åˆç†æ€§ (25%)
        if 'expected_duration_range' in musical_assessment:
            min_expected, max_expected = musical_assessment['expected_duration_range']
            duration = segment['duration']
            
            # åœ¨æœŸæœ›èŒƒå›´å†…çš„è´¨é‡æœ€é«˜
            if min_expected <= duration <= max_expected:
                duration_quality = 1.0
            else:
                # è¶…å‡ºèŒƒå›´çš„è´¨é‡é€’å‡
                if duration < min_expected:
                    duration_quality = duration / min_expected
                else:
                    duration_quality = max_expected / duration
                duration_quality = max(0.2, min(1.0, duration_quality))
            
            quality_score += duration_quality * 0.25
            total_weight += 0.25
        
        # 3. BPMç±»åˆ«é€‚åº”æ€§ (20%)
        category_quality = self._assess_category_adaptation(segment)
        quality_score += category_quality * 0.20
        total_weight += 0.20
        
        # 4. å¤æ‚åº¦è¡¥å¿æ•ˆæœ (15%)
        complexity_quality = self._assess_complexity_adaptation(segment)
        quality_score += complexity_quality * 0.15
        total_weight += 0.15
        
        # 5. åŸºç¡€éŸ³é¢‘è´¨é‡ (10%)
        if 'audio_quality' in segment.get('quality_metrics', {}):
            audio_quality = segment['quality_metrics']['audio_quality']
            quality_score += audio_quality * 0.10
            total_weight += 0.10
        
        return quality_score / total_weight if total_weight > 0 else 0.5
    
    def _assess_category_adaptation(self, segment: Dict) -> float:
        """è¯„ä¼°BPMç±»åˆ«é€‚åº”è´¨é‡"""
        if not self.bpm_info:
            return 0.5
            
        duration = segment['duration']
        category = self.bpm_info['category']
        bpm = self.bpm_info['bpm']
        
        # æ ¹æ®ä¸åŒç±»åˆ«çš„æœŸæœ›ç‰¹å¾è¯„ä¼°
        if category == 'slow':
            # æ…¢æ­Œï¼šæœŸæœ›è¾ƒé•¿çš„ç‰‡æ®µï¼Œå…è®¸è‡ªç„¶å‘¼å¸
            ideal_range = (8.0, 20.0)
        elif category == 'medium':
            # ä¸­é€Ÿï¼šæ ‡å‡†æµè¡Œæ­Œæ›²é•¿åº¦
            ideal_range = (6.0, 15.0)
        elif category == 'fast':
            # å¿«æ­Œï¼šè¾ƒçŸ­çš„ç‰‡æ®µï¼Œç´§å‡‘èŠ‚å¥
            ideal_range = (4.0, 12.0)
        else:  # very_fast
            # æå¿«ï¼šå¾ˆçŸ­çš„ç‰‡æ®µ
            ideal_range = (3.0, 8.0)
        
        if ideal_range[0] <= duration <= ideal_range[1]:
            return 1.0
        elif duration < ideal_range[0]:
            return max(0.3, duration / ideal_range[0])
        else:
            return max(0.3, ideal_range[1] / duration)
    
    def _assess_complexity_adaptation(self, segment: Dict) -> float:
        """è¯„ä¼°å¤æ‚åº¦é€‚åº”è´¨é‡"""
        if not self.bpm_info:
            return 0.5
            
        complexity = self.bpm_info['complexity']
        compensation_factor = self.bpm_info['compensation_factor']
        
        # å¤æ‚åº¦è¶Šé«˜ï¼Œè¡¥å¿å› å­åº”è¯¥è¶Šå¤§
        expected_compensation = 1.0 + complexity * 0.5
        compensation_accuracy = 1.0 - abs(compensation_factor - expected_compensation) / expected_compensation
        
        return max(0.2, compensation_accuracy)
    
    def validate_split_gaps(self, split_points: List[Dict]) -> List[Dict]:
        """éªŒè¯å’Œè°ƒæ•´åˆ†å‰²é—´éš™ - èŠ‚æ‹æ„ŸçŸ¥ç‰ˆæœ¬
        
        Args:
            split_points: åˆ†å‰²ç‚¹åˆ—è¡¨
            
        Returns:
            è°ƒæ•´åçš„åˆ†å‰²ç‚¹åˆ—è¡¨
        """
        if not split_points or not self.current_adaptive_params:
            return split_points
            
        validated_points = []
        beat_interval = self.current_adaptive_params.beat_interval
        min_gap = self.min_split_gap
        
        logger.info(f"å¼€å§‹èŠ‚æ‹æ„ŸçŸ¥åˆ†å‰²é—´éš™éªŒè¯ï¼Œæœ€å°é—´éš™: {min_gap:.3f}s")
        
        for i, point in enumerate(split_points):
            if i == 0:
                validated_points.append(point)
                continue
                
            prev_point = validated_points[-1]
            current_gap = point['split_time'] - prev_point['split_time']
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°é—´éš™è¦æ±‚
            if current_gap < min_gap:
                # å°è¯•èŠ‚æ‹å¯¹é½è°ƒæ•´
                adjusted_time = self._align_to_beat(
                    prev_point['split_time'] + min_gap, beat_interval
                )
                
                # å¦‚æœè°ƒæ•´åçš„æ—¶é—´åˆç†ï¼Œåˆ™ä½¿ç”¨è°ƒæ•´åçš„æ—¶é—´
                if adjusted_time < point['split_time'] + beat_interval:
                    point_copy = point.copy()
                    point_copy['split_time'] = adjusted_time
                    point_copy['adjustment_reason'] = f"èŠ‚æ‹å¯¹é½é—´éš™è°ƒæ•´: {current_gap:.3f}s -> {min_gap:.3f}s"
                    validated_points.append(point_copy)
                    logger.debug(f"è°ƒæ•´åˆ†å‰²ç‚¹ {i}: {point['split_time']:.3f}s -> {adjusted_time:.3f}s")
                else:
                    # è·³è¿‡æ­¤åˆ†å‰²ç‚¹
                    logger.debug(f"è·³è¿‡è¿‡è¿‘çš„åˆ†å‰²ç‚¹ {i}: é—´éš™ {current_gap:.3f}s < {min_gap:.3f}s")
                    continue
            else:
                validated_points.append(point)
        
        logger.info(f"åˆ†å‰²é—´éš™éªŒè¯å®Œæˆ: {len(split_points)} -> {len(validated_points)} ä¸ªåˆ†å‰²ç‚¹")
        return validated_points
    
    def _align_to_beat(self, time: float, beat_interval: float) -> float:
        """å°†æ—¶é—´å¯¹é½åˆ°æœ€è¿‘çš„èŠ‚æ‹"""
        beat_position = time / beat_interval
        aligned_beat = round(beat_position)
        return aligned_beat * beat_interval
    
    def _calculate_audio_quality(self, audio_data: np.ndarray) -> Dict:
        """è®¡ç®—éŸ³é¢‘è´¨é‡æŒ‡æ ‡ - ä»…ç”¨äºè®°å½•ï¼Œä¸ç”¨äºè¿‡æ»¤
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            
        Returns:
            éŸ³é¢‘è´¨é‡æŒ‡æ ‡å­—å…¸
        """
        if len(audio_data) == 0:
            return {'rms_energy': 0.0, 'dynamic_range': 0.0, 'peak_level': 0.0}
        
        # è®¡ç®—åŸºæœ¬éŸ³é¢‘è´¨é‡æŒ‡æ ‡
        rms_energy = np.sqrt(np.mean(audio_data ** 2))
        peak_level = np.max(np.abs(audio_data))
        
        # è®¡ç®—åŠ¨æ€èŒƒå›´
        if peak_level > 0:
            dynamic_range = 20 * np.log10(peak_level / (rms_energy + 1e-10))
        else:
            dynamic_range = 0.0
        
        return {
            'rms_energy': float(rms_energy),
            'dynamic_range': float(dynamic_range),
            'peak_level': float(peak_level)
        }
    
    def _calculate_vocal_content_ratio(self, vocal_segment: np.ndarray) -> float:
        """è®¡ç®—äººå£°å†…å®¹æ¯”ä¾‹
        
        Args:
            vocal_segment: äººå£°ç‰‡æ®µ
            
        Returns:
            äººå£°å†…å®¹æ¯”ä¾‹
        """
        if len(vocal_segment) == 0:
            return 0.0
        
        # è®¡ç®—RMSèƒ½é‡
        rms_energy = np.sqrt(np.mean(vocal_segment ** 2))
        
        # è®¡ç®—æœ‰æ•ˆäººå£°çš„æ¯”ä¾‹
        window_size = int(0.1 * self.sample_rate)  # 100msçª—å£
        hop_size = window_size // 2
        
        vocal_frames = 0
        total_frames = 0
        
        for i in range(0, len(vocal_segment) - window_size, hop_size):
            window = vocal_segment[i:i + window_size]
            window_rms = np.sqrt(np.mean(window ** 2))
            
            # å¦‚æœçª—å£èƒ½é‡è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæœ‰äººå£°
            if window_rms > rms_energy * 0.1:
                vocal_frames += 1
            
            total_frames += 1
        
        return vocal_frames / total_frames if total_frames > 0 else 0.0
    
    def _calculate_silence_ratio(self, audio_data: np.ndarray) -> float:
        """è®¡ç®—é™éŸ³æ¯”ä¾‹
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            
        Returns:
            é™éŸ³æ¯”ä¾‹
        """
        if len(audio_data) == 0:
            return 1.0
        
        # è®¡ç®—æ•´ä½“RMS
        overall_rms = np.sqrt(np.mean(audio_data ** 2))
        silence_threshold = overall_rms * 0.05  # 5%çš„æ•´ä½“RMSä½œä¸ºé™éŸ³é˜ˆå€¼
        
        # è®¡ç®—é™éŸ³æ ·æœ¬æ•°
        silence_samples = np.sum(np.abs(audio_data) < silence_threshold)
        
        return silence_samples / len(audio_data)
    
    def _assess_audio_quality(self, audio_data: np.ndarray) -> float:
        """è¯„ä¼°éŸ³é¢‘è´¨é‡
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            
        Returns:
            éŸ³é¢‘è´¨é‡åˆ†æ•°
        """
        if len(audio_data) == 0:
            return 0.0
        
        quality_score = 0.0
        
        # 1. åŠ¨æ€èŒƒå›´
        dynamic_range = np.max(audio_data) - np.min(audio_data)
        if dynamic_range > 0.1:
            quality_score += 0.3
        
        # 2. ä¿¡å™ªæ¯”ä¼°è®¡
        rms = np.sqrt(np.mean(audio_data ** 2))
        noise_floor = np.percentile(np.abs(audio_data), 10)
        snr_estimate = rms / (noise_floor + 1e-8)
        
        if snr_estimate > 10:
            quality_score += 0.4
        elif snr_estimate > 5:
            quality_score += 0.2
        
        # 3. é¢‘è°±å®Œæ•´æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        if len(audio_data) > 1024:
            fft = np.fft.fft(audio_data[:1024])
            spectrum = np.abs(fft)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„é¢‘è°±å†…å®¹
            if np.sum(spectrum) > 0:
                spectral_centroid = np.sum(spectrum * np.arange(len(spectrum))) / np.sum(spectrum)
                if 50 < spectral_centroid < 400:  # åˆç†çš„é¢‘è°±è´¨å¿ƒ
                    quality_score += 0.3
        
        return min(quality_score, 1.0)
    
    def _validate_split_boundaries(self, segment: Dict, 
                                 vocal_track: np.ndarray) -> float:
        """éªŒè¯åˆ†å‰²è¾¹ç•Œè´¨é‡
        
        Args:
            segment: ç‰‡æ®µä¿¡æ¯
            vocal_track: äººå£°è½¨é“
            
        Returns:
            è¾¹ç•Œè´¨é‡åˆ†æ•°
        """
        start_sample = segment['start_sample']
        end_sample = segment['end_sample']
        
        boundary_quality = 0.0
        
        # æ£€æŸ¥å¼€å§‹è¾¹ç•Œ
        start_quality = self._check_boundary_quality(
            vocal_track, start_sample, 'start'
        )
        boundary_quality += start_quality * 0.5
        
        # æ£€æŸ¥ç»“æŸè¾¹ç•Œ
        end_quality = self._check_boundary_quality(
            vocal_track, end_sample, 'end'
        )
        boundary_quality += end_quality * 0.5
        
        return boundary_quality
    
    def _check_boundary_quality(self, vocal_track: np.ndarray, 
                              boundary_sample: int, 
                              boundary_type: str) -> float:
        """æ£€æŸ¥å•ä¸ªè¾¹ç•Œçš„è´¨é‡
        
        Args:
            vocal_track: äººå£°è½¨é“
            boundary_sample: è¾¹ç•Œæ ·æœ¬ä½ç½®
            boundary_type: è¾¹ç•Œç±»å‹ ('start' æˆ– 'end')
            
        Returns:
            è¾¹ç•Œè´¨é‡åˆ†æ•°
        """
        check_window = int(self.min_pause_at_split * self.sample_rate)
        
        if boundary_type == 'start':
            # æ£€æŸ¥å¼€å§‹å‰çš„é™éŸ³
            start_idx = max(0, boundary_sample - check_window)
            end_idx = boundary_sample
        else:
            # æ£€æŸ¥ç»“æŸåçš„é™éŸ³
            start_idx = boundary_sample
            end_idx = min(len(vocal_track), boundary_sample + check_window)
        
        if start_idx >= end_idx:
            return 0.5  # è¾¹ç•Œæƒ…å†µ
        
        boundary_region = vocal_track[start_idx:end_idx]
        
        # è®¡ç®—è¾¹ç•ŒåŒºåŸŸçš„èƒ½é‡
        region_energy = np.sum(boundary_region ** 2) / len(boundary_region)
        total_energy = np.sum(vocal_track ** 2) / len(vocal_track)
        
        energy_ratio = region_energy / (total_energy + 1e-8)
        
        # è¾¹ç•ŒåŒºåŸŸåº”è¯¥ç›¸å¯¹å®‰é™
        if energy_ratio < self.max_vocal_at_split:
            return 1.0
        elif energy_ratio < self.max_vocal_at_split * 2:
            return 0.7
        else:
            return 0.3
    
    def _calculate_overall_quality(self, quality_metrics: Dict) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        
        Args:
            quality_metrics: è´¨é‡æŒ‡æ ‡å­—å…¸
            
        Returns:
            ç»¼åˆè´¨é‡åˆ†æ•°
        """
        weights = {
            'vocal_content_ratio': 0.3,
            'audio_quality': 0.3,
            'split_quality': 0.2,
            'silence_ratio': 0.2  # é™éŸ³æ¯”ä¾‹è¶Šä½è¶Šå¥½
        }
        
        weighted_score = 0.0
        total_weight = 0.0
        
        for metric, weight in weights.items():
            if metric in quality_metrics:
                value = quality_metrics[metric]
                
                # é™éŸ³æ¯”ä¾‹éœ€è¦åè½¬ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                if metric == 'silence_ratio':
                    value = 1.0 - value
                
                weighted_score += value * weight
                total_weight += weight
        
        return weighted_score / total_weight if total_weight > 0 else 0.0
    
    def _process_audio_quality(self, segments: List[Dict]) -> List[Dict]:
        """å¤„ç†éŸ³é¢‘è´¨é‡
        
        Args:
            segments: éªŒè¯é€šè¿‡çš„ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„ç‰‡æ®µåˆ—è¡¨
        """
        processed_segments = []
        
        for segment in segments:
            audio_data = segment['audio_data'].copy()
            
            # 1. å»é™¤å’”å—’å£°
            if self.remove_click_noise:
                audio_data = self._remove_click_noise(audio_data)
            
            # 2. åº”ç”¨æ¸å…¥æ¸å‡º
            audio_data = self.audio_processor._apply_fades(
                audio_data, self.sample_rate, 
                self.fade_in_duration, self.fade_out_duration
            )
            
            # 3. æ ‡å‡†åŒ–éŸ³é¢‘
            if self.normalize_audio:
                audio_data = self.audio_processor._normalize_audio(audio_data)
            
            # 4. å¹³æ»‘è¿‡æ¸¡
            if self.smooth_transitions:
                audio_data = self._smooth_transitions(audio_data)
            
            # æ›´æ–°ç‰‡æ®µä¿¡æ¯
            segment['processed_audio'] = audio_data
            segment['processing_applied'] = {
                'click_removal': self.remove_click_noise,
                'fade_applied': True,
                'normalized': self.normalize_audio,
                'smoothed': self.smooth_transitions
            }
            
            processed_segments.append(segment)
        
        return processed_segments
    
    def _remove_click_noise(self, audio_data: np.ndarray) -> np.ndarray:
        """å»é™¤å’”å—’å£°
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            
        Returns:
            å¤„ç†åçš„éŸ³é¢‘
        """
        try:
            # ç®€å•çš„å’”å—’å£°æ£€æµ‹å’Œç§»é™¤
            # æ£€æµ‹çªç„¶çš„å¹…åº¦å˜åŒ–
            diff = np.diff(audio_data)
            threshold = np.std(diff) * 3
            
            click_indices = np.where(np.abs(diff) > threshold)[0]
            
            # å¯¹æ£€æµ‹åˆ°çš„å’”å—’å£°è¿›è¡Œå¹³æ»‘å¤„ç†
            processed_audio = audio_data.copy()
            
            for idx in click_indices:
                if 0 < idx < len(processed_audio) - 1:
                    # ç”¨ç›¸é‚»æ ·æœ¬çš„å¹³å‡å€¼æ›¿æ¢
                    processed_audio[idx] = (processed_audio[idx-1] + processed_audio[idx+1]) / 2
            
            return processed_audio
            
        except Exception as e:
            logger.warning(f"å’”å—’å£°å»é™¤å¤±è´¥: {e}")
            return audio_data
    
    def _smooth_transitions(self, audio_data: np.ndarray) -> np.ndarray:
        """å¹³æ»‘è¿‡æ¸¡
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            
        Returns:
            å¹³æ»‘åçš„éŸ³é¢‘
        """
        try:
            # å¯¹éŸ³é¢‘å¼€å¤´å’Œç»“å°¾è¿›è¡Œé¢å¤–çš„å¹³æ»‘å¤„ç†
            smooth_samples = int(0.01 * self.sample_rate)  # 10ms
            
            if len(audio_data) > smooth_samples * 2:
                # å¼€å¤´å¹³æ»‘
                for i in range(smooth_samples):
                    weight = i / smooth_samples
                    audio_data[i] *= weight
                
                # ç»“å°¾å¹³æ»‘
                for i in range(smooth_samples):
                    weight = (smooth_samples - i) / smooth_samples
                    audio_data[-(i+1)] *= weight
            
            return audio_data
            
        except Exception as e:
            logger.warning(f"è¿‡æ¸¡å¹³æ»‘å¤±è´¥: {e}")
            return audio_data
    
    def _generate_quality_report(self, segments: List[Dict], 
                               vocal_track: np.ndarray) -> Dict:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        
        Args:
            segments: å¤„ç†åçš„ç‰‡æ®µåˆ—è¡¨
            vocal_track: äººå£°è½¨é“
            
        Returns:
            è´¨é‡æŠ¥å‘Š
        """
        if not segments:
            return {
                'overall_quality': 0.0,
                'segment_count': 0,
                'avg_duration': 0.0,
                'quality_distribution': {},
                'issues': ['æ²¡æœ‰æœ‰æ•ˆç‰‡æ®µ']
            }
        
        # ç»Ÿè®¡ä¿¡æ¯
        durations = [seg['duration'] for seg in segments]
        quality_scores = [seg['quality_metrics']['overall_quality'] for seg in segments]
        
        # è´¨é‡åˆ†å¸ƒ
        quality_distribution = {
            'excellent': sum(1 for q in quality_scores if q >= 0.8),
            'good': sum(1 for q in quality_scores if 0.6 <= q < 0.8),
            'fair': sum(1 for q in quality_scores if 0.4 <= q < 0.6),
            'poor': sum(1 for q in quality_scores if q < 0.4)
        }
        
        # é—®é¢˜æ£€æµ‹
        issues = []
        if np.mean(durations) < 7:
            issues.append("å¹³å‡ç‰‡æ®µé•¿åº¦åçŸ­")
        if np.mean(durations) > 13:
            issues.append("å¹³å‡ç‰‡æ®µé•¿åº¦åé•¿")
        if np.mean(quality_scores) < 0.6:
            issues.append("æ•´ä½“è´¨é‡åä½")
        if len(segments) < 3:
            issues.append("ç‰‡æ®µæ•°é‡è¿‡å°‘")
        if len(segments) > 20:
            issues.append("ç‰‡æ®µæ•°é‡è¿‡å¤š")
        
        return {
            'overall_quality': np.mean(quality_scores),
            'segment_count': len(segments),
            'avg_duration': np.mean(durations),
            'duration_std': np.std(durations),
            'quality_distribution': quality_distribution,
            'min_quality': np.min(quality_scores),
            'max_quality': np.max(quality_scores),
            'issues': issues if issues else ['æ— æ˜æ˜¾é—®é¢˜']
        }
