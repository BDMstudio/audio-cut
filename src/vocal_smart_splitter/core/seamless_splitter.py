#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# vocal_smart_splitter/core/seamless_splitter.py

import os
import numpy as np
import librosa
import logging
from typing import List, Dict, Optional
from pathlib import Path
import time
import tempfile
import soundfile as sf

from ..utils.config_manager import get_config
from ..utils.audio_processor import AudioProcessor
from .vocal_pause_detector import VocalPauseDetectorV2
from .quality_controller import QualityController
from .enhanced_vocal_separator import EnhancedVocalSeparator

logger = logging.getLogger(__name__)

class SeamlessSplitter:
    """
    [v2.3 ç»Ÿä¸€æŒ‡æŒ¥ä¸­å¿ƒ]
    æ— ç¼åˆ†å‰²å™¨ - è´Ÿè´£ç¼–æ’æ‰€æœ‰åˆ†å‰²æ¨¡å¼çš„å”¯ä¸€å¼•æ“ã€‚
    """
    
    def __init__(self, sample_rate: int = 44100):
        self.sample_rate = sample_rate
        self.audio_processor = AudioProcessor(sample_rate)
        self.pause_detector = VocalPauseDetectorV2(sample_rate)
        self.quality_controller = QualityController(sample_rate)
        self.separator = EnhancedVocalSeparator(sample_rate)
        logger.info(f"æ— ç¼åˆ†å‰²å™¨ç»Ÿä¸€æŒ‡æŒ¥ä¸­å¿ƒåˆå§‹åŒ–å®Œæˆ (SR: {self.sample_rate})")

    def split_audio_seamlessly(self, input_path: str, output_dir: str, mode: str = 'v2.2_mdd') -> Dict:
        """
        æ‰§è¡Œæ— ç¼åˆ†å‰²çš„ä¸»å…¥å£ã€‚
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            mode: åˆ†å‰²æ¨¡å¼ ('v2.1', 'v2.2_mdd', 'smart_split', 'vocal_separation')
            
        Returns:
            åˆ†å‰²ç»“æœä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ— ç¼åˆ†å‰²: {input_path} (æ¨¡å¼: {mode})")
        
        try:
            if mode == 'vocal_separation':
                return self._process_vocal_separation_only(input_path, output_dir)
            elif mode in ['v2.1', 'v2.2_mdd']:
                return self._process_pure_vocal_split(input_path, output_dir, mode)
            elif mode == 'smart_split':
                return self._process_smart_split(input_path, output_dir)
            else:
                logger.warning(f"æœªçŸ¥æ¨¡å¼ {mode}ï¼Œä½¿ç”¨é»˜è®¤v2.2 MDDæ¨¡å¼")
                return self._process_pure_vocal_split(input_path, output_dir, 'v2.2_mdd')
                
        except Exception as e:
            logger.error(f"æ— ç¼åˆ†å‰²å¤±è´¥: {e}", exc_info=True)
            return {'success': False, 'error': str(e), 'input_file': input_path}

    def _load_and_resample_if_needed(self, input_path: str):
        """åŠ è½½éŸ³é¢‘å¹¶æ ¹æ®éœ€è¦é‡é‡‡æ ·"""
        original_audio, sr = self.audio_processor.load_audio(input_path, normalize=False)
        if sr != self.sample_rate:
            logger.info(f"éŸ³é¢‘é‡‡æ ·ç‡ {sr}Hz ä¸ç›®æ ‡ {self.sample_rate}Hz ä¸ç¬¦ï¼Œå°†è¿›è¡Œé‡é‡‡æ ·ã€‚")
            original_audio = librosa.resample(original_audio, orig_sr=sr, target_sr=self.sample_rate)
        return original_audio

    def _process_pure_vocal_split(self, input_path: str, output_dir: str, mode: str) -> Dict:
        """å¤„ç†v2.1å’Œv2.2 MDDæ¨¡å¼çš„æ ¸å¿ƒé€»è¾‘"""
        logger.info(f"[{mode.upper()}] æ‰§è¡Œçº¯äººå£°åˆ†å‰²æµç¨‹...")
        overall_start_time = time.time()

        # 1. åŠ è½½éŸ³é¢‘
        original_audio = self._load_and_resample_if_needed(input_path)
        
        # 1.5. ã€v2.2 MDDæ¨¡å¼ã€‘æ˜¾å¼å¯ç”¨MDDå¢å¼ºåŠŸèƒ½
        if mode == 'v2.2_mdd':
            from ..utils.config_manager import get_config_manager
            config_manager = get_config_manager()
            # ç¡®ä¿MDDå¢å¼ºåŠŸèƒ½å¯ç”¨
            config_manager.set('musical_dynamic_density.enable', True)
            config_manager.set('vocal_pause_splitting.enable_chorus_detection', True)
            logger.info(f"[{mode.upper()}] MDDå¢å¼ºåŠŸèƒ½å·²å¯ç”¨")
        
        # 2. ã€å…³é”®ä¿®å¤ã€‘å…ˆåœ¨åŸå§‹éŸ³é¢‘ä¸Šè¿›è¡ŒBPM/MDDåˆ†æï¼Œç¡®ä¿é…ç½®ç”Ÿæ•ˆ
        logger.info(f"[{mode.upper()}-STEP1] åœ¨åŸå§‹éŸ³é¢‘ä¸Šæ‰§è¡ŒBPMå’ŒMDDåˆ†æ...")
        if hasattr(self.pause_detector, 'adaptive_enhancer') and self.pause_detector.adaptive_enhancer:
            try:
                # åœ¨åŸå§‹æ··éŸ³ä¸Šåˆ†æç¼–æ›²å¤æ‚åº¦å’ŒBPMï¼Œè¿™æ˜¯MDDç³»ç»Ÿçš„æ ¸å¿ƒ
                complexity_segments, bpm_features = self.pause_detector.adaptive_enhancer.analyze_arrangement_complexity(original_audio)
                if bpm_features:
                    logger.info(f"ğŸµ éŸ³ä¹åˆ†æå®Œæˆ: {float(bpm_features.main_bpm):.1f} BPM ({bpm_features.bpm_category})")
            except Exception as e:
                logger.warning(f"BPM/MDDåˆ†æå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°: {e}")
        
        # 3. é«˜è´¨é‡äººå£°åˆ†ç¦»
        logger.info(f"[{mode.upper()}-STEP2] æ‰§è¡Œé«˜è´¨é‡äººå£°åˆ†ç¦»...")
        separation_start = time.time()
        separation_result = self.separator.separate_for_detection(original_audio)
        separation_time = time.time() - separation_start

        if separation_result.vocal_track is None:
            return {'success': False, 'error': 'äººå£°åˆ†ç¦»å¤±è´¥', 'input_file': input_path}
        
        vocal_track = separation_result.vocal_track
        logger.info(f"[{mode.upper()}-STEP2] äººå£°åˆ†ç¦»å®Œæˆ - åç«¯: {separation_result.backend_used}, è´¨é‡: {separation_result.separation_confidence:.3f}, è€—æ—¶: {separation_time:.1f}s")
        
        # 4. åœ¨çº¯äººå£°è½¨é“ä¸Šæ‰§è¡Œåœé¡¿æ£€æµ‹ï¼ˆä½¿ç”¨åŸå§‹éŸ³é¢‘çš„MDDåˆ†æç»“æœï¼‰
        logger.info(f"[{mode.upper()}-STEP3] åœ¨çº¯äººå£°è½¨é“ä¸Šæ‰§è¡Œåœé¡¿æ£€æµ‹ï¼ˆåº”ç”¨MDDå‚æ•°ï¼‰...")
        vocal_pauses = self.pause_detector.detect_vocal_pauses(vocal_track)

        if not vocal_pauses:
            return self._create_single_segment_result(original_audio, input_path, output_dir, "æœªåœ¨çº¯äººå£°ä¸­æ‰¾åˆ°åœé¡¿")

        # 4. ç”Ÿæˆã€è¿‡æ»¤å¹¶åˆ†å‰²
        cut_points_samples = [int(p.cut_point * self.sample_rate) for p in vocal_pauses]
        final_cut_points = self._finalize_and_filter_cuts(cut_points_samples, original_audio)
        segments = self._split_at_sample_level(original_audio, final_cut_points)
        saved_files = self._save_segments(segments, output_dir)
        
        # 5. ä¿å­˜å®Œæ•´çš„åˆ†ç¦»æ–‡ä»¶
        input_name = Path(input_path).stem
        full_vocal_file = Path(output_dir) / f"{input_name}_{mode}_vocal_full.wav"
        sf.write(full_vocal_file, vocal_track, self.sample_rate, subtype='PCM_24')
        saved_files.append(str(full_vocal_file))

        if separation_result.instrumental_track is not None:
            full_instrumental_file = Path(output_dir) / f"{input_name}_{mode}_instrumental.wav"
            sf.write(full_instrumental_file, separation_result.instrumental_track, self.sample_rate, subtype='PCM_24')
            saved_files.append(str(full_instrumental_file))

        total_time = time.time() - overall_start_time
        
        return {
            'success': True, 'method': f'pure_vocal_split_{mode}', 'num_segments': len(segments),
            'saved_files': saved_files, 'backend_used': separation_result.backend_used,
            'separation_confidence': separation_result.separation_confidence, 'processing_time': total_time,
            'input_file': input_path, 'output_dir': output_dir
        }

    def _process_smart_split(self, input_path: str, output_dir: str) -> Dict:
        """å¤„ç†ä¼ ç»Ÿæ™ºèƒ½åˆ†å‰²æ¨¡å¼"""
        logger.info("[SMART_SPLIT] æ‰§è¡Œä¼ ç»Ÿæ™ºèƒ½åˆ†å‰²...")
        original_audio = self._load_and_resample_if_needed(input_path)
        vocal_pauses = self.pause_detector.detect_vocal_pauses(original_audio)
        if not vocal_pauses:
            return self._create_single_segment_result(original_audio, input_path, output_dir, "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„åœé¡¿")

        cut_points_samples = [int(p.cut_point * self.sample_rate) for p in vocal_pauses]
        final_cut_points = self._finalize_and_filter_cuts(cut_points_samples, original_audio)
        segments = self._split_at_sample_level(original_audio, final_cut_points)
        saved_files = self._save_segments(segments, output_dir)

        return {'success': True, 'method': 'smart_split', 'num_segments': len(segments), 'saved_files': saved_files, 'input_file': input_path, 'output_dir': output_dir}

    def _process_vocal_separation_only(self, input_path: str, output_dir: str) -> Dict:
        """å¤„ç†çº¯äººå£°åˆ†ç¦»æ¨¡å¼"""
        logger.info("[VOCAL_SEPARATION] æ‰§è¡Œçº¯äººå£°åˆ†ç¦»...")
        start_time = time.time()
        original_audio = self._load_and_resample_if_needed(input_path)
        separation_result = self.separator.separate_for_detection(original_audio)
        
        if separation_result.vocal_track is None:
            return {'success': False, 'error': 'äººå£°åˆ†ç¦»å¤±è´¥', 'input_file': input_path}

        input_name = Path(input_path).stem
        saved_files = []
        vocal_file = Path(output_dir) / f"{input_name}_vocal.wav"
        sf.write(vocal_file, separation_result.vocal_track, self.sample_rate, subtype='PCM_24')
        saved_files.append(str(vocal_file))

        if separation_result.instrumental_track is not None:
            instrumental_file = Path(output_dir) / f"{input_name}_instrumental.wav"
            sf.write(instrumental_file, separation_result.instrumental_track, self.sample_rate, subtype='PCM_24')
            saved_files.append(str(instrumental_file))
            
        processing_time = time.time() - start_time

        return {
            'success': True, 'method': 'vocal_separation_only', 'num_segments': 0, 'saved_files': saved_files,
            'backend_used': separation_result.backend_used, 'separation_confidence': separation_result.separation_confidence,
            'processing_time': processing_time, 'input_file': input_path, 'output_dir': output_dir
        }

    def _finalize_and_filter_cuts(self, cut_points_samples: List[int], audio: np.ndarray) -> List[int]:
        """å¯¹åˆ‡å‰²ç‚¹è¿›è¡Œæœ€ç»ˆçš„æ’åºã€å»é‡å’Œå®‰å…¨æ ¡éªŒ"""
        audio_duration_s = len(audio) / self.sample_rate
        cut_times = sorted(list(set([p / self.sample_rate for p in cut_points_samples])))
        validated_times = [t for t in cut_times if self.quality_controller.enforce_quiet_cut(audio, self.sample_rate, t) >= 0]
        final_times = self.quality_controller.pure_filter_cut_points(validated_times, audio_duration_s)
        final_samples = [0] + [int(t * self.sample_rate) for t in final_times] + [len(audio)]
        return sorted(list(set(final_samples)))

    def _split_at_sample_level(self, audio: np.ndarray, final_cut_points: List[int]) -> List[np.ndarray]:
        """æ‰§è¡Œæ ·æœ¬çº§åˆ†å‰²"""
        segments = []
        for i in range(len(final_cut_points) - 1):
            start = final_cut_points[i]
            end = final_cut_points[i+1]
            segments.append(audio[start:end])
        return segments
    
    def _save_segments(self, segments: List[np.ndarray], output_dir: str) -> List[str]:
        """ä¿å­˜åˆ†å‰²åçš„ç‰‡æ®µ"""
        saved_files = []
        for i, segment_audio in enumerate(segments):
            output_path = Path(output_dir) / f"segment_{i+1:03d}.wav"
            sf.write(output_path, segment_audio, self.sample_rate, subtype='PCM_24')
            saved_files.append(str(output_path))
        return saved_files

    def _create_single_segment_result(self, audio: np.ndarray, input_path: str, output_dir: str, reason: str) -> Dict:
        """å½“æ— æ³•åˆ†å‰²æ—¶ï¼Œåˆ›å»ºå•ä¸ªç‰‡æ®µçš„ç»“æœ"""
        logger.warning(f"{reason}ï¼Œå°†è¾“å‡ºä¸ºå•ä¸ªæ–‡ä»¶ã€‚")
        saved_files = self._save_segments([audio], output_dir)
        return {
            'success': True, 'num_segments': 1, 'saved_files': saved_files,
            'note': reason, 'input_file': input_path, 'output_dir': output_dir
        }