#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# vocal_smart_splitter/core/adaptive_vad_enhancer.py
# AI-SUMMARY: ç¼–æ›²å¤æ‚åº¦è‡ªé€‚åº”VADå¢å¼ºå™¨ï¼Œè§£å†³æµè¡ŒéŸ³ä¹ååŠéƒ¨åˆ†ç¼–æ›²å¤æ‚åŒ–é—®é¢˜

import numpy as np
import librosa
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from sklearn.cluster import KMeans
from scipy.signal import find_peaks

logger = logging.getLogger(__name__)

@dataclass
class BPMFeatures:
    """BPMåˆ†æç‰¹å¾"""
    main_bpm: float              # ä¸»è¦BPM
    bpm_category: str            # BPMç±»åˆ« (slow/medium/fast)
    beat_strength: float         # èŠ‚æ‹å¼ºåº¦ (0-1)
    bpm_confidence: float        # BPMæ£€æµ‹ç½®ä¿¡åº¦ (0-1) 
    tempo_variance: float        # èŠ‚æ‹å˜åŒ–ç¨‹åº¦ (0-1)
    adaptive_factors: Dict = None # BPMè‡ªé€‚åº”å› å­
    beat_positions: np.ndarray = None  # èŠ‚æ‹ä½ç½®æ•°ç»„

class BPMAnalyzer:
    """BPMèŠ‚æ‹åˆ†æå™¨ - æå–éŸ³ä¹èŠ‚æ‹ç‰¹å¾ç”¨äºè‡ªé€‚åº”VADè°ƒæ•´"""
    
    def __init__(self, sample_rate: int = 44100):
        """åˆå§‹åŒ–BPMåˆ†æå™¨
        
        Args:
            sample_rate: é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        
        # BPMåˆ†ç±»é˜ˆå€¼
        self.bpm_categories = {
            'slow': (50, 80),       # æ…¢æ­Œ
            'medium': (80, 120),    # ä¸­é€Ÿ
            'fast': (120, 160),     # å¿«æ­Œ  
            'very_fast': (160, 200) # æå¿«
        }
        
        logger.info(f"BPMåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ (é‡‡æ ·ç‡: {sample_rate})")
    
    def extract_bpm_features(self, audio: np.ndarray) -> BPMFeatures:
        """æå–BPMç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            
        Returns:
            BPMç‰¹å¾å¯¹è±¡
        """
        logger.info("å¼€å§‹åˆ†æéŸ³é¢‘BPMç‰¹å¾...")
        
        try:
            # 1. åŸºç¡€èŠ‚æ‹æ£€æµ‹
            tempo, beats = librosa.beat.beat_track(
                y=audio, 
                sr=self.sample_rate,
                hop_length=512,
                start_bpm=120.0,
                tightness=100
            )
            
            # 2. èŠ‚æ‹ç¨³å®šæ€§è®¡ç®—
            beat_stability = self._calculate_beat_stability(beats, len(audio))
            
            # 3. èŠ‚æ‹å˜åŒ–åˆ†æ
            tempo_variance = self._calculate_tempo_variance(audio)
            
            # 4. éŸ³ä¹ç±»å‹åˆ†ç±»
            music_category = self._classify_music_by_bpm(tempo)
            
            # 5. è®¡ç®—è‡ªé€‚åº”å› å­
            adaptive_factors = self._calculate_bpm_adaptive_factors(tempo, beat_stability, tempo_variance)
            
            bpm_features = BPMFeatures(
                main_bpm=tempo,
                bpm_category=music_category,
                beat_strength=beat_stability,
                bpm_confidence=0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                tempo_variance=tempo_variance,
                adaptive_factors=adaptive_factors,  # æ·»åŠ è‡ªé€‚åº”å› å­
                beat_positions=beats  # æ·»åŠ èŠ‚æ‹ä½ç½®
            )
            
            logger.info(f"BPMåˆ†æå®Œæˆ: {float(tempo):.1f} BPM, ç±»å‹: {music_category}, ç¨³å®šæ€§: {float(beat_stability):.3f}")
            return bpm_features
            
        except Exception as e:
            logger.error(f"BPMç‰¹å¾æå–å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾
            return self._get_default_bpm_features()
    
    def _calculate_beat_stability(self, beats: np.ndarray, audio_length: int) -> float:
        """è®¡ç®—èŠ‚æ‹ç¨³å®šæ€§
        
        Args:
            beats: èŠ‚æ‹ä½ç½®æ•°ç»„
            audio_length: éŸ³é¢‘æ€»é•¿åº¦
            
        Returns:
            èŠ‚æ‹ç¨³å®šæ€§ (0-1, 1è¡¨ç¤ºéå¸¸ç¨³å®š)
        """
        if len(beats) < 3:
            return 0.5  # é»˜è®¤ä¸­ç­‰ç¨³å®šæ€§
        
        # è®¡ç®—ç›¸é‚»èŠ‚æ‹é—´éš”
        beat_intervals = np.diff(beats)
        
        if len(beat_intervals) < 2:
            return 0.5
        
        # ç¨³å®šæ€§ = 1 - (æ ‡å‡†å·® / å‡å€¼)ï¼Œå€¼è¶Šå¤§è¶Šç¨³å®š
        mean_interval = np.mean(beat_intervals)
        std_interval = np.std(beat_intervals)
        
        if mean_interval == 0:
            return 0.5
            
        stability = 1.0 - (std_interval / mean_interval)
        return np.clip(stability, 0.0, 1.0)
    
    def _calculate_tempo_variance(self, audio: np.ndarray) -> float:
        """è®¡ç®—èŠ‚æ‹å˜åŒ–ç¨‹åº¦
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            
        Returns:
            èŠ‚æ‹å˜åŒ–ç¨‹åº¦ (å€¼è¶Šå¤§å˜åŒ–è¶Šå¤§)
        """
        try:
            # è®¡ç®—çŸ­æ—¶èŠ‚æ‹å˜åŒ–
            hop_length = 512
            frame_length = 2048
            
            # ä½¿ç”¨onset strengthä½œä¸ºèŠ‚æ‹å˜åŒ–çš„åŸºç¡€
            onset_envelope = librosa.onset.onset_strength(
                y=audio, 
                sr=self.sample_rate,
                hop_length=hop_length,
                aggregate=np.median
            )
            
            # è®¡ç®—åŠ¨æ€tempoå˜åŒ–
            tempo_curve = librosa.beat.tempo(
                onset_envelope=onset_envelope,
                sr=self.sample_rate,
                hop_length=hop_length,
                aggregate=None  # ä¸èšåˆï¼Œä¿æŒæ—¶é—´å˜åŒ–
            )
            
            if len(tempo_curve) > 1:
                # ç¡®ä¿æ•°ç»„æ“ä½œçš„å…¼å®¹æ€§
                tempo_array = np.asarray(tempo_curve, dtype=np.float64)
                variance = float(np.std(tempo_array)) / (float(np.mean(tempo_array)) + 1e-8)
                return float(np.clip(variance, 0.0, 1.0))
            else:
                return 0.1  # é»˜è®¤ä½å˜åŒ–
                
        except Exception as e:
            logger.warning(f"èŠ‚æ‹å˜åŒ–åˆ†æå¤±è´¥: {e}")
            return 0.1
    
    def _classify_music_by_bpm(self, bpm: float) -> str:
        """æ ¹æ®BPMåˆ†ç±»éŸ³ä¹ç±»å‹
        
        Args:
            bpm: èŠ‚æ‹é€Ÿåº¦
            
        Returns:
            éŸ³ä¹ç±»å‹æ ‡ç­¾
        """
        for category, (min_bpm, max_bpm) in self.bpm_categories.items():
            if min_bpm <= bpm < max_bpm:
                return category
        
        # è¶…å‡ºèŒƒå›´çš„å¤„ç†
        if bpm < 50:
            return 'very_slow'
        else:
            return 'extreme_fast'
    
    def _calculate_bpm_adaptive_factors(self, bpm: float, stability: float, variance: float) -> Dict:
        """è®¡ç®—BPMè‡ªé€‚åº”è°ƒæ•´å› å­
        
        Args:
            bpm: èŠ‚æ‹é€Ÿåº¦
            stability: èŠ‚æ‹ç¨³å®šæ€§
            variance: èŠ‚æ‹å˜åŒ–ç¨‹åº¦
            
        Returns:
            è‡ªé€‚åº”å› å­å­—å…¸
        """
        # åŸºäºBPMçš„åŸºç¡€è°ƒæ•´ï¼ˆå‚æ•°åŒ–é…ç½®ï¼‰
        from ..utils.config_manager import get_config
        
        if bpm < 70:  # æ…¢æ­Œ
            pause_multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.slow_song_multiplier', 1.5)
            base_factors = {
                'threshold_modifier': -0.05,    # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
                'min_pause_modifier': pause_multiplier,      # ä½¿ç”¨é…ç½®çš„æ…¢æ­Œä¹˜æ•°
                'min_speech_modifier': 1.2,     # å…è®¸æ›´é•¿è¯­éŸ³
                'sensitivity': 'high'
            }
        elif bpm < 100:  # ä¸­é€Ÿ
            pause_multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.medium_song_multiplier', 1.0)
            base_factors = {
                'threshold_modifier': 0.0,      # åŸºå‡†é˜ˆå€¼
                'min_pause_modifier': pause_multiplier,      # ä½¿ç”¨é…ç½®çš„ä¸­é€Ÿæ­Œä¹˜æ•°
                'min_speech_modifier': 1.0,     # æ ‡å‡†è¯­éŸ³
                'sensitivity': 'medium'
            }
        elif bpm < 140:  # å¿«æ­Œ
            pause_multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.fast_song_multiplier', 0.7)
            base_factors = {
                'threshold_modifier': 0.1,      # æé«˜é˜ˆå€¼ï¼Œæ›´ä¿å®ˆ
                'min_pause_modifier': pause_multiplier,      # ä½¿ç”¨é…ç½®çš„å¿«æ­Œä¹˜æ•°
                'min_speech_modifier': 0.8,     # ç¼©çŸ­æœ€å°è¯­éŸ³
                'sensitivity': 'low'
            }
        else:  # æå¿«
            pause_multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.fast_song_multiplier', 0.7)
            base_factors = {
                'threshold_modifier': 0.15,
                'min_pause_modifier': pause_multiplier,      # ä½¿ç”¨å¿«æ­Œé…ç½®
                'min_speech_modifier': 0.6,
                'sensitivity': 'very_low'
            }
        
        # ç¨³å®šæ€§è°ƒæ•´ï¼šä¸ç¨³å®šæ—¶æ›´ä¿å®ˆ
        stability_adjustment = (1.0 - stability) * 0.1  # æœ€å¤šå¢åŠ 0.1çš„é˜ˆå€¼
        base_factors['threshold_modifier'] += stability_adjustment
        
        # å˜åŒ–åº¦è°ƒæ•´ï¼šå˜åŒ–å¤§æ—¶éœ€è¦æ›´é«˜ç½®ä¿¡åº¦
        variance_adjustment = variance * 0.05
        base_factors['threshold_modifier'] += variance_adjustment
        
        # æ·»åŠ å…·ä½“æ•°å€¼
        base_factors.update({
            'bpm_value': bpm,
            'stability_score': stability,
            'variance_score': variance,
            'recommended_window_size': self._calculate_analysis_window_size(bpm),
            'beat_sync_important': bpm > 100  # å¿«æ­Œéœ€è¦èŠ‚æ‹å¯¹é½
        })
        
        return base_factors
    
    def _calculate_analysis_window_size(self, bpm: float) -> float:
        """æ ¹æ®BPMè®¡ç®—æœ€ä½³åˆ†æçª—å£å¤§å°
        
        Args:
            bpm: èŠ‚æ‹é€Ÿåº¦
            
        Returns:
            åˆ†æçª—å£å¤§å°ï¼ˆç§’ï¼‰
        """
        # å¿«æ­Œç”¨è¾ƒçŸ­çª—å£ï¼Œæ…¢æ­Œç”¨è¾ƒé•¿çª—å£
        if bpm < 70:
            return 12.0  # æ…¢æ­Œï¼š12ç§’çª—å£
        elif bpm < 120:
            return 10.0  # ä¸­é€Ÿï¼š10ç§’çª—å£
        else:
            return 8.0   # å¿«æ­Œï¼š8ç§’çª—å£
    
    def _get_default_bpm_features(self) -> BPMFeatures:
        """è·å–é»˜è®¤BPMç‰¹å¾ï¼ˆå½“åˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        
        Returns:
            é»˜è®¤BPMç‰¹å¾
        """
        # é»˜è®¤è‡ªé€‚åº”å› å­
        default_adaptive_factors = {
            'threshold_modifier': 0.0,
            'min_pause_modifier': 1.0,
            'min_speech_modifier': 1.0,
            'sensitivity': 'medium',
            'bpm_value': 110.0,
            'stability_score': 0.6,
            'variance_score': 0.2,
            'recommended_window_size': 10.0,
            'beat_sync_important': False
        }
        
        return BPMFeatures(
            main_bpm=110.0,  # é»˜è®¤ä¸­é€Ÿ
            bpm_category='medium',
            beat_strength=0.6,
            bpm_confidence=0.5,
            tempo_variance=0.2,
            adaptive_factors=default_adaptive_factors,
            beat_positions=np.array([])
        )

@dataclass
class ArrangementComplexitySegment:
    """ç¼–æ›²å¤æ‚åº¦ç‰‡æ®µ"""
    start_time: float           # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end_time: float             # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    complexity_score: float     # å¤æ‚åº¦è¯„åˆ† (0-1)
    spectral_density: float     # é¢‘è°±å¯†åº¦
    harmonic_content: float     # è°æ³¢å†…å®¹
    bpm_influence: float        # BPMå½±å“å› å­ (0-1)
    beat_alignment: float       # èŠ‚æ‹å¯¹é½åº¦ (0-1)
    recommended_threshold: float # æ¨èVADé˜ˆå€¼
    recommended_min_pause: float # æ¨èæœ€å°åœé¡¿æ—¶é•¿
    instrument_count: int = 0    # æ£€æµ‹åˆ°çš„ä¹å™¨æ•°é‡
    arrangement_density: float = 0.0  # ç¼–æ›²å¯†åº¦è¯„åˆ†

class InstrumentComplexityAnalyzer:
    """ä¹å™¨å¤æ‚åº¦åˆ†æå™¨ - æ£€æµ‹ç¼–æ›²ä¸­çš„ä¹å™¨æ•°é‡å’Œå¤æ‚åº¦"""
    
    def __init__(self, sample_rate: int = 44100):
        """åˆå§‹åŒ–ä¹å™¨å¤æ‚åº¦åˆ†æå™¨
        
        Args:
            sample_rate: é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        
        # ä¹å™¨é¢‘æ®µå®šä¹‰ï¼ˆHzï¼‰
        self.instrument_bands = {
            'bass': (40, 250),           # è´æ–¯
            'kick_drum': (50, 120),      # åº•é¼“
            'snare_drum': (150, 300),    # å†›é¼“
            'guitar_low': (80, 800),     # å‰ä»–ä½é¢‘
            'guitar_mid': (800, 3000),   # å‰ä»–ä¸­é¢‘
            'vocal_main': (200, 2000),   # äººå£°ä¸»é¢‘æ®µ
            'vocal_formant': (1000, 4000), # äººå£°å…±æŒ¯å³°
            'cymbals': (3000, 12000),    # é•²ç‰‡
            'piano_low': (80, 500),      # é’¢ç´ä½éŸ³
            'piano_mid': (500, 2000),    # é’¢ç´ä¸­éŸ³
            'piano_high': (2000, 8000),  # é’¢ç´é«˜éŸ³
            'strings': (200, 4000),      # å¼¦ä¹
            'brass': (100, 3000),        # é“œç®¡
            'synth_lead': (200, 8000),   # åˆæˆå™¨ä¸»éŸ³
            'synth_pad': (50, 2000)      # åˆæˆå™¨é“ºåº•
        }
        
        logger.info("ä¹å™¨å¤æ‚åº¦åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_instrument_complexity(self, audio: np.ndarray) -> Dict:
        """åˆ†æéŸ³é¢‘ä¸­çš„ä¹å™¨å¤æ‚åº¦
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            
        Returns:
            ä¹å™¨å¤æ‚åº¦åˆ†æç»“æœ
        """
        logger.info("å¼€å§‹ä¹å™¨å¤æ‚åº¦åˆ†æ...")
        
        try:
            # 1. é¢‘æ®µèƒ½é‡åˆ†æ
            band_energies = self._analyze_frequency_bands(audio)
            
            # 2. ä¹å™¨æ•°é‡ä¼°ç®—
            instrument_count = self._estimate_instrument_count(band_energies, audio)
            
            # 3. ç¼–æ›²å¯†åº¦åˆ†æ
            arrangement_density = self._calculate_arrangement_density(audio)
            
            # 4. éŸ³è‰²å¤æ‚åº¦åˆ†æ
            timbre_complexity = self._analyze_timbre_complexity(audio)
            
            # 5. è°æ³¢å±‚æ¬¡åˆ†æ
            harmonic_layers = self._analyze_harmonic_layers(audio)
            
            # 6. æ—¶åºå¤æ‚åº¦åˆ†æ
            temporal_complexity = self._analyze_temporal_complexity(audio)
            
            # 7. ä¹å™¨åˆ†ç¦»ç½®ä¿¡åº¦
            separation_confidence = self._calculate_separation_confidence(band_energies)
            
            # 8. äººå£°å¹²æ‰°è¯„ä¼°
            vocal_interference = self._assess_vocal_interference(band_energies, audio)
            
            complexity_result = {
                'instrument_count': instrument_count,
                'arrangement_density': arrangement_density,
                'timbre_complexity': timbre_complexity,
                'harmonic_layers': harmonic_layers,
                'temporal_complexity': temporal_complexity,
                'separation_confidence': separation_confidence,
                'vocal_interference': vocal_interference,
                'band_energies': band_energies,
                'overall_complexity': self._calculate_overall_complexity(
                    instrument_count, arrangement_density, timbre_complexity,
                    harmonic_layers, temporal_complexity, vocal_interference
                )
            }
            
            logger.info(f"ä¹å™¨å¤æ‚åº¦åˆ†æå®Œæˆ: {instrument_count}ç§ä¹å™¨, æ€»å¤æ‚åº¦: {complexity_result['overall_complexity']:.3f}")
            
            return complexity_result
            
        except Exception as e:
            logger.error(f"ä¹å™¨å¤æ‚åº¦åˆ†æå¤±è´¥: {e}")
            return self._get_default_complexity_result()
    
    def _analyze_frequency_bands(self, audio: np.ndarray) -> Dict[str, float]:
        """åˆ†æå„é¢‘æ®µçš„èƒ½é‡åˆ†å¸ƒ"""
        # è®¡ç®—çŸ­æ—¶å‚…é‡Œå¶å˜æ¢
        stft = librosa.stft(audio, n_fft=2048, hop_length=512)
        magnitude = np.abs(stft)
        power_spectrum = magnitude ** 2
        
        # é¢‘ç‡è½´
        freq_axis = librosa.fft_frequencies(sr=self.sample_rate, n_fft=2048)
        
        band_energies = {}
        
        for band_name, (low_freq, high_freq) in self.instrument_bands.items():
            # æ‰¾åˆ°é¢‘ç‡èŒƒå›´å¯¹åº”çš„ç´¢å¼•
            low_idx = np.argmin(np.abs(freq_axis - low_freq))
            high_idx = np.argmin(np.abs(freq_axis - high_freq))
            
            # è®¡ç®—è¯¥é¢‘æ®µçš„å¹³å‡èƒ½é‡
            band_power = np.mean(power_spectrum[low_idx:high_idx, :])
            band_energies[band_name] = float(band_power)
        
        return band_energies
    
    def _estimate_instrument_count(self, band_energies: Dict[str, float], audio: np.ndarray) -> int:
        """ä¼°ç®—æ´»è·ƒä¹å™¨æ•°é‡"""
        total_energy = sum(band_energies.values())
        if total_energy == 0:
            return 1
        
        # ä¹å™¨ç»„åˆé€»è¾‘æ£€æµ‹
        instrument_evidence = {
            'bass': band_energies['bass'] > 0.08 * total_energy,
            'drums': (band_energies['kick_drum'] > 0.04 * total_energy or 
                     band_energies['snare_drum'] > 0.04 * total_energy or
                     band_energies['cymbals'] > 0.02 * total_energy),
            'guitar': (band_energies['guitar_low'] > 0.06 * total_energy or
                      band_energies['guitar_mid'] > 0.06 * total_energy),
            'piano': (band_energies['piano_low'] > 0.05 * total_energy and
                     band_energies['piano_mid'] > 0.05 * total_energy),
            'strings': band_energies['strings'] > 0.08 * total_energy,
            'brass': band_energies['brass'] > 0.06 * total_energy,
            'synth': (band_energies['synth_lead'] > 0.05 * total_energy or
                     band_energies['synth_pad'] > 0.08 * total_energy),
            'vocal': (band_energies['vocal_main'] > 0.12 * total_energy and
                     band_energies['vocal_formant'] > 0.08 * total_energy)
        }
        
        active_instruments = sum(evidence for evidence in instrument_evidence.values())
        return int(np.clip(active_instruments, 1, 8))
    
    def _calculate_arrangement_density(self, audio: np.ndarray) -> float:
        """è®¡ç®—ç¼–æ›²å¯†åº¦"""
        try:
            # ä½¿ç”¨é¢‘è°±è´¨å¿ƒåˆ†æ
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate)[0]
            centroid_std = np.std(spectral_centroids) / (np.mean(spectral_centroids) + 1e-8)
            
            # é¢‘è°±å±•å¼€åº¦
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=self.sample_rate)[0]
            rolloff_variation = np.std(spectral_rolloff) / (np.mean(spectral_rolloff) + 1e-8)
            
            density_score = min(centroid_std * 0.001 + rolloff_variation * 0.0001, 1.0)
            return float(np.clip(density_score, 0.0, 1.0))
        except:
            return 0.5
    
    def _analyze_timbre_complexity(self, audio: np.ndarray) -> float:
        """åˆ†æéŸ³è‰²å¤æ‚åº¦"""
        try:
            mfccs = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=13)
            mfcc_variance = np.mean(np.var(mfccs, axis=1))
            complexity = min(mfcc_variance / 100.0, 1.0)
            return float(np.clip(complexity, 0.0, 1.0))
        except:
            return 0.5
    
    def _analyze_harmonic_layers(self, audio: np.ndarray) -> float:
        """åˆ†æè°æ³¢å±‚æ¬¡"""
        try:
            chroma = librosa.feature.chroma_stft(y=audio, sr=self.sample_rate)
            chroma_threshold = 0.3 * np.max(chroma, axis=0, keepdims=True)
            active_pitches_per_frame = np.sum(chroma > chroma_threshold, axis=0)
            avg_active_pitches = np.mean(active_pitches_per_frame)
            harmonic_complexity = min(avg_active_pitches / 6.0, 1.0)
            return float(np.clip(harmonic_complexity, 0.0, 1.0))
        except:
            return 0.5
    
    def _analyze_temporal_complexity(self, audio: np.ndarray) -> float:
        """åˆ†ææ—¶åºå¤æ‚åº¦"""
        try:
            rms_energy = librosa.feature.rms(y=audio, frame_length=2048, hop_length=512)[0]
            energy_variance = np.std(rms_energy) / (np.mean(rms_energy) + 1e-8)
            temporal_complexity = min(energy_variance * 2.0, 1.0)
            return float(np.clip(temporal_complexity, 0.0, 1.0))
        except:
            return 0.5
    
    def _calculate_separation_confidence(self, band_energies: Dict[str, float]) -> float:
        """è®¡ç®—ä¹å™¨åˆ†ç¦»ç½®ä¿¡åº¦"""
        total_energy = sum(band_energies.values())
        if total_energy == 0:
            return 0.5
        
        vocal_energy = band_energies['vocal_main'] + band_energies['vocal_formant']
        competing_energy = (band_energies['guitar_mid'] + band_energies['piano_mid'] + 
                          band_energies['strings'] + band_energies['synth_lead'])
        
        vocal_ratio = vocal_energy / total_energy
        competing_ratio = competing_energy / total_energy
        
        if vocal_ratio > competing_ratio * 1.5:
            return 0.8
        elif vocal_ratio > competing_ratio:
            return 0.6
        else:
            return 0.3
    
    def _assess_vocal_interference(self, band_energies: Dict[str, float], audio: np.ndarray) -> float:
        """è¯„ä¼°äººå£°æ£€æµ‹çš„å¹²æ‰°ç¨‹åº¦"""
        total_energy = sum(band_energies.values())
        if total_energy == 0:
            return 0.5
        
        interference_sources = {
            'guitar_interference': band_energies['guitar_mid'] / total_energy,
            'piano_interference': band_energies['piano_mid'] / total_energy,
            'strings_interference': band_energies['strings'] / total_energy,
            'synth_interference': band_energies['synth_lead'] / total_energy,
            'brass_interference': band_energies['brass'] / total_energy
        }
        
        total_interference = sum(interference_sources.values())
        return float(np.clip(total_interference, 0.0, 1.0))
    
    def _calculate_overall_complexity(self, instrument_count: int, arrangement_density: float,
                                   timbre_complexity: float, harmonic_layers: float,
                                   temporal_complexity: float, vocal_interference: float) -> float:
        """è®¡ç®—æ€»ä½“å¤æ‚åº¦è¯„åˆ†"""
        normalized_instruments = min(instrument_count / 8.0, 1.0)
        
        overall_score = (
            0.25 * normalized_instruments +
            0.20 * arrangement_density +
            0.15 * timbre_complexity +
            0.15 * harmonic_layers +
            0.10 * temporal_complexity +
            0.15 * vocal_interference
        )
        
        return float(np.clip(overall_score, 0.0, 1.0))
    
    def _get_default_complexity_result(self) -> Dict:
        """è·å–é»˜è®¤å¤æ‚åº¦åˆ†æç»“æœ"""
        return {
            'instrument_count': 3,
            'arrangement_density': 0.5,
            'timbre_complexity': 0.5,
            'harmonic_layers': 0.5,
            'temporal_complexity': 0.5,
            'separation_confidence': 0.5,
            'vocal_interference': 0.5,
            'band_energies': {},
            'overall_complexity': 0.5
        }

class AdaptiveVADEnhancer:
    """BPMæ„ŸçŸ¥çš„ç¼–æ›²å¤æ‚åº¦è‡ªé€‚åº”VADå¢å¼ºå™¨
    
    è§£å†³æµè¡ŒéŸ³ä¹ä¸­å¸¸è§çš„é—®é¢˜ï¼š
    1. å‰åŠéƒ¨åˆ†ç¼–æ›²ç®€å•ï¼ŒVADè¿‡æ•æ„Ÿ â†’ äº§ç”Ÿè¶…çŸ­ç‰‡æ®µ  
    2. ååŠéƒ¨åˆ†ç¼–æ›²å¤æ‚ï¼ŒVADä¸æ•æ„Ÿ â†’ æ¼æ£€çœŸå®åœé¡¿
    3. ä¸åŒBPMçš„éŸ³ä¹éœ€è¦ä¸åŒçš„æ£€æµ‹ç­–ç•¥
    """
    
    def __init__(self, sample_rate: int = 44100):
        """åˆå§‹åŒ–è‡ªé€‚åº”å¢å¼ºå™¨
        
        Args:
            sample_rate: é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        
        # åˆå§‹åŒ–BPMåˆ†æå™¨
        self.bpm_analyzer = BPMAnalyzer(sample_rate)
        
        # åˆå§‹åŒ–ä¹å™¨å¤æ‚åº¦åˆ†æå™¨
        self.instrument_analyzer = InstrumentComplexityAnalyzer(sample_rate)
        
        # ç¼–æ›²å¤æ‚åº¦åˆ†æå‚æ•°ï¼ˆå°†æ ¹æ®BPMåŠ¨æ€è°ƒæ•´ï¼‰
        self.default_analysis_window = 10.0  # é»˜è®¤çª—å£å¤§å°
        self.complexity_threshold = 0.6      # å¤æ‚åº¦é˜ˆå€¼
        
        # VADè‡ªé€‚åº”é˜ˆå€¼èŒƒå›´ï¼ˆæ‰©å±•ä»¥é€‚åº”å¤šä¹å™¨ç¯å¢ƒï¼‰
        self.min_vad_threshold = 0.20  # ç®€å•ç¼–æ›²æœ€ä½é˜ˆå€¼ï¼ˆé™ä½ï¼‰
        self.max_vad_threshold = 0.80  # å¤æ‚ç¼–æ›²æœ€é«˜é˜ˆå€¼ï¼ˆæé«˜ï¼‰
        self.base_threshold = 0.35     # åŸºå‡†é˜ˆå€¼
        
        # æ–°å¢ï¼šBPMæ„ŸçŸ¥æƒé‡
        self.bpm_weight = 0.35         # BPMæŒ‡æ ‡åœ¨å¤æ‚åº¦è¯„åˆ†ä¸­çš„æƒé‡
        self.beat_alignment_weight = 0.1  # èŠ‚æ‹å¯¹é½çš„æƒé‡
        
        logger.info("BPMæ„ŸçŸ¥çš„ç¼–æ›²å¤æ‚åº¦è‡ªé€‚åº”VADå¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_bpm(self, audio: np.ndarray) -> BPMFeatures:
        """åˆ†æéŸ³é¢‘çš„BPMç‰¹å¾ï¼ˆå¯¹å¤–æ¥å£ï¼‰
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            
        Returns:
            BPMç‰¹å¾å¯¹è±¡
        """
        return self.bpm_analyzer.extract_bpm_features(audio)
    
    def generate_adaptive_thresholds(self, bpm_features: BPMFeatures, 
                                   complexity_scores: List[float]) -> Dict:
        """ç”ŸæˆBPMè‡ªé€‚åº”é˜ˆå€¼
        
        Args:
            bpm_features: BPMç‰¹å¾
            complexity_scores: å¤æ‚åº¦è¯„åˆ†åˆ—è¡¨
            
        Returns:
            è‡ªé€‚åº”é˜ˆå€¼å­—å…¸
        """
        # æ ¹æ®BPMç±»åˆ«è°ƒæ•´åŸºç¡€é˜ˆå€¼
        if bpm_features.bpm_category == 'slow':
            base_threshold = self.base_threshold * 0.8  # æ…¢æ­Œæ›´æ•æ„Ÿ
            bpm_factor = 0.7
        elif bpm_features.bpm_category == 'fast':
            base_threshold = self.base_threshold * 1.2  # å¿«æ­Œæ›´ä¿å®ˆ
            bpm_factor = 1.3
        else:
            base_threshold = self.base_threshold
            bpm_factor = 1.0
        
        # æ ¹æ®å¤æ‚åº¦ç”Ÿæˆåˆ†æ®µé˜ˆå€¼ï¼ˆå¢å¼ºå¤šä¹å™¨é€‚åº”æ€§ï¼‰
        segment_thresholds = []
        for score in complexity_scores:
            # å¤šä¹å™¨ç¯å¢ƒä¸‹çš„å¢å¼ºè°ƒæ•´
            if bpm_features.bpm_category in ['fast', 'very_fast']:
                # å¿«æ­Œç¯å¢ƒï¼šæ›´ä¿å®ˆçš„é˜ˆå€¼è°ƒæ•´
                instrument_boost = score * 0.35  # ä»0.2å¢åŠ åˆ°0.35
            elif score > 0.6:  # é«˜å¤æ‚åº¦ç¯å¢ƒ
                instrument_boost = score * 0.45  # æ˜¾è‘—æå‡é˜ˆå€¼
            else:
                instrument_boost = score * 0.25  # è½»åº¦è°ƒæ•´
            
            adaptive_threshold = base_threshold + instrument_boost
            adaptive_threshold = np.clip(adaptive_threshold, self.min_vad_threshold, self.max_vad_threshold)
            segment_thresholds.append(adaptive_threshold)
        
        return {
            'base_threshold': base_threshold,
            'segment_thresholds': segment_thresholds,
            'bpm_factor': bpm_factor,
            'bpm_category': bpm_features.bpm_category
        }
    
    def analyze_arrangement_complexity(self, audio: np.ndarray) -> Tuple[List[ArrangementComplexitySegment], BPMFeatures]:
        """åˆ†æéŸ³é¢‘çš„ç¼–æ›²å¤æ‚åº¦å˜åŒ–ï¼ˆé›†æˆBPMåˆ†æï¼‰
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            
        Returns:
            (ç¼–æ›²å¤æ‚åº¦ç‰‡æ®µåˆ—è¡¨, BPMç‰¹å¾)
        """
        logger.info("å¼€å§‹BPMæ„ŸçŸ¥çš„ç¼–æ›²å¤æ‚åº¦åˆ†æ...")
        
        try:
            # 1. é¦–å…ˆæå–æ•´ä½“BPMç‰¹å¾
            bpm_features = self.bpm_analyzer.extract_bpm_features(audio)
            
            # 2. æ ¹æ®BPMè°ƒæ•´åˆ†æçª—å£å¤§å°
            analysis_window = bpm_features.adaptive_factors['recommended_window_size']
            
            # 3. è®¡ç®—éŸ³é¢‘æ€»é•¿åº¦
            total_duration = len(audio) / self.sample_rate
            segments = []
            
            # 4. å…¨å±€ä¹å™¨å¤æ‚åº¦åˆ†æ
            instrument_complexity = self.instrument_analyzer.analyze_instrument_complexity(audio)
            logger.info(f"æ£€æµ‹åˆ° {instrument_complexity['instrument_count']} ç§ä¹å™¨ï¼Œæ€»å¤æ‚åº¦: {instrument_complexity['overall_complexity']:.3f}")
            
            # 5. æŒ‰çª—å£åˆ†æå¤æ‚åº¦
            window_samples = int(analysis_window * self.sample_rate)
            hop_samples = window_samples // 2  # 50%é‡å 
            
            # 5. èŠ‚æ‹å¯¹é½çš„åˆ†æçª—å£ï¼ˆå¦‚æœBPMç¨³å®šä¸”éœ€è¦èŠ‚æ‹åŒæ­¥ï¼‰
            beat_positions = bpm_features.beat_positions
            use_beat_alignment = (
                bpm_features.adaptive_factors['beat_sync_important'] and 
                len(beat_positions) > 0
            )
            
            for i in range(0, len(audio) - window_samples, hop_samples):
                start_sample = i
                end_sample = min(i + window_samples, len(audio))
                segment_audio = audio[start_sample:end_sample]
                
                start_time = start_sample / self.sample_rate
                end_time = end_sample / self.sample_rate
                
                # 6. è®¡ç®—ä¼ ç»Ÿå¤æ‚åº¦æŒ‡æ ‡
                complexity_metrics = self._calculate_complexity_metrics(segment_audio)
                
                # 7. è®¡ç®—BPMå½±å“å› å­å’ŒèŠ‚æ‹å¯¹é½åº¦
                bpm_influence = self._calculate_bpm_influence_factor(
                    bpm_features, start_time, end_time, total_duration
                )
                beat_alignment = self._calculate_beat_alignment(
                    beat_positions, start_sample, end_sample
                ) if use_beat_alignment else 0.5
                
                # 8. ç»¼åˆå¤æ‚åº¦è¯„åˆ†ï¼ˆé›†æˆBPMï¼‰
                complexity_score = self._calculate_enhanced_complexity(
                    complexity_metrics, bpm_features, bpm_influence, beat_alignment
                )
                
                # 9. å¤šç»´è‡ªé€‚åº”é˜ˆå€¼å’Œåœé¡¿æ—¶é•¿
                adaptive_params = self._calculate_multi_dimensional_adaptive_params(
                    complexity_score, bpm_features, start_time / total_duration
                )
                
                segment = ArrangementComplexitySegment(
                    start_time=start_time,
                    end_time=end_time,
                    complexity_score=complexity_score,
                    spectral_density=complexity_metrics['spectral_density'],
                    harmonic_content=complexity_metrics['harmonic_content'],
                    bpm_influence=bpm_influence,
                    beat_alignment=beat_alignment,
                    recommended_threshold=adaptive_params['voice_threshold'],
                    recommended_min_pause=adaptive_params['min_pause_duration']
                )
                
                segments.append(segment)
            
            # 10. å¹³æ»‘å¤æ‚åº¦å˜åŒ–ï¼ˆé¿å…é˜ˆå€¼çªå˜ï¼‰
            segments = self._smooth_complexity_transitions(segments)
            
            logger.info(f"BPMæ„ŸçŸ¥å¤æ‚åº¦åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(segments)} ä¸ªç‰‡æ®µ")
            self._log_enhanced_complexity_summary(segments, bpm_features)
            
            return segments, bpm_features
            
        except Exception as e:
            logger.error(f"BPMæ„ŸçŸ¥å¤æ‚åº¦åˆ†æå¤±è´¥: {e}")
            return [], self.bpm_analyzer._get_default_bpm_features()
    
    def _calculate_complexity_metrics(self, audio_segment: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—ç¼–æ›²å¤æ‚åº¦æŒ‡æ ‡
        
        Args:
            audio_segment: éŸ³é¢‘ç‰‡æ®µ
            
        Returns:
            å¤æ‚åº¦æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # 1. é¢‘è°±å¯†åº¦ - è¡¡é‡é¢‘ç‡æˆåˆ†çš„ä¸°å¯Œç¨‹åº¦
        stft = librosa.stft(audio_segment, hop_length=512, n_fft=2048)
        magnitude = np.abs(stft)
        
        # è®¡ç®—æ¯ä¸ªé¢‘ç‡binçš„æ´»è·ƒåº¦
        freq_activity = np.mean(magnitude > 0.01 * np.max(magnitude), axis=1)
        metrics['spectral_density'] = np.mean(freq_activity)
        
        # 2. è°æ³¢å†…å®¹ - æ£€æµ‹å¤šä¹å™¨å åŠ 
        try:
            # ä½¿ç”¨chromagramæ£€æµ‹å’Œå£°å¤æ‚åº¦
            chroma = librosa.feature.chroma_stft(S=magnitude**2, sr=self.sample_rate)
            # è®¡ç®—åŒæ—¶æ´»è·ƒçš„éŸ³é«˜ç±»åˆ«æ•°é‡
            active_pitches = np.mean(np.sum(chroma > 0.3 * np.max(chroma, axis=0), axis=0))
            metrics['harmonic_content'] = min(active_pitches / 6.0, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
        except:
            metrics['harmonic_content'] = 0.5  # é»˜è®¤ä¸­ç­‰å¤æ‚åº¦
        
        # 3. åŠ¨æ€èŒƒå›´ - è¡¡é‡éŸ³é‡å˜åŒ–çš„å‰§çƒˆç¨‹åº¦
        rms = librosa.feature.rms(y=audio_segment, hop_length=512)[0]
        if len(rms) > 1:
            dynamic_range = np.std(rms) / (np.mean(rms) + 1e-8)
            metrics['dynamic_range'] = min(dynamic_range * 2.0, 1.0)
        else:
            metrics['dynamic_range'] = 0.3
        
        # 4. é¢‘è°±è´¨å¿ƒå˜åŒ– - è¡¡é‡éŸ³è‰²å˜åŒ–
        try:
            spectral_centroids = librosa.feature.spectral_centroid(
                y=audio_segment, sr=self.sample_rate, hop_length=512
            )[0]
            if len(spectral_centroids) > 1:
                centroid_variation = np.std(spectral_centroids) / (np.mean(spectral_centroids) + 1e-8)
                metrics['spectral_variation'] = min(centroid_variation * 0.001, 1.0)
            else:
                metrics['spectral_variation'] = 0.3
        except:
            metrics['spectral_variation'] = 0.3
        
        # 5. é›¶äº¤å‰ç‡å˜åŒ– - è¡¡é‡ç¬æ€å†…å®¹
        zcr = librosa.feature.zero_crossing_rate(audio_segment, hop_length=512)[0]
        if len(zcr) > 1:
            zcr_variation = np.std(zcr) / (np.mean(zcr) + 1e-8)
            metrics['transient_content'] = min(zcr_variation * 10.0, 1.0)
        else:
            metrics['transient_content'] = 0.3
        
        return metrics
    
    def _calculate_bpm_influence_factor(
        self, bpm_features: BPMFeatures, start_time: float, end_time: float, total_duration: float
    ) -> float:
        """è®¡ç®—BPMåœ¨å½“å‰æ—¶é—´æ®µçš„å½±å“å› å­
        
        Args:
            bpm_features: BPMç‰¹å¾
            start_time: ç‰‡æ®µå¼€å§‹æ—¶é—´
            end_time: ç‰‡æ®µç»“æŸæ—¶é—´
            total_duration: éŸ³é¢‘æ€»æ—¶é•¿
            
        Returns:
            BPMå½±å“å› å­ (0-1)
        """
        # åŸºç¡€BPMå½±å“ - å¿«æ­Œå½±å“æ›´å¤§
        bpm_base_influence = min(bpm_features.main_bpm / 160.0, 1.0)
        
        # æ—¶é—´ä½ç½®å½±å“ - ååŠéƒ¨åˆ†é€šå¸¸ç¼–æ›²æ›´å¤æ‚
        time_position = (start_time + end_time) / (2 * total_duration)
        time_influence = 0.5 + 0.5 * time_position  # 0.5-1.0
        
        # èŠ‚æ‹ç¨³å®šæ€§å½±å“ - ä¸ç¨³å®šçš„èŠ‚æ‹å¢åŠ å¤æ‚åº¦
        stability_influence = 1.0 - bpm_features.beat_strength
        
        # ç»¼åˆå½±å“å› å­
        influence = (
            0.5 * bpm_base_influence +
            0.3 * time_influence +
            0.2 * stability_influence
        )
        
        return np.clip(influence, 0.0, 1.0)
    
    def _calculate_beat_alignment(
        self, beat_positions: np.ndarray, start_sample: int, end_sample: int
    ) -> float:
        """è®¡ç®—å½“å‰ç‰‡æ®µä¸èŠ‚æ‹çš„å¯¹é½åº¦
        
        Args:
            beat_positions: èŠ‚æ‹ä½ç½®æ•°ç»„ï¼ˆæ ·æœ¬ç‚¹ï¼‰
            start_sample: ç‰‡æ®µå¼€å§‹æ ·æœ¬ç‚¹
            end_sample: ç‰‡æ®µç»“æŸæ ·æœ¬ç‚¹
            
        Returns:
            èŠ‚æ‹å¯¹é½åº¦ (0-1, 1è¡¨ç¤ºå®Œå…¨å¯¹é½)
        """
        if len(beat_positions) == 0:
            return 0.5  # é»˜è®¤ä¸­ç­‰å¯¹é½åº¦
        
        # æ‰¾åˆ°ç‰‡æ®µå†…çš„èŠ‚æ‹ç‚¹
        beats_in_segment = beat_positions[
            (beat_positions >= start_sample) & (beat_positions <= end_sample)
        ]
        
        if len(beats_in_segment) == 0:
            return 0.3  # æ— èŠ‚æ‹ç‚¹ï¼Œå¯¹é½åº¦è¾ƒä½
        
        # è®¡ç®—èŠ‚æ‹åˆ†å¸ƒçš„å‡åŒ€æ€§
        segment_length = end_sample - start_sample
        expected_beats = len(beats_in_segment)
        
        if expected_beats < 2:
            return 0.5
        
        # è®¡ç®—å®é™…èŠ‚æ‹é—´éš”çš„ä¸€è‡´æ€§
        actual_intervals = np.diff(beats_in_segment)
        interval_consistency = 1.0 - (np.std(actual_intervals) / (np.mean(actual_intervals) + 1e-8))
        
        return np.clip(interval_consistency, 0.0, 1.0)
    
    def _calculate_enhanced_complexity(
        self, 
        traditional_metrics: Dict[str, float], 
        bpm_features: BPMFeatures,
        bpm_influence: float,
        beat_alignment: float
    ) -> float:
        """è®¡ç®—å¢å¼ºçš„å¤æ‚åº¦è¯„åˆ†ï¼ˆé›†æˆBPMï¼‰
        
        Args:
            traditional_metrics: ä¼ ç»Ÿå¤æ‚åº¦æŒ‡æ ‡
            bpm_features: BPMç‰¹å¾
            bpm_influence: BPMå½±å“å› å­
            beat_alignment: èŠ‚æ‹å¯¹é½åº¦
            
        Returns:
            å¢å¼ºçš„å¤æ‚åº¦è¯„åˆ† (0-1)
        """
        # æ–°æƒé‡åˆ†é…ï¼ˆBPMæ„ŸçŸ¥ï¼‰
        weights = {
            'bpm_factor': 0.25,           # BPMå› å­æœ€é‡è¦
            'beat_alignment': 0.1,        # èŠ‚æ‹å¯¹é½åº¦
            'spectral_density': 0.2,      # é¢‘è°±å¯†åº¦ï¼ˆä»0.3é™æƒï¼‰
            'harmonic_content': 0.15,     # å’Œå£°å¤æ‚åº¦ï¼ˆä»0.25é™æƒï¼‰
            'dynamic_range': 0.15,        # åŠ¨æ€èŒƒå›´
            'spectral_variation': 0.1,    # é¢‘è°±å˜åŒ–
            'transient_content': 0.05     # ç¬æ€å†…å®¹
        }
        
        # è®¡ç®—ç»¼åˆå¤æ‚åº¦
        complexity_score = 0.0
        
        # BPMç›¸å…³å› å­
        complexity_score += weights['bpm_factor'] * bpm_influence
        complexity_score += weights['beat_alignment'] * (1.0 - beat_alignment)  # å¯¹é½åº¦ä½=å¤æ‚åº¦é«˜
        
        # ä¼ ç»ŸæŒ‡æ ‡
        for metric, weight in weights.items():
            if metric in ['bpm_factor', 'beat_alignment']:
                continue
            complexity_score += traditional_metrics.get(metric, 0.5) * weight
        
        return np.clip(complexity_score, 0.0, 1.0)
    
    def _calculate_multi_dimensional_adaptive_params(
        self, complexity_score: float, bpm_features: BPMFeatures, time_position: float
    ) -> Dict[str, float]:
        """å¤šç»´è‡ªé€‚åº”é˜ˆå€¼ç”Ÿæˆç®—æ³•
        
        Args:
            complexity_score: ç»¼åˆå¤æ‚åº¦è¯„åˆ†
            bpm_features: BPMç‰¹å¾
            time_position: æ—¶é—´ä½ç½®æ¯”ä¾‹ (0-1)
            
        Returns:
            è‡ªé€‚åº”å‚æ•°å­—å…¸
        """
        # è·å–BPMåŸºç¡€è°ƒæ•´å› å­
        bpm_factors = bpm_features.adaptive_factors
        
        # 1. åŸºç¡€é˜ˆå€¼è®¡ç®—
        base_threshold = self.base_threshold
        
        # 2. BPMè°ƒæ•´
        bpm_adjustment = bpm_factors['threshold_modifier']
        
        # 3. å¤æ‚åº¦è°ƒæ•´
        complexity_adjustment = (complexity_score - 0.5) * 0.3
        
        # 4. æ—¶é—´ä½ç½®è°ƒæ•´ï¼ˆååŠéƒ¨åˆ†æ›´ä¿å®ˆï¼‰
        time_adjustment = 0.15 * time_position if time_position > 0.6 else 0
        
        # 5. ç»¼åˆé˜ˆå€¼
        final_threshold = base_threshold + bpm_adjustment + complexity_adjustment + time_adjustment
        final_threshold = np.clip(final_threshold, self.min_vad_threshold, self.max_vad_threshold)
        
        # 6. è‡ªé€‚åº”åœé¡¿æ—¶é•¿
        base_min_pause = 1.0
        pause_adjustment = (
            base_min_pause * 
            bpm_factors['min_pause_modifier'] * 
            (1.0 + 0.3 * complexity_score)  # å¤æ‚åº¦é«˜æ—¶éœ€è¦æ›´é•¿åœé¡¿
        )
        
        # 7. è‡ªé€‚åº”è¯­éŸ³æ—¶é•¿
        base_min_speech = 0.4
        speech_adjustment = (
            base_min_speech * 
            bpm_factors['min_speech_modifier'] * 
            (1.0 + 0.2 * complexity_score)
        )
        
        return {
            'voice_threshold': round(float(final_threshold), 3),
            'min_pause_duration': round(float(pause_adjustment), 2),
            'min_speech_duration': round(float(speech_adjustment), 2),
            'complexity_context': {
                'bpm_category': bpm_features.bpm_category,
                'bpm_value': bpm_features.main_bpm,
                'complexity_score': complexity_score,
                'time_position': time_position
            }
        }
    
    def get_enhanced_adaptive_vad_params(
        self, 
        segments: List[ArrangementComplexitySegment], 
        bpm_features: BPMFeatures,
        current_time: float
    ) -> Dict[str, float]:
        """è·å–å½“å‰æ—¶é—´ç‚¹çš„BPMæ„ŸçŸ¥è‡ªé€‚åº”VADå‚æ•°
        
        Args:
            segments: ç¼–æ›²å¤æ‚åº¦ç‰‡æ®µåˆ—è¡¨
            bpm_features: BPMç‰¹å¾
            current_time: å½“å‰æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            å¢å¼ºçš„è‡ªé€‚åº”VADå‚æ•°å­—å…¸
        """
        # æ‰¾åˆ°å½“å‰æ—¶é—´å¯¹åº”çš„å¤æ‚åº¦ç‰‡æ®µ
        current_segment = None
        for segment in segments:
            if segment.start_time <= current_time <= segment.end_time:
                current_segment = segment
                break
        
        if current_segment is None:
            # å¦‚æœæ²¡æ‰¾åˆ°å¯¹åº”ç‰‡æ®µï¼Œä½¿ç”¨BPMåŸºç¡€å‚æ•°
            base_factors = bpm_features.adaptive_factors
            return {
                'voice_threshold': self.base_threshold + base_factors['threshold_modifier'],
                'min_silence_duration_ms': int(1000 * base_factors['min_pause_modifier']),
                'min_speech_duration_ms': int(400 * base_factors['min_speech_modifier']),
                'bpm_context': {
                    'bpm_value': bpm_features.main_bpm,
                    'music_category': bpm_features.bpm_category,
                    'complexity_score': 0.5
                }
            }
        
        # ä½¿ç”¨ç‰‡æ®µçš„æ¨èå‚æ•°
        return {
            'voice_threshold': current_segment.recommended_threshold,
            'min_silence_duration_ms': int(current_segment.recommended_min_pause * 1000),
            'min_speech_duration_ms': int(400 * bpm_features.adaptive_factors['min_speech_modifier']),
            'bpm_context': {
                'bpm_value': bpm_features.main_bpm,
                'music_category': bpm_features.bpm_category,
                'complexity_score': current_segment.complexity_score,
                'bpm_influence': current_segment.bpm_influence,
                'beat_alignment': current_segment.beat_alignment
            },
            'window_size_samples': int(bpm_features.adaptive_factors['recommended_window_size'] * self.sample_rate / 100)
        }
    
    def _calculate_overall_complexity(self, metrics: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆå¤æ‚åº¦è¯„åˆ†
        
        Args:
            metrics: å¤æ‚åº¦æŒ‡æ ‡
            
        Returns:
            ç»¼åˆå¤æ‚åº¦è¯„åˆ† (0-1)
        """
        # æƒé‡è®¾è®¡ï¼ˆåŸºäºæµè¡ŒéŸ³ä¹ç‰¹ç‚¹ï¼‰
        weights = {
            'spectral_density': 0.3,    # é¢‘è°±å¯†åº¦æœ€é‡è¦
            'harmonic_content': 0.25,   # å’Œå£°å¤æ‚åº¦æ¬¡é‡è¦
            'dynamic_range': 0.2,       # åŠ¨æ€èŒƒå›´
            'spectral_variation': 0.15, # é¢‘è°±å˜åŒ–
            'transient_content': 0.1    # ç¬æ€å†…å®¹
        }
        
        complexity_score = 0.0
        for metric, weight in weights.items():
            complexity_score += metrics.get(metric, 0.5) * weight
        
        return min(max(complexity_score, 0.0), 1.0)
    
    def _calculate_adaptive_threshold(self, complexity_score: float) -> float:
        """æ ¹æ®å¤æ‚åº¦è®¡ç®—è‡ªé€‚åº”VADé˜ˆå€¼
        
        Args:
            complexity_score: å¤æ‚åº¦è¯„åˆ† (0-1)
            
        Returns:
            æ¨èçš„VADé˜ˆå€¼
        """
        # å¤æ‚åº¦è¶Šé«˜ï¼ŒVADé˜ˆå€¼è¶Šé«˜ï¼ˆæ›´ä¿å®ˆï¼‰
        # å¤æ‚åº¦è¶Šä½ï¼ŒVADé˜ˆå€¼è¶Šä½ï¼ˆæ›´æ•æ„Ÿï¼‰
        
        if complexity_score < 0.3:
            # ç®€å•ç¼–æ›²ï¼šä½¿ç”¨è¾ƒä½é˜ˆå€¼ï¼Œä½†ä¸èƒ½å¤ªä½ï¼ˆé¿å…è¶…çŸ­ç‰‡æ®µï¼‰
            threshold = self.min_vad_threshold + (complexity_score / 0.3) * 0.1
        elif complexity_score > 0.7:
            # å¤æ‚ç¼–æ›²ï¼šä½¿ç”¨è¾ƒé«˜é˜ˆå€¼
            threshold = self.base_threshold + ((complexity_score - 0.7) / 0.3) * (
                self.max_vad_threshold - self.base_threshold
            )
        else:
            # ä¸­ç­‰ç¼–æ›²ï¼šçº¿æ€§æ’å€¼
            threshold = self.min_vad_threshold + complexity_score * (
                self.base_threshold - self.min_vad_threshold
            )
        
        return round(float(threshold), 3)
    
    def _smooth_complexity_transitions(
        self, segments: List[ArrangementComplexitySegment]
    ) -> List[ArrangementComplexitySegment]:
        """å¹³æ»‘å¤æ‚åº¦å˜åŒ–ï¼Œé¿å…é˜ˆå€¼çªå˜
        
        Args:
            segments: åŸå§‹å¤æ‚åº¦ç‰‡æ®µ
            
        Returns:
            å¹³æ»‘åçš„å¤æ‚åº¦ç‰‡æ®µ
        """
        if len(segments) < 3:
            return segments
        
        # ä½¿ç”¨ç§»åŠ¨å¹³å‡å¹³æ»‘é˜ˆå€¼
        window_size = 3
        for i in range(len(segments)):
            start_idx = max(0, i - window_size // 2)
            end_idx = min(len(segments), i + window_size // 2 + 1)
            
            # è®¡ç®—çª—å£å†…çš„å¹³å‡é˜ˆå€¼
            avg_threshold = np.mean([
                seg.recommended_threshold for seg in segments[start_idx:end_idx]
            ])
            
            # æ›´æ–°é˜ˆå€¼ï¼ˆä¿æŒä¸€å®šçš„åŸå§‹ç‰¹æ€§ï¼‰
            segments[i].recommended_threshold = (
                0.7 * segments[i].recommended_threshold + 0.3 * avg_threshold
            )
        
        return segments
    
    def _log_enhanced_complexity_summary(
        self, segments: List[ArrangementComplexitySegment], bpm_features: BPMFeatures
    ):
        """è®°å½•å¢å¼ºç‰ˆå¤æ‚åº¦åˆ†ææ‘˜è¦ï¼ˆåŒ…å«BPMä¿¡æ¯ï¼‰"""
        if not segments:
            return
        
        complexity_scores = [seg.complexity_score for seg in segments]
        thresholds = [seg.recommended_threshold for seg in segments]
        min_pauses = [seg.recommended_min_pause for seg in segments]
        bpm_influences = [seg.bpm_influence for seg in segments]
        beat_alignments = [seg.beat_alignment for seg in segments]
        
        logger.info("=== BPMæ„ŸçŸ¥å¤æ‚åº¦åˆ†ææ‘˜è¦ ===")
        logger.info(f"ğŸµ éŸ³ä¹ç‰¹å¾: {float(bpm_features.main_bpm):.1f} BPM ({bpm_features.bpm_category})")
        logger.info(f"ğŸ¼ èŠ‚æ‹ç¨³å®šæ€§: {float(bpm_features.beat_strength):.3f}")
        logger.info(f"ğŸ“Š å¤æ‚åº¦èŒƒå›´: {float(min(complexity_scores)):.3f} - {float(max(complexity_scores)):.3f}")
        logger.info(f"ğŸ“Š å¹³å‡å¤æ‚åº¦: {float(np.mean(complexity_scores)):.3f}")
        logger.info(f"âš™ï¸  VADé˜ˆå€¼èŒƒå›´: {float(min(thresholds)):.3f} - {float(max(thresholds)):.3f}")
        logger.info(f"â±ï¸  åœé¡¿æ—¶é•¿èŒƒå›´: {float(min(min_pauses)):.2f}s - {float(max(min_pauses)):.2f}s")
        logger.info(f"ğŸµ BPMå½±å“å› å­: {float(np.mean(bpm_influences)):.3f}")
        logger.info(f"ğŸ¼ èŠ‚æ‹å¯¹é½åº¦: {float(np.mean(beat_alignments)):.3f}")
        
        # BPMè‡ªé€‚åº”æ•ˆæœåˆ†æ
        bpm_factors = bpm_features.adaptive_factors
        logger.info(f"ğŸ”§ BPMè‡ªé€‚åº”è°ƒæ•´: é˜ˆå€¼{bpm_factors['threshold_modifier']:+.3f}, åœé¡¿Ã—{bpm_factors['min_pause_modifier']:.2f}")
        
        # æ£€æµ‹ç¼–æ›²å¤æ‚åº¦è¶‹åŠ¿
        first_half = complexity_scores[:len(complexity_scores)//2]
        second_half = complexity_scores[len(complexity_scores)//2:]
        
        if np.mean(second_half) > np.mean(first_half) + 0.1:
            logger.info("ğŸ“ˆ æ£€æµ‹åˆ°ç¼–æ›²å¤æ‚åº¦é€’å¢è¶‹åŠ¿ï¼ˆå…¸å‹æµè¡ŒéŸ³ä¹æ¨¡å¼ï¼‰+ BPMè‡ªé€‚åº”è°ƒæ•´")
        elif np.mean(first_half) > np.mean(second_half) + 0.1:
            logger.info("ğŸ“‰ æ£€æµ‹åˆ°ç¼–æ›²å¤æ‚åº¦é€’å‡è¶‹åŠ¿ + BPMè‡ªé€‚åº”è°ƒæ•´")
        else:
            logger.info("â¡ï¸  ç¼–æ›²å¤æ‚åº¦ç›¸å¯¹ç¨³å®š + BPMåŸºå‡†è°ƒæ•´")
    
    def _log_complexity_summary(self, segments: List[ArrangementComplexitySegment]):
        """è®°å½•å¤æ‚åº¦åˆ†ææ‘˜è¦"""
        if not segments:
            return
        
        complexity_scores = [seg.complexity_score for seg in segments]
        thresholds = [seg.recommended_threshold for seg in segments]
        
        logger.info("=== ç¼–æ›²å¤æ‚åº¦åˆ†ææ‘˜è¦ ===")
        logger.info(f"å¤æ‚åº¦èŒƒå›´: {min(complexity_scores):.3f} - {max(complexity_scores):.3f}")
        logger.info(f"å¹³å‡å¤æ‚åº¦: {np.mean(complexity_scores):.3f}")
        logger.info(f"VADé˜ˆå€¼èŒƒå›´: {min(thresholds):.3f} - {max(thresholds):.3f}")
        
        # æ£€æµ‹ç¼–æ›²å¤æ‚åº¦è¶‹åŠ¿
        first_half = complexity_scores[:len(complexity_scores)//2]
        second_half = complexity_scores[len(complexity_scores)//2:]
        
        if np.mean(second_half) > np.mean(first_half) + 0.1:
            logger.info("ğŸ“ˆ æ£€æµ‹åˆ°ç¼–æ›²å¤æ‚åº¦é€’å¢è¶‹åŠ¿ï¼ˆå…¸å‹æµè¡ŒéŸ³ä¹æ¨¡å¼ï¼‰")
        elif np.mean(first_half) > np.mean(second_half) + 0.1:
            logger.info("ğŸ“‰ æ£€æµ‹åˆ°ç¼–æ›²å¤æ‚åº¦é€’å‡è¶‹åŠ¿")
        else:
            logger.info("â¡ï¸  ç¼–æ›²å¤æ‚åº¦ç›¸å¯¹ç¨³å®š")
    
    def get_adaptive_vad_params(
        self, 
        segments: List[ArrangementComplexitySegment], 
        current_time: float
    ) -> Dict[str, float]:
        """è·å–å½“å‰æ—¶é—´ç‚¹çš„è‡ªé€‚åº”VADå‚æ•°
        
        Args:
            segments: ç¼–æ›²å¤æ‚åº¦ç‰‡æ®µåˆ—è¡¨
            current_time: å½“å‰æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            è‡ªé€‚åº”VADå‚æ•°å­—å…¸
        """
        # æ‰¾åˆ°å½“å‰æ—¶é—´å¯¹åº”çš„å¤æ‚åº¦ç‰‡æ®µ
        current_segment = None
        for segment in segments:
            if segment.start_time <= current_time <= segment.end_time:
                current_segment = segment
                break
        
        if current_segment is None:
            # å¦‚æœæ²¡æ‰¾åˆ°å¯¹åº”ç‰‡æ®µï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
            return {
                'voice_threshold': self.base_threshold,
                'min_silence_duration_ms': 1000,
                'min_speech_duration_ms': 400
            }
        
        # æ ¹æ®å¤æ‚åº¦è°ƒæ•´å‚æ•°
        complexity = current_segment.complexity_score
        threshold = current_segment.recommended_threshold
        
        # å¤æ‚åº¦é«˜æ—¶ï¼Œå¢åŠ æœ€å°é™éŸ³å’Œè¯­éŸ³æ—¶é•¿è¦æ±‚
        min_silence_ms = 800 + int(complexity * 600)  # 800-1400ms
        min_speech_ms = 300 + int(complexity * 400)   # 300-700ms
        
        return {
            'voice_threshold': threshold,
            'min_silence_duration_ms': min_silence_ms,
            'min_speech_duration_ms': min_speech_ms,
            'complexity_score': complexity
        }