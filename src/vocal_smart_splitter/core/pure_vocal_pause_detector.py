#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# File: src/vocal_smart_splitter/core/pure_vocal_pause_detector.py
# AI-SUMMARY: çº¯äººå£°åœé¡¿æ£€æµ‹å™¨ - åŸºäºMDX23/Demucsåˆ†ç¦»åçš„çº¯äººå£°è¿›è¡Œå¤šç»´ç‰¹å¾åˆ†æï¼Œè§£å†³é«˜é¢‘æ¢æ°”è¯¯åˆ¤é—®é¢˜

import numpy as np
import librosa
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from scipy import signal
from scipy.ndimage import gaussian_filter1d

from ..utils.config_manager import get_config

logger = logging.getLogger(__name__)

@dataclass
class VocalFeatures:
    """äººå£°ç‰¹å¾æ•°æ®ç»“æ„"""
    f0_contour: np.ndarray           # åŸºé¢‘è½¨è¿¹
    f0_confidence: np.ndarray        # åŸºé¢‘ç½®ä¿¡åº¦
    formant_energies: List[np.ndarray]  # å…±æŒ¯å³°èƒ½é‡åºåˆ—
    spectral_centroid: np.ndarray    # é¢‘è°±è´¨å¿ƒ
    harmonic_ratio: np.ndarray       # è°æ³¢æ¯”ç‡
    zero_crossing_rate: np.ndarray   # è¿‡é›¶ç‡
    rms_energy: np.ndarray           # RMSèƒ½é‡

@dataclass
class PureVocalPause:
    """çº¯äººå£°åœé¡¿ç»“æ„"""
    start_time: float
    end_time: float
    duration: float
    pause_type: str  # 'true_pause', 'breath', 'uncertain'
    confidence: float
    features: Dict  # è¯¦ç»†ç‰¹å¾ä¿¡æ¯
    cut_point: float = 0.0  # æœ€ä½³åˆ‡å‰²ç‚¹ï¼ˆæ–°å¢ï¼‰
    quality_grade: str = 'B'  # è´¨é‡ç­‰çº§ï¼ˆæ–°å¢ï¼‰
    is_valid: bool = True   # æ˜¯å¦æœ‰æ•ˆï¼ˆæ–°å¢ï¼‰
    
class PureVocalPauseDetector:
    """åŸºäºçº¯äººå£°çš„å¤šç»´ç‰¹å¾åœé¡¿æ£€æµ‹å™¨
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. F0è¿ç»­æ€§åˆ†æ - æ£€æµ‹åŸºé¢‘çªå˜è¯†åˆ«çœŸåœé¡¿
    2. å…±æŒ¯å³°èƒ½é‡åˆ†å¸ƒ - åŒºåˆ†æ¢æ°”vsé™éŸ³
    3. é¢‘è°±è´¨å¿ƒè¿½è¸ª - è¯†åˆ«é«˜é¢‘è¡°å‡æ¨¡å¼
    4. è°æ³¢å¼ºåº¦åˆ†æ - è¯„ä¼°å‘å£°è´¨é‡
    """
    
    def __init__(self, sample_rate: int = 44100):
        """åˆå§‹åŒ–çº¯äººå£°åœé¡¿æ£€æµ‹å™¨
        
        Args:
            sample_rate: é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        
        # ä»é…ç½®åŠ è½½å‚æ•°
        self.min_pause_duration = get_config('pure_vocal_detection.min_pause_duration', 0.5)
        self.breath_duration_range = get_config('pure_vocal_detection.breath_duration_range', [0.1, 0.3])
        self.f0_weight = get_config('pure_vocal_detection.f0_weight', 0.3)
        self.formant_weight = get_config('pure_vocal_detection.formant_weight', 0.25)
        self.spectral_weight = get_config('pure_vocal_detection.spectral_weight', 0.25)
        self.duration_weight = get_config('pure_vocal_detection.duration_weight', 0.2)
        
        # æ£€æµ‹é˜ˆå€¼
        self.energy_threshold_db = get_config('pure_vocal_detection.energy_threshold_db', -40)
        self.f0_drop_threshold = get_config('pure_vocal_detection.f0_drop_threshold', 0.7)
        self.breath_confidence_threshold = get_config('pure_vocal_detection.breath_filter_threshold', 0.3)
        self.pause_confidence_threshold = get_config('pure_vocal_detection.pause_confidence_threshold', 0.7)
        
        # åˆ†æå‚æ•°
        self.hop_length = int(sample_rate * 0.01)  # 10ms hop
        self.frame_length = int(sample_rate * 0.025)  # 25ms frame
        self.n_fft = 2048
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šé›†æˆVocalPauseDetectorV2çš„èƒ½é‡è°·æ£€æµ‹èƒ½åŠ›
        from .vocal_pause_detector import VocalPauseDetectorV2
        self._cut_point_calculator = VocalPauseDetectorV2(sample_rate)
        
        logger.info(f"çº¯äººå£°åœé¡¿æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ (é‡‡æ ·ç‡: {sample_rate}) - å·²é›†æˆèƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—")
    
    def detect_pure_vocal_pauses(self, vocal_audio: np.ndarray, 
                                enable_mdd_enhancement: bool = False,
                                original_audio: Optional[np.ndarray] = None) -> List[PureVocalPause]:
        """æ£€æµ‹çº¯äººå£°ä¸­çš„åœé¡¿
        
        Args:
            vocal_audio: åˆ†ç¦»åçš„çº¯äººå£°éŸ³é¢‘
            original_audio: åŸå§‹æ··éŸ³(å¯é€‰ï¼Œç”¨äºå¯¹æ¯”)
            
        Returns:
            æ£€æµ‹åˆ°çš„åœé¡¿åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹çº¯äººå£°åœé¡¿æ£€æµ‹... (MDDå¢å¼º: {enable_mdd_enhancement})")
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¯ç”¨ç›¸å¯¹èƒ½é‡è°·æ£€æµ‹
        enable_relative_mode = get_config('pure_vocal_detection.enable_relative_energy_mode', False)
        if enable_relative_mode:
            logger.info("ä½¿ç”¨ç›¸å¯¹èƒ½é‡è°·æ£€æµ‹æ¨¡å¼...")
            peak_ratio = get_config('pure_vocal_detection.peak_relative_threshold_ratio', 0.1)
            rms_ratio = get_config('pure_vocal_detection.rms_relative_threshold_ratio', 0.05)
            # BPM/MDD è‡ªé€‚åº”å€ç‡ï¼ˆåœ¨ç›¸å¯¹èƒ½é‡æ¨¡å¼ä¸‹å¯ç”¨ï¼‰
            if get_config('pure_vocal_detection.relative_threshold_adaptation.enable', True):
                ref_audio = original_audio if original_audio is not None else vocal_audio
                try:
                    tempo_est, _ = librosa.beat.beat_track(y=ref_audio, sr=self.sample_rate)
                    # å…¼å®¹ ndarray/æ ‡é‡ï¼Œç»Ÿä¸€ä¸º float
                    tempo = float(np.squeeze(np.asarray(tempo_est))) if tempo_est is not None else 0.0
                except Exception:
                    tempo = 0.0
                bpm_cfg = get_config('vocal_pause_splitting.bpm_adaptive_settings', {})
                slow_thr = bpm_cfg.get('slow_bpm_threshold', 80)
                fast_thr = bpm_cfg.get('fast_bpm_threshold', 120)
                bpm_mul_slow = get_config('pure_vocal_detection.relative_threshold_adaptation.bpm.slow_multiplier', 1.10)
                bpm_mul_med = get_config('pure_vocal_detection.relative_threshold_adaptation.bpm.medium_multiplier', 1.00)
                bpm_mul_fast = get_config('pure_vocal_detection.relative_threshold_adaptation.bpm.fast_multiplier', 0.85)
                if tempo and tempo > 0:
                    if tempo < slow_thr:
                        mul_bpm = bpm_mul_slow; bpm_tag = 'slow'
                    elif tempo > fast_thr:
                        mul_bpm = bpm_mul_fast; bpm_tag = 'fast'
                    else:
                        mul_bpm = bpm_mul_med; bpm_tag = 'medium'
                else:
                    mul_bpm = bpm_mul_med; bpm_tag = 'unknown'

                # ä¼°ç®—å…¨æ›² MDDï¼ˆç®€åŒ–ç‰ˆï¼‰
                def _mdd_score_simple(x, sr):
                    try:
                        rms = librosa.feature.rms(y=x, hop_length=512)[0]
                        flat = librosa.feature.spectral_flatness(y=x)[0]
                        onset_env = librosa.onset.onset_strength(y=x, sr=sr, hop_length=512)
                        onsets = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, hop_length=512)
                        dur = max(0.1, len(x)/sr)
                        onset_rate = len(onsets)/dur
                        def nz(v):
                            v = np.asarray(v); q10, q90 = np.quantile(v,0.1), np.quantile(v,0.9)
                            if q90-q10 < 1e-9: return 0.0
                            return float(np.clip((np.mean(v)-q10)/(q90-q10), 0, 1))
                        rms_s, flat_s = nz(rms), nz(flat)
                        onset_s = float(np.clip(onset_rate/10.0, 0, 1))
                        return float(np.clip(0.5*rms_s + 0.3*flat_s + 0.2*onset_s, 0, 1))
                    except Exception:
                        return 0.5
                mdd_s = _mdd_score_simple(ref_audio, self.sample_rate)
                mdd_base = get_config('pure_vocal_detection.relative_threshold_adaptation.mdd.base', 1.0)
                mdd_gain = get_config('pure_vocal_detection.relative_threshold_adaptation.mdd.gain', 0.2)
                mul_mdd = mdd_base + (0.1 - mdd_gain*mdd_s)
                clamp_min = get_config('pure_vocal_detection.relative_threshold_adaptation.clamp_min', 0.75)
                clamp_max = get_config('pure_vocal_detection.relative_threshold_adaptation.clamp_max', 1.25)
                mul = float(np.clip(mul_bpm*mul_mdd, clamp_min, clamp_max))
                peak_ratio *= mul; rms_ratio *= mul
                logger.info(f"ç›¸å¯¹é˜ˆå€¼è‡ªé€‚åº”ï¼šBPM={tempo:.1f}({bpm_tag}), MDD={mdd_s:.2f}, mul={mul:.2f} â†’ peak={peak_ratio:.3f}, rms={rms_ratio:.3f}")
            
            # ä½¿ç”¨ç›¸å¯¹èƒ½é‡è°·æ£€æµ‹
            filtered_pauses = self._detect_energy_valleys(vocal_audio, peak_ratio, rms_ratio)
        else:
            # åŸæœ‰çš„å¤šç»´ç‰¹å¾æ£€æµ‹æµç¨‹
            # 1. æå–å¤šç»´ç‰¹å¾
            features = self._extract_vocal_features(vocal_audio)
            
            # 2. æ£€æµ‹å€™é€‰åœé¡¿åŒºåŸŸ
            candidate_pauses = self._detect_candidate_pauses(features)
            
            # 3. ç‰¹å¾èåˆåˆ†æ
            analyzed_pauses = self._analyze_pause_features(candidate_pauses, features, vocal_audio)
            
            # 4. åˆ†ç±»è¿‡æ»¤
            filtered_pauses = self._classify_and_filter(analyzed_pauses)
            
        # 5. MDDå¢å¼ºå¤„ç†
        if enable_mdd_enhancement and original_audio is not None:
            logger.info("åº”ç”¨MDDå¢å¼ºå¤„ç†...")
            filtered_pauses = self._apply_mdd_enhancement(filtered_pauses, original_audio)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨VocalPauseDetectorV2è®¡ç®—ç²¾ç¡®åˆ‡ç‚¹
        if filtered_pauses and vocal_audio is not None:
            filtered_pauses = self._calculate_precise_cut_points(filtered_pauses, vocal_audio)
        
        logger.info(f"æ£€æµ‹å®Œæˆ: {len(filtered_pauses)}ä¸ªé«˜è´¨é‡åœé¡¿ç‚¹")
        return filtered_pauses
    
    def _extract_vocal_features(self, audio: np.ndarray) -> VocalFeatures:
        """æå–äººå£°å¤šç»´ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            æå–çš„ç‰¹å¾é›†åˆ
        """
        logger.debug("æå–äººå£°ç‰¹å¾...")
        
        # 1. åŸºé¢‘(F0)æå– - ä½¿ç”¨librosaçš„pyinç®—æ³•
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio, 
            fmin=librosa.note_to_hz('C2'),  # 65Hz
            fmax=librosa.note_to_hz('C7'),  # 2093Hz
            sr=self.sample_rate,
            hop_length=self.hop_length
        )
        
        # 2. å…±æŒ¯å³°åˆ†æ - ä½¿ç”¨LPCåˆ†æ
        formant_energies = self._extract_formants(audio)
        
        # 3. é¢‘è°±è´¨å¿ƒ
        spectral_centroid = librosa.feature.spectral_centroid(
            y=audio, sr=self.sample_rate, hop_length=self.hop_length
        )[0]
        
        # 4. è°æ³¢åˆ†æ - åŸºäºçº¯äººå£°ä¿¡å·ï¼Œæ— éœ€å†åˆ†ç¦»
        harmonic_ratio = self._calculate_harmonic_ratio_direct(audio)
        
        # 5. è¿‡é›¶ç‡
        zero_crossing_rate = librosa.feature.zero_crossing_rate(
            audio, hop_length=self.hop_length
        )[0]
        
        # 6. RMSèƒ½é‡
        rms_energy = librosa.feature.rms(
            y=audio, hop_length=self.hop_length
        )[0]
        
        return VocalFeatures(
            f0_contour=f0,
            f0_confidence=voiced_probs,
            formant_energies=formant_energies,
            spectral_centroid=spectral_centroid,
            harmonic_ratio=harmonic_ratio,
            zero_crossing_rate=zero_crossing_rate,
            rms_energy=rms_energy
        )
    
    def _extract_formants(self, audio: np.ndarray, n_formants: int = 3) -> List[np.ndarray]:
        """æå–å…±æŒ¯å³°èƒ½é‡
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            n_formants: è¦æå–çš„å…±æŒ¯å³°æ•°é‡
            
        Returns:
            å…±æŒ¯å³°èƒ½é‡åºåˆ—
        """
        formants = []
        
        # åˆ†å¸§å¤„ç†
        frames = librosa.util.frame(audio, frame_length=self.frame_length, 
                                   hop_length=self.hop_length)
        
        for frame in frames.T:
            # LPCåˆ†æ
            try:
                # ä½¿ç”¨è‡ªç›¸å…³æ–¹æ³•ä¼°è®¡LPCç³»æ•°
                lpc_order = 2 + self.sample_rate // 1000  # ç»éªŒå…¬å¼
                a = librosa.lpc(frame, order=min(lpc_order, len(frame) - 1))
                
                # ä»LPCç³»æ•°æå–å…±æŒ¯å³°é¢‘ç‡
                roots = np.roots(a)
                roots = roots[np.imag(roots) >= 0]  # åªä¿ç•™æ­£é¢‘ç‡
                
                # è½¬æ¢ä¸ºé¢‘ç‡
                angles = np.angle(roots)
                freqs = angles * self.sample_rate / (2 * np.pi)
                
                # æ’åºå¹¶é€‰æ‹©å‰nä¸ªå…±æŒ¯å³°
                freqs = sorted(freqs[freqs > 0])[:n_formants]
                
                # å¦‚æœå…±æŒ¯å³°æ•°é‡ä¸è¶³ï¼Œå¡«å……é›¶
                while len(freqs) < n_formants:
                    freqs.append(0)
                    
                formants.append(freqs)
            except:
                # LPCå¤±è´¥æ—¶å¡«å……é›¶
                formants.append([0] * n_formants)
        
        # è½¬ç½®å¾—åˆ°æ¯ä¸ªå…±æŒ¯å³°çš„æ—¶é—´åºåˆ—
        formants = np.array(formants).T
        return [formants[i] for i in range(n_formants)]
    
    def _calculate_harmonic_ratio(self, harmonic: np.ndarray, 
                                 original: np.ndarray) -> np.ndarray:
        """è®¡ç®—è°æ³¢æ¯”ç‡
        
        Args:
            harmonic: è°æ³¢æˆåˆ†
            original: åŸå§‹ä¿¡å·
            
        Returns:
            è°æ³¢æ¯”ç‡æ—¶é—´åºåˆ—
        """
        # è®¡ç®—èƒ½é‡æ¯”
        harmonic_rms = librosa.feature.rms(y=harmonic, hop_length=self.hop_length)[0]
        original_rms = librosa.feature.rms(y=original, hop_length=self.hop_length)[0]
        
        # é¿å…é™¤é›¶
        ratio = np.zeros_like(harmonic_rms)
        non_zero = original_rms > 1e-10
        ratio[non_zero] = harmonic_rms[non_zero] / original_rms[non_zero]
        
        return ratio

    def _detect_candidate_pauses(self, features: VocalFeatures) -> List[Tuple[int, int]]:
        """
        [v2.7 å…³é”®ä¿®å¤ç‰ˆ] æ£€æµ‹å€™é€‰åœé¡¿åŒºåŸŸ
        å…³é”®ä¿®å¤: å°†èƒ½é‡å’ŒF0çš„åˆ¤æ–­é€»è¾‘ä»â€œæˆ–â€æ”¹ä¸ºâ€œä¸â€ï¼Œç¡®ä¿ç›¸å¯¹èƒ½é‡é˜ˆå€¼é…ç½®ç”Ÿæ•ˆã€‚
        """
        enable_relative_mode = get_config('pure_vocal_detection.enable_relative_energy_mode', False)

        if enable_relative_mode:
            # --- ç›¸å¯¹èƒ½é‡æ¨¡å¼ ---
            logger.info("å¯ç”¨ç›¸å¯¹èƒ½é‡è°·æ£€æµ‹æ¨¡å¼...")
            peak_energy = np.max(features.rms_energy)
            avg_energy = np.mean(features.rms_energy)
            
            peak_ratio = get_config('pure_vocal_detection.peak_relative_threshold_ratio', 0.1)
            rms_ratio = get_config('pure_vocal_detection.rms_relative_threshold_ratio', 0.2)

            threshold_from_peak = peak_energy * peak_ratio
            threshold_from_rms = avg_energy * rms_ratio
            energy_threshold = min(threshold_from_peak, threshold_from_rms)

            logger.info(f"å…¨å±€èƒ½é‡åˆ†æ: å³°å€¼={peak_energy:.4f}, å¹³å‡å€¼={avg_energy:.4f}")
            logger.info(f"åŠ¨æ€èƒ½é‡é˜ˆå€¼: åŸºäºå³°å€¼({peak_ratio*100}%) -> {threshold_from_peak:.4f}, "
                       f"åŸºäºRMS({rms_ratio*100}%) -> {threshold_from_rms:.4f}")
            logger.info(f"æœ€ç»ˆèƒ½é‡è£å†³é˜ˆå€¼: {energy_threshold:.4f}")
            
            low_energy = features.rms_energy < energy_threshold
        else:
            # --- ä¼ ç»Ÿç»å¯¹dBæ¨¡å¼ ---
            logger.info("ä½¿ç”¨ç»å¯¹dBèƒ½é‡è°·æ£€æµ‹æ¨¡å¼...")
            energy_threshold_db = get_config('pure_vocal_detection.energy_threshold_db', -40)
            energy_db = librosa.amplitude_to_db(features.rms_energy, ref=np.max)
            low_energy = energy_db < energy_threshold_db

        # F0ä¸è¿ç»­æ£€æµ‹
        f0_drop_threshold = get_config('pure_vocal_detection.f0_drop_threshold', 0.7)
        f0_missing = features.f0_confidence < f0_drop_threshold
        
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨â€œä¸â€é€»è¾‘ (&)ï¼Œå¿…é¡»åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶
        pause_frames = low_energy & f0_missing
        
        # å¹³æ»‘å¤„ç†
        pause_frames = gaussian_filter1d(pause_frames.astype(float), sigma=3) > 0.5
        
        # æŸ¥æ‰¾è¿ç»­åŒºé—´ (ä¿æŒä¸å˜)
        candidates = []
        in_pause = False
        start_idx = 0
        
        min_duration_s = get_config('pure_vocal_detection.breath_duration_range', [0.1, 0.3])[0]

        for i, is_pause in enumerate(pause_frames):
            if is_pause and not in_pause:
                start_idx = i
                in_pause = True
            elif not is_pause and in_pause:
                duration = (i - start_idx) * self.hop_length / self.sample_rate
                if duration >= min_duration_s:
                    candidates.append((start_idx, i))
                in_pause = False
        
        if in_pause:
            duration = (len(pause_frames) - start_idx) * self.hop_length / self.sample_rate
            if duration >= min_duration_s:
                candidates.append((start_idx, len(pause_frames)))

        logger.info(f"æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰åœé¡¿åŒºåŸŸ (åŸºäº'ä¸'é€»è¾‘)")
        return candidates
    
    def _analyze_pause_features(self, candidates: List[Tuple[int, int]], 
                               features: VocalFeatures,
                               audio: np.ndarray) -> List[PureVocalPause]:
        """åˆ†æå€™é€‰åœé¡¿çš„ç‰¹å¾
        
        Args:
            candidates: å€™é€‰åœé¡¿åŒºé—´
            features: ç‰¹å¾æ•°æ®
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            åˆ†æåçš„åœé¡¿åˆ—è¡¨
        """
        analyzed_pauses = []
        
        for start_idx, end_idx in candidates:
            # æ—¶é—´ä¿¡æ¯
            start_time = start_idx * self.hop_length / self.sample_rate
            end_time = end_idx * self.hop_length / self.sample_rate
            duration = end_time - start_time
            
            # æå–åŒºé—´ç‰¹å¾
            pause_features = self._extract_pause_interval_features(
                features, start_idx, end_idx, audio
            )
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_pause_confidence(pause_features, duration)
            
            # åˆæ­¥åˆ†ç±»
            if duration <= self.breath_duration_range[1]:
                pause_type = 'breath'
            elif duration >= self.min_pause_duration:
                pause_type = 'true_pause'
            else:
                pause_type = 'uncertain'
            
            analyzed_pauses.append(PureVocalPause(
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                pause_type=pause_type,
                confidence=confidence,
                features=pause_features
            ))
        
        return analyzed_pauses
    
    def _extract_pause_interval_features(self, features: VocalFeatures,
                                        start_idx: int, end_idx: int,
                                        audio: np.ndarray) -> Dict:
        """æå–åœé¡¿åŒºé—´çš„è¯¦ç»†ç‰¹å¾
        
        Args:
            features: å…¨å±€ç‰¹å¾
            start_idx: å¼€å§‹å¸§ç´¢å¼•
            end_idx: ç»“æŸå¸§ç´¢å¼•
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            åŒºé—´ç‰¹å¾å­—å…¸
        """
        # å‰åæ–‡çª—å£(å‰åå„0.5ç§’)
        context_frames = int(0.5 * self.sample_rate / self.hop_length)
        pre_start = max(0, start_idx - context_frames)
        post_end = min(len(features.rms_energy), end_idx + context_frames)
        
        # F0ç‰¹å¾
        f0_drop_rate = 0.0
        if pre_start < start_idx:
            pre_f0 = np.nanmean(features.f0_contour[pre_start:start_idx])
            pause_f0 = np.nanmean(features.f0_contour[start_idx:end_idx])
            if not np.isnan(pre_f0) and not np.isnan(pause_f0) and pre_f0 > 0:
                f0_drop_rate = 1.0 - (pause_f0 / pre_f0)
        
        # èƒ½é‡ç‰¹å¾
        pre_energy = np.mean(features.rms_energy[pre_start:start_idx]) if pre_start < start_idx else 0
        pause_energy = np.mean(features.rms_energy[start_idx:end_idx])
        post_energy = np.mean(features.rms_energy[end_idx:post_end]) if end_idx < post_end else 0
        
        energy_drop = (pre_energy - pause_energy) / (pre_energy + 1e-10)
        energy_rise = (post_energy - pause_energy) / (pause_energy + 1e-10)
        
        # é¢‘è°±ç‰¹å¾
        centroid_shift = 0.0
        if pre_start < start_idx:
            pre_centroid = np.mean(features.spectral_centroid[pre_start:start_idx])
            pause_centroid = np.mean(features.spectral_centroid[start_idx:end_idx])
            centroid_shift = abs(pre_centroid - pause_centroid) / (pre_centroid + 1e-10)
        
        # è°æ³¢ç‰¹å¾
        harmonic_drop = 0.0
        if pre_start < start_idx:
            pre_harmonic = np.mean(features.harmonic_ratio[pre_start:start_idx])
            pause_harmonic = np.mean(features.harmonic_ratio[start_idx:end_idx])
            harmonic_drop = (pre_harmonic - pause_harmonic) / (pre_harmonic + 1e-10)
        
        # å…±æŒ¯å³°ç‰¹å¾
        formant_stability = []
        for formant_track in features.formant_energies:
            if len(formant_track) > end_idx:
                pause_formant = formant_track[start_idx:end_idx]
                stability = 1.0 - (np.std(pause_formant) / (np.mean(pause_formant) + 1e-10))
                formant_stability.append(stability)
        
        return {
            'f0_drop_rate': f0_drop_rate,
            'energy_drop': energy_drop,
            'energy_rise': energy_rise,
            'centroid_shift': centroid_shift,
            'harmonic_drop': harmonic_drop,
            'formant_stability': np.mean(formant_stability) if formant_stability else 0.5,
            'pre_energy': pre_energy,
            'pause_energy': pause_energy,
            'post_energy': post_energy
        }
    
    def _calculate_pause_confidence(self, features: Dict, duration: float) -> float:
        """è®¡ç®—åœé¡¿ç½®ä¿¡åº¦
        
        Args:
            features: åœé¡¿ç‰¹å¾
            duration: åœé¡¿æ—¶é•¿
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•°(0-1)
        """
        # F0å¾—åˆ†
        f0_score = min(1.0, features['f0_drop_rate'] / 0.5)  # 50%ä¸‹é™å¾—æ»¡åˆ†
        
        # èƒ½é‡å¾—åˆ†
        energy_score = min(1.0, features['energy_drop'] / 0.7)  # 70%ä¸‹é™å¾—æ»¡åˆ†
        
        # é¢‘è°±å¾—åˆ†
        spectral_score = min(1.0, features['centroid_shift'] / 0.3)  # 30%åç§»å¾—æ»¡åˆ†
        
        # æ—¶é•¿å¾—åˆ†
        if duration < self.breath_duration_range[1]:
            # æ¢æ°”æ—¶é•¿ï¼Œä½åˆ†
            duration_score = 0.3
        elif duration >= self.min_pause_duration:
            # çœŸåœé¡¿æ—¶é•¿ï¼Œé«˜åˆ†
            duration_score = min(1.0, duration / 1.0)  # 1ç§’å¾—æ»¡åˆ†
        else:
            # ä¸­é—´æ—¶é•¿ï¼Œä¸­ç­‰åˆ†
            duration_score = 0.5
        
        # åŠ æƒè®¡ç®—
        confidence = (
            self.f0_weight * f0_score +
            self.formant_weight * (1.0 - features.get('formant_stability', 0.5)) +
            self.spectral_weight * spectral_score +
            self.duration_weight * duration_score
        )
        
        # èƒ½é‡ä½œä¸ºé¢å¤–åŠ æˆ
        confidence = confidence * (0.7 + 0.3 * energy_score)
        
        return min(1.0, confidence)
    
    def _classify_and_filter(self, pauses: List[PureVocalPause]) -> List[PureVocalPause]:
        """åˆ†ç±»å¹¶è¿‡æ»¤åœé¡¿
        
        Args:
            pauses: å€™é€‰åœé¡¿åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„é«˜è´¨é‡åœé¡¿
        """
        filtered = []
        
        for pause in pauses:
            # æ ¹æ®ç½®ä¿¡åº¦é‡æ–°åˆ†ç±»
            if pause.confidence >= self.pause_confidence_threshold:
                pause.pause_type = 'true_pause'
                filtered.append(pause)
                logger.debug(f"çœŸåœé¡¿: {pause.start_time:.2f}-{pause.end_time:.2f}s, "
                           f"ç½®ä¿¡åº¦: {pause.confidence:.3f}")
            elif pause.confidence <= self.breath_confidence_threshold:
                pause.pause_type = 'breath'
                # è¿‡æ»¤æ‰æ¢æ°”
                logger.debug(f"è¿‡æ»¤æ¢æ°”: {pause.start_time:.2f}-{pause.end_time:.2f}s, "
                           f"ç½®ä¿¡åº¦: {pause.confidence:.3f}")
            else:
                # ä¸ç¡®å®šçš„æƒ…å†µï¼Œæ ¹æ®æ—¶é•¿å†³å®š
                if pause.duration >= self.min_pause_duration:
                    pause.pause_type = 'true_pause'
                    filtered.append(pause)
                    logger.debug(f"æ—¶é•¿åˆ¤å®šä¸ºåœé¡¿: {pause.start_time:.2f}-{pause.end_time:.2f}s")
                else:
                    logger.debug(f"è¿‡æ»¤ä¸ç¡®å®š: {pause.start_time:.2f}-{pause.end_time:.2f}s")
        
        # åˆå¹¶ç›¸é‚»åœé¡¿
        filtered = self._merge_adjacent_pauses(filtered)
        
        logger.info(f"åˆ†ç±»è¿‡æ»¤å®Œæˆ: {len(pauses)}ä¸ªå€™é€‰ -> {len(filtered)}ä¸ªé«˜è´¨é‡åœé¡¿")
        return filtered
    
    def _merge_adjacent_pauses(self, pauses: List[PureVocalPause], 
                              merge_threshold: float = 0.3) -> List[PureVocalPause]:
        """åˆå¹¶ç›¸é‚»çš„åœé¡¿
        
        Args:
            pauses: åœé¡¿åˆ—è¡¨
            merge_threshold: åˆå¹¶é˜ˆå€¼(ç§’)
            
        Returns:
            åˆå¹¶åçš„åœé¡¿åˆ—è¡¨
        """
        if not pauses:
            return pauses
        
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        pauses = sorted(pauses, key=lambda p: p.start_time)
        
        merged = []
        current = pauses[0]
        
        for next_pause in pauses[1:]:
            gap = next_pause.start_time - current.end_time
            
            if gap <= merge_threshold:
                # åˆå¹¶
                current = PureVocalPause(
                    start_time=current.start_time,
                    end_time=next_pause.end_time,
                    duration=next_pause.end_time - current.start_time,
                    pause_type='true_pause',
                    confidence=max(current.confidence, next_pause.confidence),
                    features={**current.features, **next_pause.features}
                )
            else:
                merged.append(current)
                current = next_pause
        
        merged.append(current)
        
        if len(merged) < len(pauses):
            logger.debug(f"åˆå¹¶ç›¸é‚»åœé¡¿: {len(pauses)} -> {len(merged)}")
        
        return merged
    
    def _calculate_harmonic_ratio_direct(self, audio: np.ndarray) -> np.ndarray:
        """ç›´æ¥è®¡ç®—çº¯äººå£°çš„è°æ³¢æ¯”ç‡
        
        Args:
            audio: çº¯äººå£°éŸ³é¢‘ä¿¡å·
            
        Returns:
            è°æ³¢æ¯”ç‡åºåˆ—
        """
        # è®¡ç®—çŸ­æ—¶è°±
        stft = librosa.stft(audio, hop_length=self.hop_length)
        magnitude = np.abs(stft)
        
        # è®¡ç®—è°æ³¢æ¯”ç‡ï¼šå‰1/3é¢‘æ®µèƒ½é‡ vs å2/3é¢‘æ®µèƒ½é‡
        n_bins = magnitude.shape[0]
        low_freq_energy = np.sum(magnitude[:n_bins//3, :], axis=0)
        high_freq_energy = np.sum(magnitude[n_bins//3:, :], axis=0)
        
        # è°æ³¢æ¯”ç‡ï¼šä½é¢‘èƒ½é‡å æ¯”ï¼ˆäººå£°ä¸»è¦åœ¨ä½é¢‘ï¼‰
        total_energy = low_freq_energy + high_freq_energy
        harmonic_ratio = low_freq_energy / (total_energy + 1e-10)
        
        return harmonic_ratio
    
    def _extract_formants(self, audio: np.ndarray) -> List[np.ndarray]:
        """æå–å…±æŒ¯å³°ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            å…±æŒ¯å³°èƒ½é‡åºåˆ—åˆ—è¡¨
        """
        # ä½¿ç”¨çº¿æ€§é¢„æµ‹ç¼–ç (LPC)åˆ†æå…±æŒ¯å³°
        frame_length = int(0.025 * self.sample_rate)  # 25msçª—å£
        hop_length = self.hop_length
        
        formant_tracks = [[] for _ in range(3)]  # F1, F2, F3
        
        for i in range(0, len(audio) - frame_length, hop_length):
            frame = audio[i:i + frame_length]
            
            if len(frame) < frame_length:
                break
                
            # é¢„åŠ é‡
            frame = np.append(frame[0], frame[1:] - 0.95 * frame[:-1])
            
            # LPCåˆ†æ
            try:
                # ä½¿ç”¨librosaçš„LPC
                lpc_coeffs = librosa.lpc(frame, order=12)
                
                # ä»LPCç³»æ•°è®¡ç®—é¢‘ç‡å“åº”
                w, h = signal.freqz(1, lpc_coeffs, worN=512, fs=self.sample_rate)
                
                # æ‰¾å³°å€¼ä½œä¸ºå…±æŒ¯å³°
                magnitude = np.abs(h)
                peaks, _ = signal.find_peaks(magnitude, height=np.max(magnitude) * 0.1)
                
                # å–å‰3ä¸ªå³°å€¼çš„é¢‘ç‡
                peak_freqs = w[peaks] if len(peaks) > 0 else []
                peak_mags = magnitude[peaks] if len(peaks) > 0 else []
                
                # æ’åºå¹¶åˆ†é…ç»™F1, F2, F3
                if len(peak_freqs) > 0:
                    sorted_indices = np.argsort(peak_freqs)
                    for j in range(min(3, len(sorted_indices))):
                        if j < len(peak_mags):
                            formant_tracks[j].append(peak_mags[sorted_indices[j]])
                        else:
                            formant_tracks[j].append(0.0)
                else:
                    for j in range(3):
                        formant_tracks[j].append(0.0)
                        
            except Exception as e:
                # LPCåˆ†æå¤±è´¥æ—¶å¡«å……é›¶å€¼
                for j in range(3):
                    formant_tracks[j].append(0.0)
        
        return [np.array(track) for track in formant_tracks]
    
    def _calculate_precise_cut_points(self, pure_vocal_pauses: List[PureVocalPause], 
                                    vocal_audio: np.ndarray) -> List[PureVocalPause]:
        """ä½¿ç”¨VocalPauseDetectorV2è®¡ç®—ç²¾ç¡®åˆ‡ç‚¹
        
        Args:
            pure_vocal_pauses: çº¯äººå£°åœé¡¿åˆ—è¡¨
            vocal_audio: çº¯äººå£°éŸ³é¢‘æ•°æ®
            
        Returns:
            åŒ…å«ç²¾ç¡®åˆ‡ç‚¹çš„åœé¡¿åˆ—è¡¨
        """
        logger.info(f"ğŸ”¥ ä½¿ç”¨èƒ½é‡è°·ç®—æ³•è®¡ç®— {len(pure_vocal_pauses)} ä¸ªåœé¡¿çš„ç²¾ç¡®åˆ‡ç‚¹...")
        
        # è½¬æ¢ä¸ºVocalPauseæ ¼å¼ä»¥ä½¿ç”¨èƒ½é‡è°·è®¡ç®—
        from .vocal_pause_detector import VocalPause
        vocal_pauses = []
        
        for i, pure_pause in enumerate(pure_vocal_pauses):
            vocal_pause = VocalPause(
                start_time=pure_pause.start_time,
                end_time=pure_pause.end_time, 
                duration=pure_pause.duration,
                position_type='middle',  # é»˜è®¤ä¸­é—´åœé¡¿
                confidence=pure_pause.confidence,
                cut_point=(pure_pause.start_time + pure_pause.end_time) / 2  # ä¸´æ—¶åˆ‡ç‚¹
            )
            vocal_pauses.append(vocal_pause)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè°ƒç”¨VocalPauseDetectorV2çš„èƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—ï¼Œä¼ å…¥vocal_audioä½œä¸ºwaveform
        try:
            vocal_pauses = self._cut_point_calculator._calculate_cut_points(
                vocal_pauses, 
                bpm_features=None,  # çº¯äººå£°æ¨¡å¼ä¸ä½¿ç”¨BPMå¯¹é½
                waveform=vocal_audio  # å…³é”®ï¼šä¼ é€’çº¯äººå£°éŸ³é¢‘æ•°æ®ç”¨äºèƒ½é‡è°·æ£€æµ‹
            )
            logger.info("âœ… èƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ èƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—å¤±è´¥: {e}") 
            logger.info("ä½¿ç”¨åœé¡¿ä¸­å¿ƒä½œä¸ºå…œåº•åˆ‡ç‚¹")
            for vocal_pause in vocal_pauses:
                vocal_pause.cut_point = (vocal_pause.start_time + vocal_pause.end_time) / 2
        
        # å°†ç»“æœæ˜ å°„å›PureVocalPause
        for i, (pure_pause, vocal_pause) in enumerate(zip(pure_vocal_pauses, vocal_pauses)):
            pure_pause.cut_point = vocal_pause.cut_point
            pure_pause.quality_grade = 'A' if hasattr(vocal_pause, 'cut_point') and vocal_pause.cut_point != (vocal_pause.start_time + vocal_pause.end_time) / 2 else 'B'
            logger.debug(f"åœé¡¿ {i+1}: [{pure_pause.start_time:.3f}s, {pure_pause.end_time:.3f}s] -> åˆ‡ç‚¹ {pure_pause.cut_point:.3f}s ({pure_pause.quality_grade})")
        
        return pure_vocal_pauses

    def _detect_energy_valleys(self, vocal_audio: np.ndarray, peak_ratio: float, rms_ratio: float) -> List[PureVocalPause]:
        """
        ğŸ”¥ ç›¸å¯¹èƒ½é‡è°·æ£€æµ‹ - è§£å†³é•¿éŸ³é¢‘åˆ†å‰²ä¸è¶³é—®é¢˜
        
        Args:
            vocal_audio: çº¯äººå£°éŸ³é¢‘
            peak_ratio: å³°å€¼èƒ½é‡æ¯”ç‡é˜ˆå€¼
            rms_ratio: RMSèƒ½é‡æ¯”ç‡é˜ˆå€¼
            
        Returns:
            æ£€æµ‹åˆ°çš„èƒ½é‡è°·åœé¡¿åˆ—è¡¨
        """
        logger.info(f"ğŸ”¥ ç›¸å¯¹èƒ½é‡è°·æ£€æµ‹: peak_ratio={peak_ratio}, rms_ratio={rms_ratio}")
        
        # 1. è®¡ç®—RMSèƒ½é‡åŒ…ç»œ
        frame_length = int(self.sample_rate * 0.025)  # 25ms
        hop_length = int(self.sample_rate * 0.01)     # 10ms
        rms_energy = librosa.feature.rms(y=vocal_audio, frame_length=frame_length, hop_length=hop_length)[0]
        
        # 2. è®¡ç®—åŠ¨æ€é˜ˆå€¼
        peak_energy = np.max(rms_energy)
        avg_energy = np.mean(rms_energy)
        peak_threshold = peak_energy * peak_ratio
        rms_threshold = avg_energy * rms_ratio
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼(å–è¾ƒå°å€¼)
        energy_threshold = min(peak_threshold, rms_threshold)
        logger.info(f"å³°å€¼èƒ½é‡: {peak_energy:.6f}, å¹³å‡èƒ½é‡: {avg_energy:.6f}")
        logger.info(f"èƒ½é‡è°·é˜ˆå€¼: {energy_threshold:.6f} (peak:{peak_threshold:.6f}, rms:{rms_threshold:.6f})")
        
        # 3. æ‰¾åˆ°ä½äºé˜ˆå€¼çš„åŒºåŸŸ
        low_energy_mask = rms_energy < energy_threshold
        time_frames = librosa.frames_to_time(np.arange(len(rms_energy)), sr=self.sample_rate, hop_length=hop_length)
        
        # 4. å°†è¿ç»­çš„ä½èƒ½é‡åŒºåŸŸåˆå¹¶
        pauses = []
        in_pause = False
        pause_start = 0.0
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¯¹äºèƒ½é‡è°·æ£€æµ‹ï¼Œä½¿ç”¨æ›´çŸ­çš„æœ€å°åœé¡¿æ—¶é•¿
        min_pause_duration = 0.2  # 200msï¼Œé€‚åˆéŸ³ä¹ä¸­çš„çŸ­æš‚åœé¡¿
        
        for i, (is_low, time) in enumerate(zip(low_energy_mask, time_frames)):
            if is_low and not in_pause:
                # å¼€å§‹æ–°çš„åœé¡¿
                pause_start = time
                in_pause = True
            elif not is_low and in_pause:
                # ç»“æŸå½“å‰åœé¡¿
                pause_end = time
                duration = pause_end - pause_start
                
                if duration >= min_pause_duration:
                    # è®¡ç®—åœé¡¿çš„å¹³å‡èƒ½é‡ä½œä¸ºç½®ä¿¡åº¦
                    start_frame = max(0, int(pause_start * self.sample_rate / hop_length))
                    end_frame = min(len(rms_energy), int(pause_end * self.sample_rate / hop_length))
                    
                    if start_frame < end_frame:
                        pause_energy = np.mean(rms_energy[start_frame:end_frame])
                        confidence = 1.0 - (pause_energy / energy_threshold)  # è¶Šä½èƒ½é‡ç½®ä¿¡åº¦è¶Šé«˜
                        confidence = max(0.1, min(0.95, confidence))
                        
                        pause = PureVocalPause(
                            start_time=pause_start,
                            end_time=pause_end,
                            duration=duration,
                            pause_type='energy_valley',
                            confidence=confidence,
                            features={'energy': pause_energy, 'threshold': energy_threshold},
                            cut_point=(pause_start + pause_end) / 2
                        )
                        pauses.append(pause)
                        logger.debug(f"èƒ½é‡è°·åœé¡¿: {pause_start:.3f}-{pause_end:.3f}s (æ—¶é•¿:{duration:.3f}s, ç½®ä¿¡åº¦:{confidence:.3f})")
                
                in_pause = False
        
        # å¤„ç†æ–‡ä»¶æœ«å°¾çš„åœé¡¿
        if in_pause:
            pause_end = time_frames[-1]
            duration = pause_end - pause_start
            if duration >= min_pause_duration:
                confidence = 0.8  # æœ«å°¾åœé¡¿ç»™äºˆè¾ƒé«˜ç½®ä¿¡åº¦
                pause = PureVocalPause(
                    start_time=pause_start,
                    end_time=pause_end,
                    duration=duration,
                    pause_type='energy_valley',
                    confidence=confidence,
                    features={'energy': 0.0, 'threshold': energy_threshold},
                    cut_point=(pause_start + pause_end) / 2
                )
                pauses.append(pause)
        
        logger.info(f"ğŸ”¥ èƒ½é‡è°·æ£€æµ‹å®Œæˆ: å‘ç°{len(pauses)}ä¸ªèƒ½é‡è°·åœé¡¿")
        return pauses

    def _apply_mdd_enhancement(self, pauses: List[PureVocalPause], original_audio: np.ndarray) -> List[PureVocalPause]:
        """
        ğŸ”¥ MDD (éŸ³ä¹åŠ¨æ€å¯†åº¦) å¢å¼ºå¤„ç†
        
        Args:
            pauses: åŸå§‹åœé¡¿åˆ—è¡¨
            original_audio: åŸå§‹æ··éŸ³éŸ³é¢‘
            
        Returns:
            MDDå¢å¼ºåçš„åœé¡¿åˆ—è¡¨
        """
        logger.info("ğŸ”¥ å¼€å§‹MDDå¢å¼ºå¤„ç†...")
        
        if not pauses:
            return pauses
            
        # 1. è®¡ç®—éŸ³ä¹åŠ¨æ€å¯†åº¦
        frame_length = int(self.sample_rate * 0.1)  # 100msçª—å£
        hop_length = int(self.sample_rate * 0.05)   # 50msè·³è·ƒ
        
        # RMSèƒ½é‡å¯†åº¦
        rms_energy = librosa.feature.rms(y=original_audio, frame_length=frame_length, hop_length=hop_length)[0]
        
        # é¢‘è°±å¹³å¦åº¦
        spectral_flatness = librosa.feature.spectral_flatness(y=original_audio, hop_length=hop_length)[0]
        
        # éŸ³ç¬¦èµ·å§‹æ£€æµ‹
        onset_frames = librosa.onset.onset_detect(y=original_audio, sr=self.sample_rate, hop_length=hop_length)
        onset_strength = librosa.onset.onset_strength(y=original_audio, sr=self.sample_rate, hop_length=hop_length)
        
        # æ—¶é—´è½´
        time_frames = librosa.frames_to_time(np.arange(len(rms_energy)), sr=self.sample_rate, hop_length=hop_length)
        
        # 2. è®¡ç®—MDDæŒ‡æ ‡æƒé‡
        energy_weight = get_config('musical_dynamic_density.energy_weight', 0.7)
        spectral_weight = get_config('musical_dynamic_density.spectral_weight', 0.3)
        onset_weight = get_config('musical_dynamic_density.onset_weight', 0.2)
        
        # 3. ä¸ºæ¯ä¸ªåœé¡¿è®¡ç®—MDDè¯„åˆ†
        enhanced_pauses = []
        threshold_multiplier = get_config('musical_dynamic_density.threshold_multiplier', 0.3)
        max_multiplier = get_config('musical_dynamic_density.max_multiplier', 1.4)
        min_multiplier = get_config('musical_dynamic_density.min_multiplier', 0.6)
        
        for pause in pauses:
            # æ‰¾åˆ°åœé¡¿å¯¹åº”çš„æ—¶é—´çª—å£
            start_frame = np.argmin(np.abs(time_frames - pause.start_time))
            end_frame = np.argmin(np.abs(time_frames - pause.end_time))
            
            if start_frame >= end_frame or start_frame >= len(rms_energy):
                enhanced_pauses.append(pause)
                continue
                
            # è®¡ç®—åœé¡¿å‘¨å›´çš„MDD
            window_start = max(0, start_frame - 10)  # æ‰©å±•çª—å£
            window_end = min(len(rms_energy), end_frame + 10)
            
            # RMSèƒ½é‡å¯†åº¦
            local_rms = np.mean(rms_energy[window_start:window_end])
            energy_score = local_rms / np.max(rms_energy) if np.max(rms_energy) > 0 else 0.0
            
            # é¢‘è°±å¹³å¦åº¦ (è¶Šå¹³å¦å¯†åº¦è¶Šä½)
            local_flatness = np.mean(spectral_flatness[window_start:window_end])
            spectral_score = 1.0 - local_flatness  # åè½¬ï¼Œå¯†åº¦è¶Šé«˜åˆ†æ•°è¶Šé«˜
            
            # éŸ³ç¬¦èµ·å§‹å¯†åº¦
            onset_count = np.sum((onset_frames >= window_start) & (onset_frames < window_end))
            onset_score = min(1.0, onset_count / 5.0)  # å½’ä¸€åŒ–åˆ°0-1
            
            # ç»¼åˆMDDè¯„åˆ†
            mdd_score = (energy_score * energy_weight + 
                        spectral_score * spectral_weight + 
                        onset_score * onset_weight)
            
            # æ ¹æ®MDDè°ƒæ•´åœé¡¿ç½®ä¿¡åº¦
            confidence_multiplier = 1.0 + (mdd_score * threshold_multiplier)
            confidence_multiplier = max(min_multiplier, min(max_multiplier, confidence_multiplier))
            
            # åˆ›å»ºå¢å¼ºçš„åœé¡¿
            enhanced_pause = PureVocalPause(
                start_time=pause.start_time,
                end_time=pause.end_time,
                duration=pause.duration,
                pause_type=f"{pause.pause_type}_mdd",
                confidence=pause.confidence * confidence_multiplier,
                features={**pause.features, 'mdd_score': mdd_score, 'confidence_multiplier': confidence_multiplier},
                cut_point=pause.cut_point,
                quality_grade=pause.quality_grade
            )
            enhanced_pauses.append(enhanced_pause)
            
            logger.debug(f"MDDå¢å¼º - åœé¡¿{pause.start_time:.2f}s: MDD={mdd_score:.3f}, ç½®ä¿¡åº¦å€æ•°={confidence_multiplier:.3f}")
        
        logger.info(f"ğŸ”¥ MDDå¢å¼ºå®Œæˆ: {len(enhanced_pauses)}ä¸ªåœé¡¿å·²ä¼˜åŒ–")
        return enhanced_pauses
