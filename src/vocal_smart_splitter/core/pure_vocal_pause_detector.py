#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# File: src/vocal_smart_splitter/core/pure_vocal_pause_detector.py
# AI-SUMMARY: çº¯äººå£°åœé¡¿æ£€æµ‹å™¨ - åŸºäºMDX23/Demucsåˆ†ç¦»åçš„çº¯äººå£°è¿›è¡Œå¤šç»´ç‰¹å¾åˆ†æï¼Œè§£å†³é«˜é¢‘æ¢æ°”è¯¯åˆ¤é—®é¢˜

import numpy as np
import librosa
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from scipy import signal
from scipy.ndimage import gaussian_filter1d

from ..utils.config_manager import get_config

logger = logging.getLogger(__name__)

@dataclass
class VocalFeatures:
    """äººå£°ç‰¹å¾æ•°æ®ç»“æ„"""
    f0_contour: np.ndarray           # åŸºé¢‘è½¨è¿¹
    f0_confidence: np.ndarray        # åŸºé¢‘ç½®ä¿¡åº¦
    formant_energies: List[np.ndarray]  # å…±æŒ¯å³°èƒ½é‡åºåˆ—
    spectral_centroid: np.ndarray    # é¢‘è°±è´¨å¿ƒ
    harmonic_ratio: np.ndarray       # è°æ³¢æ¯”ç‡
    zero_crossing_rate: np.ndarray   # è¿‡é›¶ç‡
    rms_energy: np.ndarray           # RMSèƒ½é‡

@dataclass
class PureVocalPause:
    """çº¯äººå£°åœé¡¿ç»“æ„"""
    start_time: float
    end_time: float
    duration: float
    pause_type: str  # 'true_pause', 'breath', 'uncertain'
    confidence: float
    features: Dict  # è¯¦ç»†ç‰¹å¾ä¿¡æ¯
    cut_point: float = 0.0  # æœ€ä½³åˆ‡å‰²ç‚¹ï¼ˆæ–°å¢ï¼‰
    quality_grade: str = 'B'  # è´¨é‡ç­‰çº§ï¼ˆæ–°å¢ï¼‰
    is_valid: bool = True   # æ˜¯å¦æœ‰æ•ˆï¼ˆæ–°å¢ï¼‰
    
class PureVocalPauseDetector:
    """åŸºäºçº¯äººå£°çš„å¤šç»´ç‰¹å¾åœé¡¿æ£€æµ‹å™¨
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. F0è¿ç»­æ€§åˆ†æ - æ£€æµ‹åŸºé¢‘çªå˜è¯†åˆ«çœŸåœé¡¿
    2. å…±æŒ¯å³°èƒ½é‡åˆ†å¸ƒ - åŒºåˆ†æ¢æ°”vsé™éŸ³
    3. é¢‘è°±è´¨å¿ƒè¿½è¸ª - è¯†åˆ«é«˜é¢‘è¡°å‡æ¨¡å¼
    4. è°æ³¢å¼ºåº¦åˆ†æ - è¯„ä¼°å‘å£°è´¨é‡
    """
    
    def __init__(self, sample_rate: int = 44100):
        """åˆå§‹åŒ–çº¯äººå£°åœé¡¿æ£€æµ‹å™¨
        
        Args:
            sample_rate: é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        
        # ä»é…ç½®åŠ è½½å‚æ•°
        self.min_pause_duration = get_config('pure_vocal_detection.min_pause_duration', 0.5)
        self.breath_duration_range = get_config('pure_vocal_detection.breath_duration_range', [0.1, 0.3])
        self.f0_weight = get_config('pure_vocal_detection.f0_weight', 0.3)
        self.formant_weight = get_config('pure_vocal_detection.formant_weight', 0.25)
        self.spectral_weight = get_config('pure_vocal_detection.spectral_weight', 0.25)
        self.duration_weight = get_config('pure_vocal_detection.duration_weight', 0.2)
        
        # æ£€æµ‹é˜ˆå€¼
        self.energy_threshold_db = get_config('pure_vocal_detection.energy_threshold_db', -40)
        self.f0_drop_threshold = get_config('pure_vocal_detection.f0_drop_threshold', 0.7)
        self.breath_confidence_threshold = get_config('pure_vocal_detection.breath_filter_threshold', 0.3)
        self.pause_confidence_threshold = get_config('pure_vocal_detection.pause_confidence_threshold', 0.7)
        
        # åˆ†æå‚æ•°
        self.hop_length = int(sample_rate * 0.01)  # 10ms hop
        self.frame_length = int(sample_rate * 0.025)  # 25ms frame
        self.n_fft = 2048
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šé›†æˆVocalPauseDetectorV2çš„èƒ½é‡è°·æ£€æµ‹èƒ½åŠ›
        from .vocal_pause_detector import VocalPauseDetectorV2
        self._cut_point_calculator = VocalPauseDetectorV2(sample_rate)
        
        logger.info(f"çº¯äººå£°åœé¡¿æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ (é‡‡æ ·ç‡: {sample_rate}) - å·²é›†æˆèƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—")
    
    def detect_pure_vocal_pauses(self, vocal_audio: np.ndarray, 
                                original_audio: Optional[np.ndarray] = None) -> List[PureVocalPause]:
        """æ£€æµ‹çº¯äººå£°ä¸­çš„åœé¡¿
        
        Args:
            vocal_audio: åˆ†ç¦»åçš„çº¯äººå£°éŸ³é¢‘
            original_audio: åŸå§‹æ··éŸ³(å¯é€‰ï¼Œç”¨äºå¯¹æ¯”)
            
        Returns:
            æ£€æµ‹åˆ°çš„åœé¡¿åˆ—è¡¨
        """
        logger.info("å¼€å§‹çº¯äººå£°åœé¡¿æ£€æµ‹...")
        
        # 1. æå–å¤šç»´ç‰¹å¾
        features = self._extract_vocal_features(vocal_audio)
        
        # 2. æ£€æµ‹å€™é€‰åœé¡¿åŒºåŸŸ
        candidate_pauses = self._detect_candidate_pauses(features)
        
        # 3. ç‰¹å¾èåˆåˆ†æ
        analyzed_pauses = self._analyze_pause_features(candidate_pauses, features, vocal_audio)
        
        # 4. åˆ†ç±»è¿‡æ»¤
        filtered_pauses = self._classify_and_filter(analyzed_pauses)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨VocalPauseDetectorV2è®¡ç®—ç²¾ç¡®åˆ‡ç‚¹
        if filtered_pauses and vocal_audio is not None:
            filtered_pauses = self._calculate_precise_cut_points(filtered_pauses, vocal_audio)
        
        logger.info(f"æ£€æµ‹å®Œæˆ: {len(filtered_pauses)}ä¸ªé«˜è´¨é‡åœé¡¿ç‚¹")
        return filtered_pauses
    
    def _extract_vocal_features(self, audio: np.ndarray) -> VocalFeatures:
        """æå–äººå£°å¤šç»´ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            æå–çš„ç‰¹å¾é›†åˆ
        """
        logger.debug("æå–äººå£°ç‰¹å¾...")
        
        # 1. åŸºé¢‘(F0)æå– - ä½¿ç”¨librosaçš„pyinç®—æ³•
        f0, voiced_flag, voiced_probs = librosa.pyin(
            audio, 
            fmin=librosa.note_to_hz('C2'),  # 65Hz
            fmax=librosa.note_to_hz('C7'),  # 2093Hz
            sr=self.sample_rate,
            hop_length=self.hop_length
        )
        
        # 2. å…±æŒ¯å³°åˆ†æ - ä½¿ç”¨LPCåˆ†æ
        formant_energies = self._extract_formants(audio)
        
        # 3. é¢‘è°±è´¨å¿ƒ
        spectral_centroid = librosa.feature.spectral_centroid(
            y=audio, sr=self.sample_rate, hop_length=self.hop_length
        )[0]
        
        # 4. è°æ³¢åˆ†æ - åŸºäºçº¯äººå£°ä¿¡å·ï¼Œæ— éœ€å†åˆ†ç¦»
        harmonic_ratio = self._calculate_harmonic_ratio_direct(audio)
        
        # 5. è¿‡é›¶ç‡
        zero_crossing_rate = librosa.feature.zero_crossing_rate(
            audio, hop_length=self.hop_length
        )[0]
        
        # 6. RMSèƒ½é‡
        rms_energy = librosa.feature.rms(
            y=audio, hop_length=self.hop_length
        )[0]
        
        return VocalFeatures(
            f0_contour=f0,
            f0_confidence=voiced_probs,
            formant_energies=formant_energies,
            spectral_centroid=spectral_centroid,
            harmonic_ratio=harmonic_ratio,
            zero_crossing_rate=zero_crossing_rate,
            rms_energy=rms_energy
        )
    
    def _extract_formants(self, audio: np.ndarray, n_formants: int = 3) -> List[np.ndarray]:
        """æå–å…±æŒ¯å³°èƒ½é‡
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            n_formants: è¦æå–çš„å…±æŒ¯å³°æ•°é‡
            
        Returns:
            å…±æŒ¯å³°èƒ½é‡åºåˆ—
        """
        formants = []
        
        # åˆ†å¸§å¤„ç†
        frames = librosa.util.frame(audio, frame_length=self.frame_length, 
                                   hop_length=self.hop_length)
        
        for frame in frames.T:
            # LPCåˆ†æ
            try:
                # ä½¿ç”¨è‡ªç›¸å…³æ–¹æ³•ä¼°è®¡LPCç³»æ•°
                lpc_order = 2 + self.sample_rate // 1000  # ç»éªŒå…¬å¼
                a = librosa.lpc(frame, order=min(lpc_order, len(frame) - 1))
                
                # ä»LPCç³»æ•°æå–å…±æŒ¯å³°é¢‘ç‡
                roots = np.roots(a)
                roots = roots[np.imag(roots) >= 0]  # åªä¿ç•™æ­£é¢‘ç‡
                
                # è½¬æ¢ä¸ºé¢‘ç‡
                angles = np.angle(roots)
                freqs = angles * self.sample_rate / (2 * np.pi)
                
                # æ’åºå¹¶é€‰æ‹©å‰nä¸ªå…±æŒ¯å³°
                freqs = sorted(freqs[freqs > 0])[:n_formants]
                
                # å¦‚æœå…±æŒ¯å³°æ•°é‡ä¸è¶³ï¼Œå¡«å……é›¶
                while len(freqs) < n_formants:
                    freqs.append(0)
                    
                formants.append(freqs)
            except:
                # LPCå¤±è´¥æ—¶å¡«å……é›¶
                formants.append([0] * n_formants)
        
        # è½¬ç½®å¾—åˆ°æ¯ä¸ªå…±æŒ¯å³°çš„æ—¶é—´åºåˆ—
        formants = np.array(formants).T
        return [formants[i] for i in range(n_formants)]
    
    def _calculate_harmonic_ratio(self, harmonic: np.ndarray, 
                                 original: np.ndarray) -> np.ndarray:
        """è®¡ç®—è°æ³¢æ¯”ç‡
        
        Args:
            harmonic: è°æ³¢æˆåˆ†
            original: åŸå§‹ä¿¡å·
            
        Returns:
            è°æ³¢æ¯”ç‡æ—¶é—´åºåˆ—
        """
        # è®¡ç®—èƒ½é‡æ¯”
        harmonic_rms = librosa.feature.rms(y=harmonic, hop_length=self.hop_length)[0]
        original_rms = librosa.feature.rms(y=original, hop_length=self.hop_length)[0]
        
        # é¿å…é™¤é›¶
        ratio = np.zeros_like(harmonic_rms)
        non_zero = original_rms > 1e-10
        ratio[non_zero] = harmonic_rms[non_zero] / original_rms[non_zero]
        
        return ratio
    
    def _detect_candidate_pauses(self, features: VocalFeatures) -> List[Tuple[int, int]]:
        """æ£€æµ‹å€™é€‰åœé¡¿åŒºåŸŸ
        
        Args:
            features: æå–çš„ç‰¹å¾
            
        Returns:
            å€™é€‰åœé¡¿çš„å¸§ç´¢å¼•åŒºé—´åˆ—è¡¨
        """
        # èƒ½é‡é˜ˆå€¼æ£€æµ‹
        energy_db = librosa.amplitude_to_db(features.rms_energy, ref=np.max)
        low_energy = energy_db < self.energy_threshold_db
        
        # F0ä¸è¿ç»­æ£€æµ‹
        f0_missing = features.f0_confidence < self.f0_drop_threshold
        
        # ç»„åˆæ¡ä»¶
        pause_frames = low_energy | f0_missing
        
        # å¹³æ»‘å¤„ç†
        pause_frames = gaussian_filter1d(pause_frames.astype(float), sigma=3) > 0.5
        
        # æŸ¥æ‰¾è¿ç»­åŒºé—´
        candidates = []
        in_pause = False
        start_idx = 0
        
        for i, is_pause in enumerate(pause_frames):
            if is_pause and not in_pause:
                start_idx = i
                in_pause = True
            elif not is_pause and in_pause:
                # è½¬æ¢ä¸ºæ—¶é—´å¹¶æ£€æŸ¥æœ€å°æ—¶é•¿
                duration = (i - start_idx) * self.hop_length / self.sample_rate
                if duration >= self.breath_duration_range[0]:  # è‡³å°‘è¾¾åˆ°æ¢æ°”æœ€å°æ—¶é•¿
                    candidates.append((start_idx, i))
                in_pause = False
        
        # å¤„ç†æœ«å°¾
        if in_pause:
            duration = (len(pause_frames) - start_idx) * self.hop_length / self.sample_rate
            if duration >= self.breath_duration_range[0]:
                candidates.append((start_idx, len(pause_frames)))
        
        logger.debug(f"æ£€æµ‹åˆ°{len(candidates)}ä¸ªå€™é€‰åœé¡¿åŒºåŸŸ")
        return candidates
    
    def _analyze_pause_features(self, candidates: List[Tuple[int, int]], 
                               features: VocalFeatures,
                               audio: np.ndarray) -> List[PureVocalPause]:
        """åˆ†æå€™é€‰åœé¡¿çš„ç‰¹å¾
        
        Args:
            candidates: å€™é€‰åœé¡¿åŒºé—´
            features: ç‰¹å¾æ•°æ®
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            åˆ†æåçš„åœé¡¿åˆ—è¡¨
        """
        analyzed_pauses = []
        
        for start_idx, end_idx in candidates:
            # æ—¶é—´ä¿¡æ¯
            start_time = start_idx * self.hop_length / self.sample_rate
            end_time = end_idx * self.hop_length / self.sample_rate
            duration = end_time - start_time
            
            # æå–åŒºé—´ç‰¹å¾
            pause_features = self._extract_pause_interval_features(
                features, start_idx, end_idx, audio
            )
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_pause_confidence(pause_features, duration)
            
            # åˆæ­¥åˆ†ç±»
            if duration <= self.breath_duration_range[1]:
                pause_type = 'breath'
            elif duration >= self.min_pause_duration:
                pause_type = 'true_pause'
            else:
                pause_type = 'uncertain'
            
            analyzed_pauses.append(PureVocalPause(
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                pause_type=pause_type,
                confidence=confidence,
                features=pause_features
            ))
        
        return analyzed_pauses
    
    def _extract_pause_interval_features(self, features: VocalFeatures,
                                        start_idx: int, end_idx: int,
                                        audio: np.ndarray) -> Dict:
        """æå–åœé¡¿åŒºé—´çš„è¯¦ç»†ç‰¹å¾
        
        Args:
            features: å…¨å±€ç‰¹å¾
            start_idx: å¼€å§‹å¸§ç´¢å¼•
            end_idx: ç»“æŸå¸§ç´¢å¼•
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            åŒºé—´ç‰¹å¾å­—å…¸
        """
        # å‰åæ–‡çª—å£(å‰åå„0.5ç§’)
        context_frames = int(0.5 * self.sample_rate / self.hop_length)
        pre_start = max(0, start_idx - context_frames)
        post_end = min(len(features.rms_energy), end_idx + context_frames)
        
        # F0ç‰¹å¾
        f0_drop_rate = 0.0
        if pre_start < start_idx:
            pre_f0 = np.nanmean(features.f0_contour[pre_start:start_idx])
            pause_f0 = np.nanmean(features.f0_contour[start_idx:end_idx])
            if not np.isnan(pre_f0) and not np.isnan(pause_f0) and pre_f0 > 0:
                f0_drop_rate = 1.0 - (pause_f0 / pre_f0)
        
        # èƒ½é‡ç‰¹å¾
        pre_energy = np.mean(features.rms_energy[pre_start:start_idx]) if pre_start < start_idx else 0
        pause_energy = np.mean(features.rms_energy[start_idx:end_idx])
        post_energy = np.mean(features.rms_energy[end_idx:post_end]) if end_idx < post_end else 0
        
        energy_drop = (pre_energy - pause_energy) / (pre_energy + 1e-10)
        energy_rise = (post_energy - pause_energy) / (pause_energy + 1e-10)
        
        # é¢‘è°±ç‰¹å¾
        centroid_shift = 0.0
        if pre_start < start_idx:
            pre_centroid = np.mean(features.spectral_centroid[pre_start:start_idx])
            pause_centroid = np.mean(features.spectral_centroid[start_idx:end_idx])
            centroid_shift = abs(pre_centroid - pause_centroid) / (pre_centroid + 1e-10)
        
        # è°æ³¢ç‰¹å¾
        harmonic_drop = 0.0
        if pre_start < start_idx:
            pre_harmonic = np.mean(features.harmonic_ratio[pre_start:start_idx])
            pause_harmonic = np.mean(features.harmonic_ratio[start_idx:end_idx])
            harmonic_drop = (pre_harmonic - pause_harmonic) / (pre_harmonic + 1e-10)
        
        # å…±æŒ¯å³°ç‰¹å¾
        formant_stability = []
        for formant_track in features.formant_energies:
            if len(formant_track) > end_idx:
                pause_formant = formant_track[start_idx:end_idx]
                stability = 1.0 - (np.std(pause_formant) / (np.mean(pause_formant) + 1e-10))
                formant_stability.append(stability)
        
        return {
            'f0_drop_rate': f0_drop_rate,
            'energy_drop': energy_drop,
            'energy_rise': energy_rise,
            'centroid_shift': centroid_shift,
            'harmonic_drop': harmonic_drop,
            'formant_stability': np.mean(formant_stability) if formant_stability else 0.5,
            'pre_energy': pre_energy,
            'pause_energy': pause_energy,
            'post_energy': post_energy
        }
    
    def _calculate_pause_confidence(self, features: Dict, duration: float) -> float:
        """è®¡ç®—åœé¡¿ç½®ä¿¡åº¦
        
        Args:
            features: åœé¡¿ç‰¹å¾
            duration: åœé¡¿æ—¶é•¿
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•°(0-1)
        """
        # F0å¾—åˆ†
        f0_score = min(1.0, features['f0_drop_rate'] / 0.5)  # 50%ä¸‹é™å¾—æ»¡åˆ†
        
        # èƒ½é‡å¾—åˆ†
        energy_score = min(1.0, features['energy_drop'] / 0.7)  # 70%ä¸‹é™å¾—æ»¡åˆ†
        
        # é¢‘è°±å¾—åˆ†
        spectral_score = min(1.0, features['centroid_shift'] / 0.3)  # 30%åç§»å¾—æ»¡åˆ†
        
        # æ—¶é•¿å¾—åˆ†
        if duration < self.breath_duration_range[1]:
            # æ¢æ°”æ—¶é•¿ï¼Œä½åˆ†
            duration_score = 0.3
        elif duration >= self.min_pause_duration:
            # çœŸåœé¡¿æ—¶é•¿ï¼Œé«˜åˆ†
            duration_score = min(1.0, duration / 1.0)  # 1ç§’å¾—æ»¡åˆ†
        else:
            # ä¸­é—´æ—¶é•¿ï¼Œä¸­ç­‰åˆ†
            duration_score = 0.5
        
        # åŠ æƒè®¡ç®—
        confidence = (
            self.f0_weight * f0_score +
            self.formant_weight * (1.0 - features.get('formant_stability', 0.5)) +
            self.spectral_weight * spectral_score +
            self.duration_weight * duration_score
        )
        
        # èƒ½é‡ä½œä¸ºé¢å¤–åŠ æˆ
        confidence = confidence * (0.7 + 0.3 * energy_score)
        
        return min(1.0, confidence)
    
    def _classify_and_filter(self, pauses: List[PureVocalPause]) -> List[PureVocalPause]:
        """åˆ†ç±»å¹¶è¿‡æ»¤åœé¡¿
        
        Args:
            pauses: å€™é€‰åœé¡¿åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„é«˜è´¨é‡åœé¡¿
        """
        filtered = []
        
        for pause in pauses:
            # æ ¹æ®ç½®ä¿¡åº¦é‡æ–°åˆ†ç±»
            if pause.confidence >= self.pause_confidence_threshold:
                pause.pause_type = 'true_pause'
                filtered.append(pause)
                logger.debug(f"çœŸåœé¡¿: {pause.start_time:.2f}-{pause.end_time:.2f}s, "
                           f"ç½®ä¿¡åº¦: {pause.confidence:.3f}")
            elif pause.confidence <= self.breath_confidence_threshold:
                pause.pause_type = 'breath'
                # è¿‡æ»¤æ‰æ¢æ°”
                logger.debug(f"è¿‡æ»¤æ¢æ°”: {pause.start_time:.2f}-{pause.end_time:.2f}s, "
                           f"ç½®ä¿¡åº¦: {pause.confidence:.3f}")
            else:
                # ä¸ç¡®å®šçš„æƒ…å†µï¼Œæ ¹æ®æ—¶é•¿å†³å®š
                if pause.duration >= self.min_pause_duration:
                    pause.pause_type = 'true_pause'
                    filtered.append(pause)
                    logger.debug(f"æ—¶é•¿åˆ¤å®šä¸ºåœé¡¿: {pause.start_time:.2f}-{pause.end_time:.2f}s")
                else:
                    logger.debug(f"è¿‡æ»¤ä¸ç¡®å®š: {pause.start_time:.2f}-{pause.end_time:.2f}s")
        
        # åˆå¹¶ç›¸é‚»åœé¡¿
        filtered = self._merge_adjacent_pauses(filtered)
        
        logger.info(f"åˆ†ç±»è¿‡æ»¤å®Œæˆ: {len(pauses)}ä¸ªå€™é€‰ -> {len(filtered)}ä¸ªé«˜è´¨é‡åœé¡¿")
        return filtered
    
    def _merge_adjacent_pauses(self, pauses: List[PureVocalPause], 
                              merge_threshold: float = 0.3) -> List[PureVocalPause]:
        """åˆå¹¶ç›¸é‚»çš„åœé¡¿
        
        Args:
            pauses: åœé¡¿åˆ—è¡¨
            merge_threshold: åˆå¹¶é˜ˆå€¼(ç§’)
            
        Returns:
            åˆå¹¶åçš„åœé¡¿åˆ—è¡¨
        """
        if not pauses:
            return pauses
        
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        pauses = sorted(pauses, key=lambda p: p.start_time)
        
        merged = []
        current = pauses[0]
        
        for next_pause in pauses[1:]:
            gap = next_pause.start_time - current.end_time
            
            if gap <= merge_threshold:
                # åˆå¹¶
                current = PureVocalPause(
                    start_time=current.start_time,
                    end_time=next_pause.end_time,
                    duration=next_pause.end_time - current.start_time,
                    pause_type='true_pause',
                    confidence=max(current.confidence, next_pause.confidence),
                    features={**current.features, **next_pause.features}
                )
            else:
                merged.append(current)
                current = next_pause
        
        merged.append(current)
        
        if len(merged) < len(pauses):
            logger.debug(f"åˆå¹¶ç›¸é‚»åœé¡¿: {len(pauses)} -> {len(merged)}")
        
        return merged
    
    def _calculate_harmonic_ratio_direct(self, audio: np.ndarray) -> np.ndarray:
        """ç›´æ¥è®¡ç®—çº¯äººå£°çš„è°æ³¢æ¯”ç‡
        
        Args:
            audio: çº¯äººå£°éŸ³é¢‘ä¿¡å·
            
        Returns:
            è°æ³¢æ¯”ç‡åºåˆ—
        """
        # è®¡ç®—çŸ­æ—¶è°±
        stft = librosa.stft(audio, hop_length=self.hop_length)
        magnitude = np.abs(stft)
        
        # è®¡ç®—è°æ³¢æ¯”ç‡ï¼šå‰1/3é¢‘æ®µèƒ½é‡ vs å2/3é¢‘æ®µèƒ½é‡
        n_bins = magnitude.shape[0]
        low_freq_energy = np.sum(magnitude[:n_bins//3, :], axis=0)
        high_freq_energy = np.sum(magnitude[n_bins//3:, :], axis=0)
        
        # è°æ³¢æ¯”ç‡ï¼šä½é¢‘èƒ½é‡å æ¯”ï¼ˆäººå£°ä¸»è¦åœ¨ä½é¢‘ï¼‰
        total_energy = low_freq_energy + high_freq_energy
        harmonic_ratio = low_freq_energy / (total_energy + 1e-10)
        
        return harmonic_ratio
    
    def _extract_formants(self, audio: np.ndarray) -> List[np.ndarray]:
        """æå–å…±æŒ¯å³°ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·
            
        Returns:
            å…±æŒ¯å³°èƒ½é‡åºåˆ—åˆ—è¡¨
        """
        # ä½¿ç”¨çº¿æ€§é¢„æµ‹ç¼–ç (LPC)åˆ†æå…±æŒ¯å³°
        frame_length = int(0.025 * self.sample_rate)  # 25msçª—å£
        hop_length = self.hop_length
        
        formant_tracks = [[] for _ in range(3)]  # F1, F2, F3
        
        for i in range(0, len(audio) - frame_length, hop_length):
            frame = audio[i:i + frame_length]
            
            if len(frame) < frame_length:
                break
                
            # é¢„åŠ é‡
            frame = np.append(frame[0], frame[1:] - 0.95 * frame[:-1])
            
            # LPCåˆ†æ
            try:
                # ä½¿ç”¨librosaçš„LPC
                lpc_coeffs = librosa.lpc(frame, order=12)
                
                # ä»LPCç³»æ•°è®¡ç®—é¢‘ç‡å“åº”
                w, h = signal.freqz(1, lpc_coeffs, worN=512, fs=self.sample_rate)
                
                # æ‰¾å³°å€¼ä½œä¸ºå…±æŒ¯å³°
                magnitude = np.abs(h)
                peaks, _ = signal.find_peaks(magnitude, height=np.max(magnitude) * 0.1)
                
                # å–å‰3ä¸ªå³°å€¼çš„é¢‘ç‡
                peak_freqs = w[peaks] if len(peaks) > 0 else []
                peak_mags = magnitude[peaks] if len(peaks) > 0 else []
                
                # æ’åºå¹¶åˆ†é…ç»™F1, F2, F3
                if len(peak_freqs) > 0:
                    sorted_indices = np.argsort(peak_freqs)
                    for j in range(min(3, len(sorted_indices))):
                        if j < len(peak_mags):
                            formant_tracks[j].append(peak_mags[sorted_indices[j]])
                        else:
                            formant_tracks[j].append(0.0)
                else:
                    for j in range(3):
                        formant_tracks[j].append(0.0)
                        
            except Exception as e:
                # LPCåˆ†æå¤±è´¥æ—¶å¡«å……é›¶å€¼
                for j in range(3):
                    formant_tracks[j].append(0.0)
        
        return [np.array(track) for track in formant_tracks]
    
    def _calculate_precise_cut_points(self, pure_vocal_pauses: List[PureVocalPause], 
                                    vocal_audio: np.ndarray) -> List[PureVocalPause]:
        """ä½¿ç”¨VocalPauseDetectorV2è®¡ç®—ç²¾ç¡®åˆ‡ç‚¹
        
        Args:
            pure_vocal_pauses: çº¯äººå£°åœé¡¿åˆ—è¡¨
            vocal_audio: çº¯äººå£°éŸ³é¢‘æ•°æ®
            
        Returns:
            åŒ…å«ç²¾ç¡®åˆ‡ç‚¹çš„åœé¡¿åˆ—è¡¨
        """
        logger.info(f"ğŸ”¥ ä½¿ç”¨èƒ½é‡è°·ç®—æ³•è®¡ç®— {len(pure_vocal_pauses)} ä¸ªåœé¡¿çš„ç²¾ç¡®åˆ‡ç‚¹...")
        
        # è½¬æ¢ä¸ºVocalPauseæ ¼å¼ä»¥ä½¿ç”¨èƒ½é‡è°·è®¡ç®—
        from .vocal_pause_detector import VocalPause
        vocal_pauses = []
        
        for i, pure_pause in enumerate(pure_vocal_pauses):
            vocal_pause = VocalPause(
                start_time=pure_pause.start_time,
                end_time=pure_pause.end_time, 
                duration=pure_pause.duration,
                position_type='middle',  # é»˜è®¤ä¸­é—´åœé¡¿
                confidence=pure_pause.confidence,
                cut_point=(pure_pause.start_time + pure_pause.end_time) / 2  # ä¸´æ—¶åˆ‡ç‚¹
            )
            vocal_pauses.append(vocal_pause)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè°ƒç”¨VocalPauseDetectorV2çš„èƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—ï¼Œä¼ å…¥vocal_audioä½œä¸ºwaveform
        try:
            vocal_pauses = self._cut_point_calculator._calculate_cut_points(
                vocal_pauses, 
                bpm_features=None,  # çº¯äººå£°æ¨¡å¼ä¸ä½¿ç”¨BPMå¯¹é½
                waveform=vocal_audio  # å…³é”®ï¼šä¼ é€’çº¯äººå£°éŸ³é¢‘æ•°æ®ç”¨äºèƒ½é‡è°·æ£€æµ‹
            )
            logger.info("âœ… èƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ èƒ½é‡è°·åˆ‡ç‚¹è®¡ç®—å¤±è´¥: {e}") 
            logger.info("ä½¿ç”¨åœé¡¿ä¸­å¿ƒä½œä¸ºå…œåº•åˆ‡ç‚¹")
            for vocal_pause in vocal_pauses:
                vocal_pause.cut_point = (vocal_pause.start_time + vocal_pause.end_time) / 2
        
        # å°†ç»“æœæ˜ å°„å›PureVocalPause
        for i, (pure_pause, vocal_pause) in enumerate(zip(pure_vocal_pauses, vocal_pauses)):
            pure_pause.cut_point = vocal_pause.cut_point
            pure_pause.quality_grade = 'A' if hasattr(vocal_pause, 'cut_point') and vocal_pause.cut_point != (vocal_pause.start_time + vocal_pause.end_time) / 2 else 'B'
            logger.debug(f"åœé¡¿ {i+1}: [{pure_pause.start_time:.3f}s, {pure_pause.end_time:.3f}s] -> åˆ‡ç‚¹ {pure_pause.cut_point:.3f}s ({pure_pause.quality_grade})")
        
        return pure_vocal_pauses