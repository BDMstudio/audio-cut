#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# vocal_smart_splitter/core/vocal_pause_detector.py

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from ..utils.config_manager import get_config
from ..utils.adaptive_parameter_calculator import create_adaptive_calculator, AdaptiveParameters

logger = logging.getLogger(__name__)

try:
    from .adaptive_vad_enhancer import AdaptiveVADEnhancer, BPMFeatures
    ADAPTIVE_VAD_AVAILABLE = True
    logger.info("è‡ªé€‚åº”VADå¢å¼ºå™¨å¯ç”¨")
except ImportError as e:
    logger.warning(f"è‡ªé€‚åº”VADå¢å¼ºå™¨ä¸å¯ç”¨: {e}")
    ADAPTIVE_VAD_AVAILABLE = False
    BPMFeatures = None

@dataclass
class VocalPause:
    start_time: float
    end_time: float
    duration: float
    position_type: str
    confidence: float
    cut_point: float

class VocalPauseDetectorV2:
    """[v2.9 ç»ˆæä¿®å¤ç‰ˆ] æ”¹è¿›çš„äººå£°åœé¡¿æ£€æµ‹å™¨ - é›†æˆBPMè‡ªé€‚åº”èƒ½åŠ›"""

    def __init__(self, sample_rate: int = 44100):
        self.sample_rate = sample_rate
        self.adaptive_calculator = create_adaptive_calculator()
        self.current_adaptive_params: Optional[AdaptiveParameters] = None
        self.head_offset = get_config('vocal_pause_splitting.head_offset', -0.5)
        self.tail_offset = get_config('vocal_pause_splitting.tail_offset', 0.5)
        
        self.enable_bpm_adaptation = get_config('vocal_pause_splitting.enable_bpm_adaptation', True) and ADAPTIVE_VAD_AVAILABLE
        if self.enable_bpm_adaptation:
            self.adaptive_enhancer = AdaptiveVADEnhancer(sample_rate)
            logger.info("BPMè‡ªé€‚åº”å¢å¼ºå™¨å·²å¯ç”¨")
        else:
            self.adaptive_enhancer = None
            logger.info("BPMè‡ªé€‚åº”å·²ç¦ç”¨æˆ–ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å›ºå®šé˜ˆå€¼æ¨¡å¼")

        self._init_silero_vad()
        logger.info(f"VocalPauseDetectorV2 åˆå§‹åŒ–å®Œæˆ (SR: {sample_rate})")

    def _init_silero_vad(self):
        try:
            import torch
            torch.set_num_threads(1)
            self.vad_model, self.vad_utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=False)
            (self.get_speech_timestamps, _, _, _, _) = self.vad_utils
            logger.info("Silero VADæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.vad_model = None
            logger.error(f"Silero VADåˆå§‹åŒ–å¤±è´¥: {e}")

    def detect_vocal_pauses(self, detection_target_audio: np.ndarray, context_audio: Optional[np.ndarray] = None) -> List[VocalPause]:
        """
        ä¸»æ£€æµ‹æµç¨‹ï¼ŒåŒæ—¶ä½¿ç”¨èƒŒæ™¯éŸ³é¢‘å’Œç›®æ ‡éŸ³é¢‘ã€‚
        
        Args:
            detection_target_audio: ç”¨äºç²¾ç»†æ£€æµ‹çš„éŸ³é¢‘ (å¦‚: vocal_track)
            context_audio: ç”¨äºéŸ³ä¹èƒŒæ™¯åˆ†æçš„éŸ³é¢‘ (å¦‚: original_audio)
        """
        logger.info("å¼€å§‹BPMæ„ŸçŸ¥çš„äººå£°åœé¡¿æ£€æµ‹...")
        if self.vad_model is None:
            logger.error("Silero VADæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç»§ç»­")
            return []

        # å¦‚æœæ²¡æœ‰æä¾›èƒŒæ™¯éŸ³é¢‘ï¼Œåˆ™ä½¿ç”¨ç›®æ ‡éŸ³é¢‘è¿›è¡Œåˆ†æï¼ˆå…¼å®¹æ—§çš„smart_splitæ¨¡å¼ï¼‰
        if context_audio is None:
            context_audio = detection_target_audio
            logger.info("æœªæä¾›èƒŒæ™¯éŸ³é¢‘ï¼Œå°†åœ¨ç›®æ ‡éŸ³é¢‘ä¸Šè¿›è¡ŒéŸ³ä¹åˆ†æã€‚")

        bpm_features = None
        if self.enable_bpm_adaptation and self.adaptive_enhancer:
            logger.info("æ­¥éª¤ 1/5: åœ¨[èƒŒæ™¯éŸ³é¢‘]ä¸Šæ‰§è¡ŒBPMå’Œç¼–æ›²å¤æ‚åº¦åˆ†æ...")
            complexity_segments, bpm_features = self.adaptive_enhancer.analyze_arrangement_complexity(context_audio)
            if bpm_features:
                logger.info(f"ğŸµ éŸ³ä¹åˆ†æå®Œæˆ: {float(bpm_features.main_bpm):.1f} BPM ({bpm_features.bpm_category})")
                instrument_analyzer = getattr(self.adaptive_enhancer, 'instrument_analyzer', None)
                if instrument_analyzer:
                    instrument_complexity = instrument_analyzer.analyze_instrument_complexity(context_audio)
                    self.current_adaptive_params = self.adaptive_calculator.calculate_all_parameters(
                        float(bpm_features.main_bpm), float(instrument_complexity.get('overall_complexity', 0.5)), int(instrument_complexity.get('instrument_count', 3))
                    )
        
        logger.info("æ­¥éª¤ 2/5: åœ¨[ç›®æ ‡éŸ³é¢‘]ä¸Šä½¿ç”¨è‡ªé€‚åº”å‚æ•°è¿›è¡ŒVADè¯­éŸ³æ£€æµ‹...")
        speech_timestamps = self._detect_speech_timestamps(detection_target_audio)

        logger.info("æ­¥éª¤ 3/5: è®¡ç®—è¯­éŸ³é—´çš„åœé¡¿åŒºåŸŸ...")
        pause_segments = self._calculate_pause_segments(speech_timestamps, len(detection_target_audio))

        logger.info("æ­¥éª¤ 4/5: ä½¿ç”¨åŠ¨æ€é˜ˆå€¼è¿‡æ»¤æœ‰æ•ˆåœé¡¿...")
        valid_pauses = self._filter_adaptive_pauses(pause_segments, bpm_features)
        
        logger.info("æ­¥éª¤ 5/5: åˆ†ç±»åœé¡¿å¹¶è®¡ç®—æœ€ç»ˆåˆ‡ç‚¹...")
        vocal_pauses = self._classify_pause_positions(valid_pauses, len(detection_target_audio))
        vocal_pauses = self._calculate_cut_points(vocal_pauses, bpm_features=bpm_features, waveform=detection_target_audio)
        
        logger.info(f"æ£€æµ‹å®Œæˆï¼Œæ‰¾åˆ° {len(vocal_pauses)} ä¸ªæœ‰æ•ˆäººå£°åœé¡¿")
        return vocal_pauses

    # ... çœç•¥ _init_silero_vad, _detect_speech_timestamps, _calculate_pause_segments ...
    # ... å®ƒä»¬çš„å†…å®¹ä¿æŒä¸å˜ ...

    def _detect_speech_timestamps(self, audio: np.ndarray) -> List[Dict]:
        """ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³ï¼Œå‚æ•°ç”±self.current_adaptive_paramsåŠ¨æ€æä¾›"""
        try:
            import torch
            import librosa
            target_sr = 16000
            audio_16k = librosa.resample(audio, orig_sr=self.sample_rate, target_sr=target_sr)
            audio_tensor = torch.from_numpy(audio_16k).float()
            
            # åŠ¨æ€è·å–VADå‚æ•°
            if self.current_adaptive_params:
                params = self.current_adaptive_params
                vad_params = {
                    'threshold': params.vad_threshold,
                    'min_speech_duration_ms': get_config('advanced_vad.silero_min_speech_ms', 250),
                    'min_silence_duration_ms': int(params.min_pause_duration * 1000),
                    'window_size_samples': get_config('advanced_vad.silero_window_size_samples', 512),
                    'speech_pad_ms': int(params.speech_pad_ms)
                }
                logger.info(f"åº”ç”¨åŠ¨æ€VADå‚æ•°: {vad_params}")
            else: # å›é€€åˆ°é™æ€é…ç½®
                vad_params = {
                    'threshold': get_config('advanced_vad.silero_prob_threshold_down', 0.35),
                    'min_speech_duration_ms': get_config('advanced_vad.silero_min_speech_ms', 250),
                    'min_silence_duration_ms': get_config('advanced_vad.silero_min_silence_ms', 700),
                    'window_size_samples': get_config('advanced_vad.silero_window_size_samples', 512),
                    'speech_pad_ms': get_config('advanced_vad.silero_speech_pad_ms', 150)
                }
                logger.info(f"åº”ç”¨é™æ€VADå‚æ•°: {vad_params}")
            
            speech_timestamps_16k = self.get_speech_timestamps(audio_tensor, self.vad_model, sampling_rate=target_sr, **vad_params)
            
            scale_factor = self.sample_rate / target_sr
            for ts in speech_timestamps_16k:
                ts['start'] = int(ts['start'] * scale_factor)
                ts['end'] = int(ts['end'] * scale_factor)
            return speech_timestamps_16k
        except Exception as e:
            logger.error(f"Silero VADæ£€æµ‹å¤±è´¥: {e}", exc_info=True)
            return []

    def _calculate_pause_segments(self, speech_timestamps: List[Dict], audio_length: int) -> List[Dict]:
        pause_segments = []
        if not speech_timestamps:
            pause_segments.append({'start': 0, 'end': audio_length})
            return pause_segments
        if speech_timestamps[0]['start'] > 0:
            pause_segments.append({'start': 0, 'end': speech_timestamps[0]['start']})
        for i in range(len(speech_timestamps) - 1):
            if speech_timestamps[i+1]['start'] > speech_timestamps[i]['end']:
                pause_segments.append({'start': speech_timestamps[i]['end'], 'end': speech_timestamps[i+1]['start']})
        if speech_timestamps[-1]['end'] < audio_length:
            pause_segments.append({'start': speech_timestamps[-1]['end'], 'end': audio_length})
        return pause_segments

    # å…³é”®çš„ _filter_adaptive_pauses å‡½æ•°ä¿æŒæˆ‘ä»¬ä¸Šä¸€è½®ä¿®å¤åçš„ v2.5 ç‰ˆæœ¬å³å¯
    def _filter_adaptive_pauses(self, pause_segments: List[Dict], bpm_features: Optional[BPMFeatures]) -> List[Dict]:
        """
        [v2.5 ç»ˆæä¿®å¤ç‰ˆ] åŸºäºé²æ£’ç»Ÿè®¡å­¦çš„æ™ºèƒ½è£å†³ç³»ç»Ÿ
        æŠ€æœ¯: ç»Ÿä¸€ä½¿ç”¨75åˆ†ä½æ•°ä½œä¸ºåŸºç¡€åŠ¨æ€é˜ˆå€¼ï¼Œå½»åº•è§£å†³å‰å¥é•¿é™éŸ³å¯¹ç»Ÿè®¡æ¨¡å‹çš„æ±¡æŸ“é—®é¢˜ã€‚
        """
        if not self.enable_bpm_adaptation or not bpm_features or not self.current_adaptive_params:
            min_pause_duration = get_config('vocal_pause_splitting.min_pause_duration', 1.0)
            min_pause_samples = int(min_pause_duration * self.sample_rate)
            valid_pauses = [p for p in pause_segments if (p['end'] - p['start']) >= min_pause_samples]
            for p in valid_pauses:
                p['duration'] = (p['end'] - p['start']) / self.sample_rate
            logger.info(f"BPMè‡ªé€‚åº”ç¦ç”¨ï¼Œä½¿ç”¨é™æ€é˜ˆå€¼ {min_pause_duration}sï¼Œè¿‡æ»¤åå‰© {len(valid_pauses)} ä¸ªåœé¡¿")
            return valid_pauses

        absolute_min_pause_s = get_config('vocal_pause_splitting.statistical_filter.absolute_min_pause', 0.3)
        min_pause_samples = int(absolute_min_pause_s * self.sample_rate)
        
        all_candidate_durations = []
        for p in pause_segments:
            duration = (p['end'] - p['start']) / self.sample_rate
            p['duration'] = duration
            if duration >= absolute_min_pause_s:
                all_candidate_durations.append(duration)

        if not all_candidate_durations:
            logger.warning(f"åœ¨åº”ç”¨æœ€å°åˆç­› ({absolute_min_pause_s}s) åï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å€™é€‰åœé¡¿ã€‚")
            return []

        percentile_75 = np.percentile(all_candidate_durations, 75)
        median_pause = np.median(all_candidate_durations)
        logger.info(f"é²æ£’åœé¡¿æ—¶é•¿ç»Ÿè®¡æ¨¡å‹: 75åˆ†ä½={percentile_75:.3f}s, ä¸­ä½æ•°={median_pause:.3f}s")
        
        valid_pauses = []
        total_audio_length = pause_segments[-1]['end'] if pause_segments else 0
        segments_with_mdd = getattr(self.adaptive_enhancer, 'last_analyzed_segments', [])
        
        base_threshold_ratio = get_config('vocal_pause_splitting.statistical_filter.base_threshold_ratio', 0.7)
        chorus_multiplier = get_config('vocal_pause_splitting.statistical_filter.chorus_multiplier', 1.0)
        mdd_threshold_multiplier = get_config('musical_dynamic_density.threshold_multiplier', 0.2)
        
        base_dynamic_threshold = percentile_75 * base_threshold_ratio

        for pause in pause_segments:
            duration_s = pause['duration']
            if duration_s < absolute_min_pause_s:
                continue

            current_time = pause['start'] / self.sample_rate
            current_mdd = 0.5
            if segments_with_mdd:
                for seg in segments_with_mdd:
                    if seg.start_time <= current_time < seg.end_time:
                        current_mdd = seg.dynamic_density_score
                        break
            
            is_head = (pause.get('start', 0) == 0)
            is_tail = (pause.get('end', 0) >= total_audio_length * 0.95)
            
            dynamic_threshold = base_dynamic_threshold

            if self.current_adaptive_params.category in ['fast', 'very_fast']:
                if current_mdd > 0.7:
                    mdd_adjustment = 1.0 - (current_mdd * mdd_threshold_multiplier)
                    dynamic_threshold = max(median_pause, dynamic_threshold * mdd_adjustment)
            else:
                if current_mdd > 0.6:
                    mdd_adjustment = 1.0 + (current_mdd - 0.6) * mdd_threshold_multiplier
                    dynamic_threshold *= mdd_adjustment
            
            final_threshold = max(dynamic_threshold * chorus_multiplier, absolute_min_pause_s)

            if duration_s >= final_threshold or is_head or is_tail:
                valid_pauses.append(pause)
        
        logger.info(f"é²æ£’ç»Ÿè®¡è£å†³å®Œæˆ: {len(all_candidate_durations)}ä¸ªå€™é€‰ -> {len(valid_pauses)}ä¸ªæœ€ç»ˆåˆ†å‰²ç‚¹")
        return valid_pauses

    def _classify_pause_positions(self, valid_pauses: List[Dict], audio_length: int) -> List[VocalPause]:
        vocal_pauses = []
        for pause in valid_pauses:
            start_time = pause['start'] / self.sample_rate
            end_time = pause['end'] / self.sample_rate
            duration = pause['duration']
            position_type = 'head' if pause['start'] == 0 else 'tail' if pause['end'] == audio_length else 'middle'
            min_pause_duration = self.current_adaptive_params.min_pause_duration if self.current_adaptive_params else 1.0
            confidence = min(1.0, duration / (min_pause_duration * 2))
            vocal_pauses.append(VocalPause(start_time, end_time, duration, position_type, confidence, 0.0))
        return vocal_pauses

    def _calculate_cut_points(self, vocal_pauses: List[VocalPause], bpm_features: Optional['BPMFeatures'] = None, waveform: Optional[np.ndarray] = None) -> List[VocalPause]:
        logger.info(f"è®¡ç®— {len(vocal_pauses)} ä¸ªåœé¡¿çš„åˆ‡å‰²ç‚¹...")
        for i, pause in enumerate(vocal_pauses):
            search_start, search_end = self._define_search_range(pause)
            valley_point_s = self._find_energy_valley(waveform, search_start, search_end)
            if valley_point_s is None:
                valley_point_s = (pause.start_time + pause.end_time) / 2
            final_cut_point_s = valley_point_s
            if bpm_features and self.current_adaptive_params:
                final_cut_point_s = self._smart_beat_align(waveform, valley_point_s, bpm_features, search_start, search_end)
            pause.cut_point = final_cut_point_s
        return vocal_pauses

    def _define_search_range(self, pause: VocalPause) -> Tuple[float, float]:
        search_start, search_end = pause.start_time, pause.end_time
        if pause.position_type == 'head':
            search_start = max(search_start, pause.end_time + self.head_offset - 0.5)
            search_end = min(search_end, pause.end_time + self.head_offset + 0.5)
        elif pause.position_type == 'tail':
            search_start = max(search_start, pause.start_time + self.tail_offset - 0.5)
            search_end = min(search_end, pause.start_time + self.tail_offset + 0.5)
        return (search_start, search_end) if search_end > search_start else (pause.start_time, pause.end_time)

    def _find_energy_valley(self, waveform: Optional[np.ndarray], start_s: float, end_s: float) -> Optional[float]:
        if waveform is None: return None
        local_rms_ms = get_config('vocal_pause_splitting.local_rms_window_ms', 25)
        guard_ms = get_config('vocal_pause_splitting.lookahead_guard_ms', 120)
        floor_pct = get_config('vocal_pause_splitting.silence_floor_percentile', 5)
        l_idx, r_idx = int(start_s * self.sample_rate), int(end_s * self.sample_rate)
        if r_idx > l_idx:
            valley_idx = self._select_valley_cut_point(waveform, l_idx, r_idx, self.sample_rate, local_rms_ms, guard_ms, floor_pct)
            return valley_idx / self.sample_rate if valley_idx is not None else None
        return None

    def _smart_beat_align(self, waveform: np.ndarray, valley_point_s: float, bpm_features: 'BPMFeatures', search_start_s: float, search_end_s: float) -> float:
        beat_interval = 60.0 / float(bpm_features.main_bpm)
        nearest_beat_s = round(valley_point_s / beat_interval) * beat_interval
        if not (search_start_s <= nearest_beat_s <= search_end_s):
            return valley_point_s
        valley_idx, beat_idx = int(valley_point_s * self.sample_rate), int(nearest_beat_s * self.sample_rate)
        win_size = int(0.05 * self.sample_rate)
        valley_energy = np.mean(waveform[max(0, valley_idx - win_size//2) : valley_idx + win_size//2]**2)
        beat_energy = np.mean(waveform[max(0, beat_idx - win_size//2) : beat_idx + win_size//2]**2)
        if beat_energy <= valley_energy * 1.3:
            return nearest_beat_s
        return valley_point_s

    def _select_valley_cut_point(self, waveform: np.ndarray, left_idx: int, right_idx: int, sample_rate: int, local_rms_ms: int, guard_ms: int, floor_percentile: float) -> Optional[int]:
        left_idx, right_idx = max(0, int(left_idx)), min(len(waveform), int(right_idx))
        if right_idx - left_idx <= 8: return None
        win_samples = max(1, int(local_rms_ms / 1000.0 * sample_rate))
        guard_samples = max(1, int(guard_ms / 1000.0 * sample_rate)) if guard_ms > 0 else 0
        segment = waveform[left_idx:right_idx]
        rms = self._compute_rms_envelope(segment, win_samples)
        floor_val = np.percentile(np.abs(segment), floor_percentile)
        order = np.argsort(rms)
        margin_samples = max(1, int(0.02 * sample_rate))
        for j in order:
            if margin_samples <= j < (len(rms) - margin_samples):
                if guard_samples == 0 or self._future_silence_guard(rms, j, guard_samples, floor_val):
                    return left_idx + j
        return left_idx + order[0] if order.size > 0 else None

    def _compute_rms_envelope(self, waveform: np.ndarray, win_samples: int) -> np.ndarray:
        if win_samples <= 1: return np.abs(waveform).astype(np.float32)
        kernel = np.ones(int(win_samples), dtype=np.float32) / float(max(1, int(win_samples)))
        return np.sqrt(np.convolve((waveform.astype(np.float32) ** 2), kernel, mode='same'))

    def _future_silence_guard(self, rms: np.ndarray, start_idx: int, guard_samples: int, floor_val: float, allowance: float = 1.2, ratio: float = 0.7) -> bool:
        end_idx = min(start_idx + guard_samples, len(rms))
        if end_idx <= start_idx: return False
        window = rms[start_idx:end_idx]
        return (np.sum(window <= (floor_val * allowance)) / float(window.size)) >= ratio if window.size > 0 else False