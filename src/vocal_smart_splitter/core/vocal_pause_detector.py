#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# vocal_smart_splitter/core/vocal_pause_detector.py
# AI-SUMMARY: äººå£°åœé¡¿æ£€æµ‹å™¨ - ä½¿ç”¨Silero VADç›´æ¥åœ¨åŸå§‹éŸ³é¢‘ä¸Šæ£€æµ‹äººå£°åœé¡¿

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

from ..utils.config_manager import get_config

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥è‡ªé€‚åº”å¢å¼ºå™¨
try:
    from .adaptive_vad_enhancer import AdaptiveVADEnhancer
    ADAPTIVE_VAD_AVAILABLE = True
    logger.info("âœ… è‡ªé€‚åº”VADå¢å¼ºå™¨å¯ç”¨")
except ImportError as e:
    logger.warning(f"âš ï¸  è‡ªé€‚åº”VADå¢å¼ºå™¨ä¸å¯ç”¨: {e}")
    ADAPTIVE_VAD_AVAILABLE = False

@dataclass
class VocalPause:
    """äººå£°åœé¡¿æ•°æ®ç»“æ„"""
    start_time: float        # åœé¡¿å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end_time: float          # åœé¡¿ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰  
    duration: float          # åœé¡¿æ—¶é•¿ï¼ˆç§’ï¼‰
    position_type: str       # ä½ç½®ç±»å‹ï¼š'head', 'middle', 'tail'
    confidence: float        # ç½®ä¿¡åº¦ (0-1)
    cut_point: float         # åˆ‡å‰²ç‚¹æ—¶é—´ï¼ˆç§’ï¼‰

class VocalPauseDetectorV2:
    """æ”¹è¿›çš„äººå£°åœé¡¿æ£€æµ‹å™¨ - ç›´æ¥åœ¨åŸå§‹éŸ³é¢‘ä¸Šä½¿ç”¨Silero VAD"""
    
    def __init__(self, sample_rate: int = 44100):
        """åˆå§‹åŒ–äººå£°åœé¡¿æ£€æµ‹å™¨
        
        Args:
            sample_rate: é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        
        # é…ç½®å‚æ•°
        self.min_pause_duration = get_config('vocal_pause_splitting.min_pause_duration', 1.0)
        self.voice_threshold = get_config('vocal_pause_splitting.voice_threshold', 0.3)
        self.min_confidence = get_config('vocal_pause_splitting.min_confidence', 0.5)
        self.head_offset = get_config('vocal_pause_splitting.head_offset', -0.5)
        self.tail_offset = get_config('vocal_pause_splitting.tail_offset', 0.5)
        
        # BPMæ„ŸçŸ¥è‡ªé€‚åº”å¢å¼ºå™¨
        self.enable_bpm_adaptation = get_config('vocal_pause_splitting.enable_bpm_adaptation', True)
        self.adaptive_enhancer = None
        
        if self.enable_bpm_adaptation and ADAPTIVE_VAD_AVAILABLE:
            try:
                self.adaptive_enhancer = AdaptiveVADEnhancer(sample_rate)
                logger.info("ğŸµ BPMè‡ªé€‚åº”å¢å¼ºå™¨å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"BPMè‡ªé€‚åº”å¢å¼ºå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_bpm_adaptation = False
        else:
            logger.info("ä½¿ç”¨å›ºå®šé˜ˆå€¼VADæ¨¡å¼")
        
        # åˆå§‹åŒ–Silero VAD
        self._init_silero_vad()
        
        logger.info("äººå£°åœé¡¿æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ (é‡‡æ ·ç‡: {})".format(sample_rate))
    
    def _init_silero_vad(self):
        """åˆå§‹åŒ–Silero VAD"""
        try:
            import torch
            torch.set_num_threads(1)
            
            # ä¸‹è½½å¹¶åŠ è½½Silero VADæ¨¡å‹
            model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False
            )
            
            self.vad_model = model
            self.vad_utils = utils
            (self.get_speech_timestamps,
             self.save_audio, self.read_audio,
             self.VADIterator, self.collect_chunks) = utils
            
            logger.info("âœ… Silero VADæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"Silero VADåˆå§‹åŒ–å¤±è´¥: {e}")
            self.vad_model = None
    
    def detect_vocal_pauses(self, original_audio: np.ndarray) -> List[VocalPause]:
        """æ£€æµ‹äººå£°åœé¡¿ï¼ˆé›†æˆBPMæ„ŸçŸ¥è‡ªé€‚åº”å¢å¼ºï¼‰
        
        Args:
            original_audio: åŸå§‹éŸ³é¢‘ï¼ˆåŒ…å«èƒŒæ™¯éŸ³ä¹ï¼‰
            
        Returns:
            æ£€æµ‹åˆ°çš„äººå£°åœé¡¿åˆ—è¡¨
        """
        logger.info("å¼€å§‹BPMæ„ŸçŸ¥çš„äººå£°åœé¡¿æ£€æµ‹...")
        
        try:
            if self.vad_model is None:
                logger.error("Silero VADæ¨¡å‹æœªåŠ è½½")
                return []
            
            # å­˜å‚¨åˆ†æç»“æœç”¨äºè‡ªé€‚åº”è°ƒæ•´
            complexity_segments = None
            bpm_features = None
            
            # 1. BPMæ„ŸçŸ¥å¤æ‚åº¦åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_bpm_adaptation and self.adaptive_enhancer:
                logger.info("æ‰§è¡ŒBPMæ„ŸçŸ¥çš„ç¼–æ›²å¤æ‚åº¦åˆ†æ...")
                complexity_segments, bpm_features = self.adaptive_enhancer.analyze_arrangement_complexity(original_audio)
                
                if complexity_segments and bpm_features:
                    logger.info(f"ğŸµ éŸ³é¢‘åˆ†æå®Œæˆ: {float(bpm_features.main_bpm):.1f} BPM ({bpm_features.bpm_category})")
                    # ğŸ†• å­˜å‚¨ä¹å™¨å¤æ‚åº¦åˆ†æç»“æœç”¨äºå¤šä¹å™¨ç¯å¢ƒä¼˜åŒ–
                    if hasattr(self.adaptive_enhancer, 'last_instrument_analysis'):
                        self.last_complexity_analysis = self.adaptive_enhancer.last_instrument_analysis
                else:
                    logger.warning("å¤æ‚åº¦åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼æ¨¡å¼")
                    self.enable_bpm_adaptation = False
            
            # 2. è‡ªé€‚åº”VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³
            speech_timestamps = self._detect_adaptive_speech_timestamps(
                original_audio, complexity_segments, bpm_features
            )
            
            # 3. è®¡ç®—åœé¡¿åŒºåŸŸï¼ˆè¯­éŸ³ç‰‡æ®µä¹‹é—´çš„é—´éš™ï¼‰
            pause_segments = self._calculate_pause_segments(speech_timestamps, len(original_audio))
            
            # 4. è‡ªé€‚åº”è¿‡æ»¤æœ‰æ•ˆåœé¡¿
            valid_pauses = self._filter_adaptive_pauses(pause_segments, complexity_segments, bpm_features)
            
            # 5. åˆ†ç±»åœé¡¿ä½ç½®ï¼ˆå¤´éƒ¨/ä¸­é—´/å°¾éƒ¨ï¼‰
            vocal_pauses = self._classify_pause_positions(valid_pauses, speech_timestamps, len(original_audio))
            
            # 6. è®¡ç®—åˆ‡å‰²ç‚¹
            vocal_pauses = self._calculate_cut_points(vocal_pauses, bpm_features)
            
            # 7. BPMæ„ŸçŸ¥çš„åœé¡¿ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_bpm_adaptation and bpm_features:
                vocal_pauses = self._optimize_pauses_with_bpm(vocal_pauses, bpm_features)
            
            logger.info(f"æ£€æµ‹åˆ° {len(vocal_pauses)} ä¸ªæœ‰æ•ˆäººå£°åœé¡¿")
            if self.enable_bpm_adaptation and bpm_features:
                logger.info(f"ğŸµ BPMè‡ªé€‚åº”ä¼˜åŒ–å®Œæˆ ({bpm_features.bpm_category}éŸ³ä¹)")
            
            return vocal_pauses
            
        except Exception as e:
            logger.error(f"BPMæ„ŸçŸ¥äººå£°åœé¡¿æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _detect_speech_timestamps(self, audio: np.ndarray) -> List[Dict]:
        """ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            
        Returns:
            è¯­éŸ³æ—¶é—´æˆ³åˆ—è¡¨ [{'start': int, 'end': int}] (æ ·æœ¬ç´¢å¼•)
        """
        try:
            import torch
            import librosa
            
            # Silero VADåªæ”¯æŒ16000Hzï¼Œéœ€è¦é‡é‡‡æ ·
            target_sr = 16000
            if self.sample_rate != target_sr:
                audio_resampled = librosa.resample(audio, orig_sr=self.sample_rate, target_sr=target_sr)
            else:
                audio_resampled = audio
            
            # è½¬æ¢ä¸ºtorch tensor
            audio_tensor = torch.from_numpy(audio_resampled).float()
            
            # ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³
            speech_timestamps = self.get_speech_timestamps(
                audio_tensor, 
                self.vad_model,
                sampling_rate=target_sr,
                threshold=self.voice_threshold,
                min_speech_duration_ms=250,  # é™ä½è‡³250msæ£€æµ‹æ›´çŸ­è¯­éŸ³ç‰‡æ®µ
                min_silence_duration_ms=int(self.min_pause_duration * 1000),  # æœ€å°é™éŸ³æ—¶é•¿ç°ä¸º400ms
                window_size_samples=512,
                speech_pad_ms=10  # å‡å°‘å¡«å……æé«˜ç²¾åº¦
            )
            
            # å°†æ—¶é—´æˆ³æ˜ å°„å›åŸå§‹é‡‡æ ·ç‡
            if self.sample_rate != target_sr:
                scale_factor = self.sample_rate / target_sr
                for ts in speech_timestamps:
                    ts['start'] = int(ts['start'] * scale_factor)
                    ts['end'] = int(ts['end'] * scale_factor)
            
            logger.info(f"Silero VADæ£€æµ‹ç»“æœ: {len(speech_timestamps)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            
            # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            for i, ts in enumerate(speech_timestamps[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                start_sec = ts['start'] / self.sample_rate
                end_sec = ts['end'] / self.sample_rate
                duration = end_sec - start_sec
                logger.info(f"  è¯­éŸ³ç‰‡æ®µ{i+1}: {start_sec:.2f}s - {end_sec:.2f}s (æ—¶é•¿: {duration:.2f}s)")
            
            if len(speech_timestamps) > 10:
                logger.info(f"  ... è¿˜æœ‰ {len(speech_timestamps)-10} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            
            return speech_timestamps
            
        except Exception as e:
            logger.error(f"Silero VADæ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _detect_adaptive_speech_timestamps(
        self, audio: np.ndarray, complexity_segments=None, bpm_features=None
    ) -> List[Dict]:
        """è‡ªé€‚åº”VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³ï¼ˆé›†æˆBPMæ„ŸçŸ¥ï¼‰
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
            complexity_segments: ç¼–æ›²å¤æ‚åº¦ç‰‡æ®µï¼ˆå¯é€‰ï¼‰
            bpm_features: BPMç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯­éŸ³æ—¶é—´æˆ³åˆ—è¡¨
        """
        if not self.enable_bpm_adaptation or not complexity_segments or not bpm_features:
            # ä½¿ç”¨å›ºå®šé˜ˆå€¼çš„åŸå§‹æ–¹æ³•
            return self._detect_speech_timestamps(audio)
        
        try:
            import torch
            import librosa
            
            # é‡é‡‡æ ·åˆ°Silero VADæ”¯æŒçš„é‡‡æ ·ç‡
            target_sr = 16000
            if self.sample_rate != target_sr:
                audio_resampled = librosa.resample(audio, orig_sr=self.sample_rate, target_sr=target_sr)
            else:
                audio_resampled = audio
            
            # è½¬æ¢ä¸ºtorch tensor
            audio_tensor = torch.from_numpy(audio_resampled).float()
            
            # ä½¿ç”¨åˆ†æ®µè‡ªé€‚åº”æ£€æµ‹
            all_speech_timestamps = []
            
            # æŒ‰å¤æ‚åº¦ç‰‡æ®µè¿›è¡Œåˆ†æ®µæ£€æµ‹
            for segment in complexity_segments:
                # è®¡ç®—å½“å‰ç‰‡æ®µçš„æ ·æœ¬èŒƒå›´
                start_sample = int(segment.start_time * target_sr)
                end_sample = int(min(segment.end_time * target_sr, len(audio_resampled)))
                
                if end_sample <= start_sample:
                    continue
                
                segment_audio = audio_tensor[start_sample:end_sample]
                
                # è·å–å½“å‰ç‰‡æ®µçš„è‡ªé€‚åº”å‚æ•°
                adaptive_params = self.adaptive_enhancer.get_enhanced_adaptive_vad_params(
                    complexity_segments, bpm_features, (segment.start_time + segment.end_time) / 2
                )
                
                # ä½¿ç”¨è‡ªé€‚åº”å‚æ•°è¿›è¡ŒVADæ£€æµ‹
                segment_timestamps = self.get_speech_timestamps(
                    segment_audio,
                    self.vad_model,
                    sampling_rate=target_sr,
                    threshold=adaptive_params['voice_threshold'],
                    min_speech_duration_ms=adaptive_params['min_speech_duration_ms'],
                    min_silence_duration_ms=adaptive_params['min_silence_duration_ms'],
                    window_size_samples=512,
                    speech_pad_ms=30
                )
                
                # å°†ç‰‡æ®µæ—¶é—´æˆ³æ˜ å°„å›å…¨å±€æ—¶é—´
                for ts in segment_timestamps:
                    ts['start'] += start_sample
                    ts['end'] += start_sample
                
                all_speech_timestamps.extend(segment_timestamps)
            
            # åˆå¹¶é‡å çš„æ—¶é—´æˆ³
            all_speech_timestamps = self._merge_overlapping_timestamps(all_speech_timestamps)
            
            # æ˜ å°„å›åŸå§‹é‡‡æ ·ç‡
            if self.sample_rate != target_sr:
                scale_factor = self.sample_rate / target_sr
                for ts in all_speech_timestamps:
                    ts['start'] = int(ts['start'] * scale_factor)
                    ts['end'] = int(ts['end'] * scale_factor)
            
            logger.info(f"ğŸµ è‡ªé€‚åº”VADæ£€æµ‹å®Œæˆ: {len(all_speech_timestamps)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            return all_speech_timestamps
            
        except Exception as e:
            logger.error(f"è‡ªé€‚åº”VADæ£€æµ‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°å›ºå®šé˜ˆå€¼æ¨¡å¼")
            return self._detect_speech_timestamps(audio)
    
    def _merge_overlapping_timestamps(self, timestamps: List[Dict]) -> List[Dict]:
        """åˆå¹¶é‡å çš„æ—¶é—´æˆ³"""
        if not timestamps:
            return []
        
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        timestamps = sorted(timestamps, key=lambda x: x['start'])
        merged = [timestamps[0]]
        
        for current in timestamps[1:]:
            last = merged[-1]
            
            # å¦‚æœå½“å‰ç‰‡æ®µä¸ä¸Šä¸€ä¸ªç‰‡æ®µé‡å æˆ–ç›¸é‚»ï¼Œåˆ™åˆå¹¶
            if current['start'] <= last['end'] + 1000:  # 1000æ ·æœ¬çš„å®¹å¿åº¦
                last['end'] = max(last['end'], current['end'])
            else:
                merged.append(current)
        
        return merged
    
    def _calculate_pause_segments(self, speech_timestamps: List[Dict], audio_length: int) -> List[Dict]:
        """è®¡ç®—åœé¡¿åŒºåŸŸ
        
        Args:
            speech_timestamps: è¯­éŸ³æ—¶é—´æˆ³
            audio_length: éŸ³é¢‘æ€»é•¿åº¦ï¼ˆæ ·æœ¬æ•°ï¼‰
            
        Returns:
            åœé¡¿åŒºåŸŸåˆ—è¡¨ [{'start': int, 'end': int}]
        """
        pause_segments = []
        
        if not speech_timestamps:
            # æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ•´ä¸ªéŸ³é¢‘éƒ½æ˜¯åœé¡¿
            pause_segments.append({
                'start': 0,
                'end': audio_length
            })
            return pause_segments
        
        # å¤´éƒ¨åœé¡¿ï¼ˆéŸ³é¢‘å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªè¯­éŸ³ç‰‡æ®µï¼‰
        if speech_timestamps[0]['start'] > 0:
            pause_segments.append({
                'start': 0,
                'end': speech_timestamps[0]['start']
            })
        
        # ä¸­é—´åœé¡¿ï¼ˆè¯­éŸ³ç‰‡æ®µä¹‹é—´ï¼‰
        for i in range(len(speech_timestamps) - 1):
            current_end = speech_timestamps[i]['end']
            next_start = speech_timestamps[i + 1]['start']
            
            if next_start > current_end:
                pause_segments.append({
                    'start': current_end,
                    'end': next_start
                })
        
        # å°¾éƒ¨åœé¡¿ï¼ˆæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µåˆ°éŸ³é¢‘ç»“æŸï¼‰
        if speech_timestamps[-1]['end'] < audio_length:
            pause_segments.append({
                'start': speech_timestamps[-1]['end'],
                'end': audio_length
            })
        
        return pause_segments
    
    def _filter_valid_pauses(self, pause_segments: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤æœ‰æ•ˆåœé¡¿
        
        Args:
            pause_segments: åœé¡¿åŒºåŸŸåˆ—è¡¨
            
        Returns:
            æœ‰æ•ˆåœé¡¿åˆ—è¡¨
        """
        valid_pauses = []
        min_pause_samples = int(self.min_pause_duration * self.sample_rate)
        
        for pause in pause_segments:
            duration_samples = pause['end'] - pause['start']
            duration_seconds = duration_samples / self.sample_rate
            
            if duration_samples >= min_pause_samples:
                valid_pauses.append({
                    **pause,
                    'duration': duration_seconds
                })
        
        logger.debug(f"è¿‡æ»¤åä¿ç•™ {len(valid_pauses)} ä¸ªæœ‰æ•ˆåœé¡¿")
        return valid_pauses
    
    def _classify_pause_positions(self, pause_segments: List[Dict], 
                                speech_timestamps: List[Dict], 
                                audio_length: int) -> List[VocalPause]:
        """åˆ†ç±»åœé¡¿ä½ç½®
        
        Args:
            pause_segments: æœ‰æ•ˆåœé¡¿åˆ—è¡¨
            speech_timestamps: è¯­éŸ³æ—¶é—´æˆ³
            audio_length: éŸ³é¢‘æ€»é•¿åº¦
            
        Returns:
            åˆ†ç±»åçš„äººå£°åœé¡¿åˆ—è¡¨
        """
        vocal_pauses = []
        
        for pause in pause_segments:
            start_time = pause['start'] / self.sample_rate
            end_time = pause['end'] / self.sample_rate
            duration = pause['duration']
            
            # åˆ¤æ–­åœé¡¿ä½ç½®ç±»å‹
            if pause['start'] == 0:
                # å¤´éƒ¨åœé¡¿
                position_type = 'head'
            elif pause['end'] == audio_length:
                # å°¾éƒ¨åœé¡¿
                position_type = 'tail'
            else:
                # ä¸­é—´åœé¡¿
                position_type = 'middle'
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºåœé¡¿æ—¶é•¿ï¼Œè¶Šé•¿ç½®ä¿¡åº¦è¶Šé«˜ï¼‰
            confidence = min(1.0, duration / (self.min_pause_duration * 2))
            
            vocal_pause = VocalPause(
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                position_type=position_type,
                confidence=confidence,
                cut_point=0.0  # ç¨åè®¡ç®—
            )
            
            vocal_pauses.append(vocal_pause)
        
        return vocal_pauses
    
    def _calculate_cut_points(self, vocal_pauses: List[VocalPause], bpm_features: Optional['BPMFeatures'] = None) -> List[VocalPause]:
        """è®¡ç®—ç²¾ç¡®çš„åˆ‡å‰²ç‚¹ä½ç½®ï¼ˆBPMè‡ªé€‚åº”ï¼‰
        
        Args:
            vocal_pauses: äººå£°åœé¡¿åˆ—è¡¨
            bpm_features: BPMåˆ†æç‰¹å¾ï¼ˆç”¨äºè‡ªé€‚åº”åç§»ï¼‰
            
        Returns:
            åŒ…å«åˆ‡å‰²ç‚¹çš„åœé¡¿åˆ—è¡¨
        """
        # è·å–BPMè‡ªé€‚åº”åç§»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_bpm_adaptation and bpm_features:
            adaptive_head_offset, adaptive_tail_offset = self._get_adaptive_offsets(bpm_features)
        else:
            adaptive_head_offset, adaptive_tail_offset = self.head_offset, self.tail_offset
        
        for pause in vocal_pauses:
            if pause.position_type == 'head':
                # å¤´éƒ¨åœé¡¿ï¼šä½¿ç”¨è‡ªé€‚åº”åç§»
                pause.cut_point = pause.end_time + adaptive_head_offset
            elif pause.position_type == 'tail':
                # å°¾éƒ¨åœé¡¿ï¼šä½¿ç”¨è‡ªé€‚åº”åç§»
                pause.cut_point = pause.start_time + adaptive_tail_offset
            else:  # middle
                # ä¸­é—´åœé¡¿ï¼šåœ¨åœé¡¿ä¸­å¿ƒç‚¹åˆ‡å‰²
                pause.cut_point = (pause.start_time + pause.end_time) / 2
            
            # ç¡®ä¿åˆ‡å‰²ç‚¹åœ¨æœ‰æ•ˆèŒƒå›´å†…
            pause.cut_point = max(0, pause.cut_point)
        
        return vocal_pauses
    
    def _filter_adaptive_pauses(self, pause_segments: List[Dict], 
                              complexity_segments: List,
                              bpm_features: 'BPMFeatures') -> List[Dict]:
        """åŸºäºBPMç‰¹å¾è‡ªé€‚åº”è¿‡æ»¤åœé¡¿
        
        Args:
            pause_segments: åœé¡¿åŒºåŸŸåˆ—è¡¨
            bpm_features: BPMåˆ†æç‰¹å¾
            
        Returns:
            è‡ªé€‚åº”è¿‡æ»¤åçš„åœé¡¿åˆ—è¡¨
        """
        if not hasattr(self, 'adaptive_enhancer') or not self.adaptive_enhancer:
            return self._filter_valid_pauses(pause_segments)
        
        valid_pauses = []
        
        # åŸºäºBPMå’Œä¹å™¨å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æœ€å°åœé¡¿æ—¶é•¿
        if bpm_features.bpm_category == 'slow':
            # æ…¢æ­Œï¼šå…è®¸æ›´çŸ­çš„åœé¡¿ï¼ˆæ­Œæ‰‹æœ‰æ›´å¤šæ—¶é—´æ¢æ°”ï¼‰
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.slow_song_multiplier', 1.5)
            min_pause_duration = max(0.6, self.min_pause_duration * multiplier)
        elif bpm_features.bpm_category == 'fast':
            # å¿«æ­Œï¼šéœ€è¦æ›´é•¿çš„åœé¡¿æ‰è®¤ä¸ºæ˜¯çœŸæ­£çš„åœé¡¿
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.fast_song_multiplier', 0.7)
            min_pause_duration = self.min_pause_duration * multiplier
        else:
            # ä¸­ç­‰é€Ÿåº¦ï¼šä½¿ç”¨å¯é…ç½®çš„æ ‡å‡†ä¹˜æ•°
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.medium_song_multiplier', 1.0)
            min_pause_duration = self.min_pause_duration * multiplier
            
        # ğŸ†• å¤šä¹å™¨ç¯å¢ƒå¢å¼ºï¼šæ ¹æ®ä¹å™¨æ•°é‡å’Œå¤æ‚åº¦è¿›ä¸€æ­¥è°ƒæ•´
        if hasattr(self, 'last_complexity_analysis') and self.last_complexity_analysis:
            complexity = self.last_complexity_analysis.get('total_complexity', 0.0)
            instrument_count = self.last_complexity_analysis.get('instrument_count', 1)
            
            # ä¹å™¨è¶Šå¤šï¼Œéœ€è¦æ›´é•¿çš„åœé¡¿æ¥ç¡®ä¿æ˜¯çœŸå®çš„äººå£°åœé¡¿
            if instrument_count >= 4:  # 4ç§ä»¥ä¸Šä¹å™¨
                base_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_4_plus_base', 1.4)
                step_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_4_plus_step', 0.1)
                instrument_factor = base_factor + (instrument_count - 4) * step_factor
            elif instrument_count >= 3:  # 3ç§ä¹å™¨
                base_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_3_base', 1.2)
                complexity_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_3_complexity_factor', 0.4)
                instrument_factor = base_factor + (complexity - 0.5) * complexity_factor
            elif instrument_count >= 2:  # 2ç§ä¹å™¨
                base_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_2_base', 1.1)
                complexity_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_2_complexity_factor', 0.2)
                instrument_factor = base_factor + (complexity - 0.3) * complexity_factor
            else:
                instrument_factor = 1.0
                
            min_pause_duration = min_pause_duration * instrument_factor
            logger.info(f"ğŸ¸ å¤šä¹å™¨è°ƒæ•´: {instrument_count}ç§ä¹å™¨, å¤æ‚åº¦{float(complexity):.3f}, ç³»æ•°Ã—{instrument_factor:.2f}")
        
        # ç¡®ä¿ä¸ä¼šè¿‡åº¦è°ƒæ•´
        min_pause_duration = np.clip(min_pause_duration, 0.5, 3.0)
        
        min_pause_samples = int(min_pause_duration * self.sample_rate)
        
        for pause in pause_segments:
            duration_samples = pause['end'] - pause['start']
            duration_seconds = duration_samples / self.sample_rate
            
            # BPMæ„ŸçŸ¥çš„åœé¡¿éªŒè¯
            if duration_samples >= min_pause_samples:
                # æ ¹æ®èŠ‚æ‹å¼ºåº¦è°ƒæ•´ç½®ä¿¡åº¦
                confidence = 0.8  # åŸºç¡€ç½®ä¿¡åº¦
                
                # å¦‚æœåœé¡¿æ—¶é•¿ä¸èŠ‚æ‹å‘¨æœŸå¯¹é½ï¼Œæé«˜ç½®ä¿¡åº¦
                beat_duration = 60.0 / bpm_features.main_bpm if bpm_features.main_bpm > 0 else 1.0
                if abs(duration_seconds % beat_duration) < 0.1 or \
                   abs(duration_seconds % (beat_duration * 2)) < 0.1:
                    confidence += 0.1
                
                valid_pauses.append({
                    **pause,
                    'duration': duration_seconds,
                    'confidence': confidence,
                    'bpm_aligned': abs(duration_seconds % beat_duration) < 0.1
                })
        
        logger.debug(f"BPMè‡ªé€‚åº”è¿‡æ»¤åä¿ç•™ {len(valid_pauses)} ä¸ªæœ‰æ•ˆåœé¡¿ (BPM: {float(bpm_features.main_bpm):.1f})")
        return valid_pauses
    
    def _get_adaptive_offsets(self, bpm_features: 'BPMFeatures') -> Tuple[float, float]:
        """æ ¹æ®BPMè·å–åŠ¨æ€åç§»ä¹˜æ•°
        
        Args:
            bpm_features: BPMåˆ†æç‰¹å¾
            
        Returns:
            Tuple[head_offset, tail_offset]: è°ƒæ•´åçš„åç§»å€¼
        """
        if bpm_features.bpm_category == 'slow':
            # æ…¢æ­Œï¼šä½¿ç”¨æ›´é•¿çš„åç§»ï¼Œç»™æ­Œæ‰‹æ›´å¤šçš„åœé¡¿ç¼“å†²æ—¶é—´
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.offset_multipliers.slow_song_offset_multiplier', 1.6)
        elif bpm_features.bpm_category == 'fast':
            # å¿«æ­Œï¼šä½¿ç”¨æ›´çŸ­çš„åç§»ï¼Œä¿æŒç´§å‡‘çš„èŠ‚å¥æ„Ÿ
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.offset_multipliers.fast_song_offset_multiplier', 0.6)
        else:
            # ä¸­é€Ÿæ­Œï¼šä½¿ç”¨æ ‡å‡†åç§»
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.offset_multipliers.medium_song_offset_multiplier', 1.0)
        
        adaptive_head_offset = self.head_offset * multiplier
        adaptive_tail_offset = self.tail_offset * multiplier
        
        logger.debug(f"BPMè‡ªé€‚åº”åç§»: {bpm_features.bpm_category}æ­Œ, ä¹˜æ•°Ã—{multiplier:.1f}, åç§»({adaptive_head_offset:.2f}s, +{adaptive_tail_offset:.2f}s)")
        
        return adaptive_head_offset, adaptive_tail_offset
    
    def _optimize_pauses_with_bpm(self, vocal_pauses: List[VocalPause], 
                                 bpm_features: 'BPMFeatures') -> List[VocalPause]:
        """ä½¿ç”¨BPMä¿¡æ¯ä¼˜åŒ–åœé¡¿åˆ‡ç‚¹
        
        Args:
            vocal_pauses: äººå£°åœé¡¿åˆ—è¡¨
            bpm_features: BPMåˆ†æç‰¹å¾
            
        Returns:
            BPMä¼˜åŒ–åçš„åœé¡¿åˆ—è¡¨
        """
        if not hasattr(self, 'adaptive_enhancer') or not self.adaptive_enhancer:
            return vocal_pauses
        
        beat_duration = 60.0 / bpm_features.main_bpm if bpm_features.main_bpm > 0 else 1.0
        
        for pause in vocal_pauses:
            original_cut_point = pause.cut_point
            
            # å°è¯•å°†åˆ‡ç‚¹å¯¹é½åˆ°æœ€è¿‘çš„èŠ‚æ‹ç‚¹
            if bpm_features.beat_strength > 0.6:  # èŠ‚æ‹è¾ƒå¼ºæ—¶æ‰å¯¹é½
                # æ‰¾åˆ°æœ€è¿‘çš„èŠ‚æ‹ç‚¹
                beat_times = []
                current_beat = 0
                while current_beat < pause.end_time + 2:  # æœç´¢èŒƒå›´æ‰©å±•
                    beat_times.append(current_beat)
                    current_beat += beat_duration
                
                # æ‰¾åˆ°æœ€æ¥è¿‘å½“å‰åˆ‡ç‚¹çš„èŠ‚æ‹ç‚¹
                if beat_times:
                    closest_beat = min(beat_times, key=lambda x: abs(x - original_cut_point))
                    
                    # å¦‚æœèŠ‚æ‹ç‚¹åœ¨åœé¡¿èŒƒå›´å†…ä¸”è·ç¦»ä¸å¤ªè¿œï¼Œä½¿ç”¨èŠ‚æ‹ç‚¹
                    if (pause.start_time <= closest_beat <= pause.end_time and 
                        abs(closest_beat - original_cut_point) < 0.3):
                        pause.cut_point = closest_beat
                        pause.confidence += 0.05  # èŠ‚æ‹å¯¹é½æé«˜ç½®ä¿¡åº¦
                        logger.debug(f"åœé¡¿åˆ‡ç‚¹å¯¹é½åˆ°èŠ‚æ‹: {original_cut_point:.2f}s -> {closest_beat:.2f}s")
        
        return vocal_pauses

    def generate_pause_report(self, vocal_pauses: List[VocalPause]) -> Dict:
        """ç”Ÿæˆåœé¡¿æ£€æµ‹æŠ¥å‘Š
        
        Args:
            vocal_pauses: äººå£°åœé¡¿åˆ—è¡¨
            
        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        if not vocal_pauses:
            return {
                'total_pauses': 0,
                'avg_confidence': 0.0,
                'total_pause_duration': 0.0,
                'pause_types': {'head': 0, 'middle': 0, 'tail': 0}
            }
        
        # ç»Ÿè®¡åœé¡¿ç±»å‹
        pause_types = {'head': 0, 'middle': 0, 'tail': 0}
        total_duration = 0.0
        total_confidence = 0.0
        
        for pause in vocal_pauses:
            pause_types[pause.position_type] += 1
            total_duration += pause.duration
            total_confidence += pause.confidence
        
        return {
            'total_pauses': len(vocal_pauses),
            'avg_confidence': total_confidence / len(vocal_pauses),
            'total_pause_duration': total_duration,
            'pause_types': pause_types
        }