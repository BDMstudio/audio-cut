#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# vocal_smart_splitter/core/vocal_pause_detector.py

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from ..utils.config_manager import get_config
from ..utils.adaptive_parameter_calculator import create_adaptive_calculator, AdaptiveParameters

logger = logging.getLogger(__name__)

try:
    from .adaptive_vad_enhancer import AdaptiveVADEnhancer, BPMFeatures
    ADAPTIVE_VAD_AVAILABLE = True
    logger.info("è‡ªé€‚åº”VADå¢å¼ºå™¨å¯ç”¨")
except ImportError as e:
    logger.warning(f"è‡ªé€‚åº”VADå¢å¼ºå™¨ä¸å¯ç”¨: {e}")
    ADAPTIVE_VAD_AVAILABLE = False
    BPMFeatures = None

@dataclass
class VocalPause:
    start_time: float
    end_time: float
    duration: float
    position_type: str
    confidence: float
    cut_point: float

class VocalPauseDetectorV2:
    """æ”¹è¿›çš„äººå£°åœé¡¿æ£€æµ‹å™¨ - é›†æˆBPMè‡ªé€‚åº”èƒ½åŠ›"""

    def __init__(self, sample_rate: int = 44100):
        self.sample_rate = sample_rate
        self.adaptive_calculator = create_adaptive_calculator()
        self.current_adaptive_params: Optional[AdaptiveParameters] = None
        self.head_offset = get_config('vocal_pause_splitting.head_offset', -0.5)
        self.tail_offset = get_config('vocal_pause_splitting.tail_offset', 0.5)
        
        self.enable_bpm_adaptation = get_config('vocal_pause_splitting.enable_bpm_adaptation', True) and ADAPTIVE_VAD_AVAILABLE
        if self.enable_bpm_adaptation:
            self.adaptive_enhancer = AdaptiveVADEnhancer(sample_rate)
            logger.info("BPMè‡ªé€‚åº”å¢å¼ºå™¨å·²å¯ç”¨")
        else:
            self.adaptive_enhancer = None
            logger.info("BPMè‡ªé€‚åº”å·²ç¦ç”¨æˆ–ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å›ºå®šé˜ˆå€¼æ¨¡å¼")

        self._init_silero_vad()
        logger.info(f"VocalPauseDetectorV2 åˆå§‹åŒ–å®Œæˆ (SR: {sample_rate})")

    def _init_silero_vad(self):
        try:
            import torch
            torch.set_num_threads(1)
            self.vad_model, self.vad_utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=False)
            (self.get_speech_timestamps, _, _, _, _) = self.vad_utils
            logger.info("Silero VADæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.vad_model = None
            logger.error(f"Silero VADåˆå§‹åŒ–å¤±è´¥: {e}")

    def detect_vocal_pauses(self, original_audio: np.ndarray) -> List[VocalPause]:
        """ä¸»æ£€æµ‹æµç¨‹ï¼Œç°åœ¨å®Œå…¨ç”±BPMè‡ªé€‚åº”ç³»ç»Ÿé©±åŠ¨"""
        logger.info("å¼€å§‹BPMæ„ŸçŸ¥çš„äººå£°åœé¡¿æ£€æµ‹...")
        if self.vad_model is None:
            logger.error("Silero VADæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç»§ç»­")
            return []

        bpm_features = None
        if self.enable_bpm_adaptation and self.adaptive_enhancer:
            logger.info("æ­¥éª¤ 1/5: æ‰§è¡ŒBPMå’Œç¼–æ›²å¤æ‚åº¦åˆ†æ...")
            complexity_segments, bpm_features = self.adaptive_enhancer.analyze_arrangement_complexity(original_audio)
            if bpm_features:
                logger.info(f"ğŸµ éŸ³ä¹åˆ†æå®Œæˆ: {float(bpm_features.main_bpm):.1f} BPM ({bpm_features.bpm_category})")
                # ä½¿ç”¨åˆ†æç»“æœè®¡ç®—å¹¶åº”ç”¨åŠ¨æ€å‚æ•°
                analysis = getattr(self.adaptive_enhancer, 'last_instrument_analysis', {})
                instrument_analyzer = getattr(self.adaptive_enhancer, 'instrument_analyzer', None)
                if instrument_analyzer:
                    instrument_complexity = instrument_analyzer.analyze_instrument_complexity(original_audio)
                    self.current_adaptive_params = self.adaptive_calculator.calculate_all_parameters(
                        float(bpm_features.main_bpm), float(instrument_complexity.get('overall_complexity', 0.5)), int(instrument_complexity.get('instrument_count', 3))
                    )
        else:
            logger.info("æ­¥éª¤ 1/5: è·³è¿‡BPMåˆ†æï¼ˆå·²ç¦ç”¨æˆ–ä¸å¯ç”¨ï¼‰")

        logger.info("æ­¥éª¤ 2/5: ä½¿ç”¨è‡ªé€‚åº”å‚æ•°è¿›è¡ŒVADè¯­éŸ³æ£€æµ‹...")
        speech_timestamps = self._detect_speech_timestamps(original_audio)

        logger.info("æ­¥éª¤ 3/5: è®¡ç®—è¯­éŸ³é—´çš„åœé¡¿åŒºåŸŸ...")
        pause_segments = self._calculate_pause_segments(speech_timestamps, len(original_audio))

        logger.info("æ­¥éª¤ 4/5: ä½¿ç”¨åŠ¨æ€é˜ˆå€¼è¿‡æ»¤æœ‰æ•ˆåœé¡¿...")
        valid_pauses = self._filter_adaptive_pauses(pause_segments, bpm_features)
        
        logger.info("æ­¥éª¤ 5/5: åˆ†ç±»åœé¡¿å¹¶è®¡ç®—æœ€ç»ˆåˆ‡ç‚¹...")
        vocal_pauses = self._classify_pause_positions(valid_pauses, len(original_audio))
        vocal_pauses = self._calculate_cut_points(vocal_pauses, bpm_features=bpm_features, waveform=original_audio)
        
        logger.info(f"æ£€æµ‹å®Œæˆï¼Œæ‰¾åˆ° {len(vocal_pauses)} ä¸ªæœ‰æ•ˆäººå£°åœé¡¿")
        return vocal_pauses

    def _detect_speech_timestamps(self, audio: np.ndarray) -> List[Dict]:
        """ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³ï¼Œå‚æ•°ç”±self.current_adaptive_paramsåŠ¨æ€æä¾›"""
        try:
            import torch
            import librosa
            target_sr = 16000
            audio_16k = librosa.resample(audio, orig_sr=self.sample_rate, target_sr=target_sr)
            audio_tensor = torch.from_numpy(audio_16k).float()
            
            # åŠ¨æ€è·å–VADå‚æ•°
            if self.current_adaptive_params:
                params = self.current_adaptive_params
                vad_params = {
                    'threshold': params.vad_threshold,
                    'min_speech_duration_ms': get_config('advanced_vad.silero_min_speech_ms', 250), # é€šå¸¸ä¿æŒå›ºå®š
                    'min_silence_duration_ms': int(params.min_pause_duration * 1000),
                    'window_size_samples': get_config('advanced_vad.silero_window_size_samples', 512),
                    'speech_pad_ms': int(params.speech_pad_ms)
                }
                logger.info(f"åº”ç”¨åŠ¨æ€VADå‚æ•°: {vad_params}")
            else: # å›é€€åˆ°é™æ€é…ç½®
                vad_params = {
                    'threshold': get_config('advanced_vad.silero_prob_threshold_down', 0.35),
                    'min_speech_duration_ms': get_config('advanced_vad.silero_min_speech_ms', 250),
                    'min_silence_duration_ms': get_config('advanced_vad.silero_min_silence_ms', 700),
                    'window_size_samples': get_config('advanced_vad.silero_window_size_samples', 512),
                    'speech_pad_ms': get_config('advanced_vad.silero_speech_pad_ms', 150)
                }
                logger.info(f"åº”ç”¨é™æ€VADå‚æ•°: {vad_params}")
            
            speech_timestamps_16k = self.get_speech_timestamps(audio_tensor, self.vad_model, sampling_rate=target_sr, **vad_params)
            
            # æ˜ å°„å›åŸå§‹é‡‡æ ·ç‡
            scale_factor = self.sample_rate / target_sr
            for ts in speech_timestamps_16k:
                ts['start'] = int(ts['start'] * scale_factor)
                ts['end'] = int(ts['end'] * scale_factor)
            return speech_timestamps_16k
        except Exception as e:
            logger.error(f"Silero VADæ£€æµ‹å¤±è´¥: {e}", exc_info=True)
            return []

    def _calculate_pause_segments(self, speech_timestamps: List[Dict], audio_length: int) -> List[Dict]:
        """è®¡ç®—è¯­éŸ³ç‰‡æ®µä¹‹é—´çš„åœé¡¿åŒºåŸŸ"""
        pause_segments = []

        if not speech_timestamps:
            # æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ•´ä¸ªéŸ³é¢‘éƒ½æ˜¯åœé¡¿
            pause_segments.append({
                'start': 0,
                'end': audio_length
            })
            return pause_segments

        # å¤´éƒ¨åœé¡¿ï¼ˆéŸ³é¢‘å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªè¯­éŸ³ç‰‡æ®µï¼‰
        if speech_timestamps[0]['start'] > 0:
            pause_segments.append({
                'start': 0,
                'end': speech_timestamps[0]['start']
            })

        # ä¸­é—´åœé¡¿ï¼ˆè¯­éŸ³ç‰‡æ®µä¹‹é—´ï¼‰
        for i in range(len(speech_timestamps) - 1):
            current_end = speech_timestamps[i]['end']
            next_start = speech_timestamps[i + 1]['start']

            if next_start > current_end:
                pause_segments.append({
                    'start': current_end,
                    'end': next_start
                })

        # å°¾éƒ¨åœé¡¿ï¼ˆæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µåˆ°éŸ³é¢‘ç»“æŸï¼‰
        if speech_timestamps[-1]['end'] < audio_length:
            pause_segments.append({
                'start': speech_timestamps[-1]['end'],
                'end': audio_length
            })

        return pause_segments

    def _filter_adaptive_pauses(self, pause_segments: List[Dict], bpm_features: Optional[BPMFeatures]) -> List[Dict]:
        """
        [v2.3 æœ€ç»ˆç‰ˆ] åŒæ¨¡å¼æ™ºèƒ½è£å†³ç³»ç»Ÿ
        æŠ€æœ¯: å¼•å…¥"æåº¦å®½æ¾"çš„åˆç­›ï¼Œå¹¶æ ¹æ®æ­Œæ›²ç±»å‹ï¼ˆå¿«/æ…¢ï¼‰å’ŒåŠ¨æ€ï¼ˆä¸»/å‰¯æ­Œï¼‰é€‰æ‹©ä¸åŒçš„ç»Ÿè®¡ç­–ç•¥ã€‚
        """
        if not self.enable_bpm_adaptation or not bpm_features or not self.current_adaptive_params:
            # å›é€€åˆ°æœ€ç®€å•çš„é™æ€è¿‡æ»¤
            min_pause_duration = get_config('vocal_pause_splitting.min_pause_duration', 1.0)
            min_pause_samples = int(min_pause_duration * self.sample_rate)
            valid_pauses = [p for p in pause_segments if (p['end'] - p['start']) >= min_pause_samples]
            for p in valid_pauses:
                p['duration'] = (p['end'] - p['start']) / self.sample_rate
            logger.info(f"BPMè‡ªé€‚åº”ç¦ç”¨ï¼Œä½¿ç”¨é™æ€é˜ˆå€¼ {min_pause_duration}sï¼Œè¿‡æ»¤åå‰© {len(valid_pauses)} ä¸ªåœé¡¿")
            return valid_pauses

        # === æ­¥éª¤ 1: æåº¦å®½æ¾çš„åˆç­›ï¼Œæ”¶é›†æ‰€æœ‰æ½œåœ¨çš„"å¾®åœé¡¿" ===
        # æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨ä¸€ä¸ªéå¸¸å°ä¸”å›ºå®šçš„å€¼ï¼ˆå¦‚0.3sï¼‰ï¼Œè€Œä¸æ˜¯åŠ¨æ€è®¡ç®—çš„å€¼ï¼Œæ¥ç¡®ä¿å¿«æ­Œçš„çŸ­æ°”å£èƒ½è¿›å…¥å€™é€‰æ± ã€‚
        ABSOLUTE_MIN_PAUSE_S = 0.3
        min_pause_samples = int(ABSOLUTE_MIN_PAUSE_S * self.sample_rate)

        all_candidate_durations = []
        # åœ¨å¯¹ all_candidate_durations è¿›è¡Œæ“ä½œä¹‹å‰ï¼Œéœ€è¦å…ˆå¯¹ pause_segments é‡Œçš„ duration è¿›è¡Œè®¡ç®—
        for p in pause_segments:
            p['duration'] = (p['end'] - p['start']) / self.sample_rate

        for pause in pause_segments:
            if pause['duration'] >= ABSOLUTE_MIN_PAUSE_S:
                all_candidate_durations.append(pause['duration'])

        if not all_candidate_durations:
            logger.warning("åœ¨åº”ç”¨æåº¦å®½æ¾çš„åˆç­›åï¼Œä»ç„¶æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å€™é€‰åœé¡¿ã€‚æ­Œæ›²å¯èƒ½è¿‡äºè¿ç»­ã€‚")
            return []

        # === æ­¥éª¤ 2: ç»Ÿè®¡å­¦å»ºæ¨¡ï¼Œç†è§£è¿™é¦–æ­Œçš„"åœé¡¿è¯­è¨€" ===
        average_pause = np.mean(all_candidate_durations)
        median_pause = np.median(all_candidate_durations)
        std_dev = np.std(all_candidate_durations)
        
        # ä½¿ç”¨ç™¾åˆ†ä½æ•°ä¸ºå¿«æ­Œå¯»æ‰¾"å¼‚å¸¸é•¿"çš„åœé¡¿ï¼Œè¿™é€šå¸¸æ˜¯çœŸæ­£çš„åˆ†å‰²ç‚¹
        # å¯¹äºå¿«æ­Œï¼Œ75%çš„åœé¡¿å¯èƒ½éƒ½æ˜¯0.4sçš„å‘¼å¸ï¼Œè€Œç¬¬90%çš„é‚£ä¸ª0.8sçš„åœé¡¿æ‰æ˜¯æˆ‘ä»¬è¦æ‰¾çš„
        percentile_75 = np.percentile(all_candidate_durations, 75)
        percentile_90 = np.percentile(all_candidate_durations, 90)

        logger.info(f"åœé¡¿æ—¶é•¿ç»Ÿè®¡æ¨¡å‹: å¹³å‡å€¼={average_pause:.3f}s, ä¸­ä½æ•°={median_pause:.3f}s, 75åˆ†ä½={percentile_75:.3f}s, 90åˆ†ä½={percentile_90:.3f}s")
        logger.info(f"[DEBUG] åˆç­›å€™é€‰æ•°é‡: {len(all_candidate_durations)}, BPMç±»åˆ«: {getattr(self.current_adaptive_params, 'category', 'unknown')}")

        # === æ­¥éª¤ 3: "åŒæ¨¡å¼"æ™ºèƒ½è£å†³ ===
        valid_pauses = []
        total_audio_length = pause_segments[-1]['end'] if pause_segments else 0

        # è·å–MDDåˆ†æç»“æœï¼Œè¿™éœ€è¦ adaptive_enhancer åœ¨ä¸Šæ¸¸è¢«è°ƒç”¨å¹¶å­˜å‚¨ç»“æœ
        # æˆ‘ä»¬å‡è®¾ self.adaptive_enhancer.last_analyzed_segments å­˜åœ¨
        segments_with_mdd = getattr(self.adaptive_enhancer, 'last_analyzed_segments', [])

        for pause in pause_segments:
            duration_s = pause['duration']
            if duration_s < ABSOLUTE_MIN_PAUSE_S:
                continue

            # ç¡®å®šå½“å‰åœé¡¿æ‰€å¤„çš„éŸ³ä¹ç¯å¢ƒ (MDD)
            current_time = (pause['start'] / self.sample_rate)
            current_mdd = 0.5 # é»˜è®¤ä¸­ç­‰å¯†åº¦
            if segments_with_mdd:
                for seg in segments_with_mdd:
                    if seg.start_time <= current_time < seg.end_time:
                        current_mdd = seg.dynamic_density_score
                        break
            
            # å†³ç­–é€»è¾‘
            is_head = (pause.get('start', 0) == 0)
            is_tail = (pause.get('end', 0) >= total_audio_length * 0.95)
            
            # æ¨¡å¼ä¸€ï¼šå¿«æ­Œè£å†³ (BPM > 120) - å¯»æ‰¾ç»Ÿè®¡ä¸Šçš„"å¼‚å¸¸é•¿åœé¡¿"
            if self.current_adaptive_params.category in ['fast', 'very_fast']:
                # æ ¸å¿ƒä¿®å¤ï¼šå¯¹äºå¿«æ­Œï¼Œæˆ‘ä»¬çš„æ ‡å‡†æ˜¯"æ¯”å¤§éƒ¨åˆ†å‘¼å¸éƒ½é•¿"
                # æˆ‘ä»¬ä½¿ç”¨75åˆ†ä½æ•°ä½œä¸ºåŸºç¡€é˜ˆå€¼ï¼Œå› ä¸ºå®ƒèƒ½ä»£è¡¨è¿™é¦–æ­Œé‡Œ"æ¯”è¾ƒé•¿"çš„åœé¡¿æ˜¯å¤šé•¿
                dynamic_threshold = percentile_75 
                mode_type = "å¿«æ­Œæ¨¡å¼"
                # å¯¹äºéå¸¸æ¿€çƒˆçš„å‰¯æ­Œéƒ¨åˆ†ï¼ˆé«˜MDDï¼‰ï¼Œæˆ‘ä»¬ç”šè‡³å¯èƒ½éœ€è¦æ”¾å®½åˆ°ä¸­ä½æ•°ï¼Œåªæ±‚æœ‰å¾—åˆ‡
                if current_mdd > 0.7:
                    dynamic_threshold = median_pause
                    mode_type = "å¿«æ­Œ+é«˜å¯†åº¦æ¨¡å¼"
                
            # æ¨¡å¼äºŒï¼šæ…¢æ­Œ/ä¸­é€Ÿæ­Œè£å†³ (BPM <= 120) - å¯»æ‰¾"è¶³å¤Ÿé•¿ä¸”ç»“æ„åˆç†"çš„åœé¡¿
            else:
                # å¯¹äºæ…¢æ­Œï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼Œè¦æ±‚åœé¡¿å¿…é¡»æ˜¾è‘—é•¿äºå¹³å‡å‘¼å¸
                dynamic_threshold = max(average_pause, median_pause)
                mode_type = "æ…¢æ­Œæ¨¡å¼"
                # åœ¨æ¿€çƒˆçš„å‰¯æ­Œéƒ¨åˆ†ï¼ˆé«˜MDDï¼‰ï¼Œæˆ‘ä»¬æé«˜æ ‡å‡†ï¼Œé¿å…ä¹±åˆ‡
                if current_mdd > 0.6:
                    old_threshold = dynamic_threshold
                    dynamic_threshold *= (1 + (current_mdd - 0.6) * 0.5) # MDDè¶Šé«˜ï¼Œé˜ˆå€¼è¶Šé«˜
                    mode_type = "æ…¢æ­Œ+é«˜å¯†åº¦æ¨¡å¼"
                    logger.debug(f"[DEBUG] æ…¢æ­ŒMDDè°ƒæ•´: {old_threshold:.3f}s -> {dynamic_threshold:.3f}s (MDD={current_mdd:.2f})")

            # æœ€ç»ˆè£å†³
            final_threshold = max(dynamic_threshold, ABSOLUTE_MIN_PAUSE_S) # ä¿è¯ä¸ä½äºç»å¯¹ä¸‹é™
            
            # è¯¦ç»†å†³ç­–æ—¥å¿—
            if duration_s >= final_threshold or is_head or is_tail:
                logger.debug(f"[KEEP] @{current_time:.2f}s: {duration_s:.3f}s >= {final_threshold:.3f}s ({mode_type}, MDD={current_mdd:.2f})")
            else:
                logger.debug(f"[FILTER] @{current_time:.2f}s: {duration_s:.3f}s < {final_threshold:.3f}s ({mode_type}, MDD={current_mdd:.2f})")

            if duration_s >= final_threshold or is_head or is_tail:
                valid_pauses.append(pause)

        logger.info(f"åŒæ¨¡å¼æ™ºèƒ½è£å†³å®Œæˆ: {len(pause_segments)}ä¸ªå€™é€‰ -> {len(valid_pauses)}ä¸ªæœ€ç»ˆåˆ†å‰²ç‚¹")
        return valid_pauses
    
    def _get_mdd_score_for_pause(self, pause: Dict) -> float:
        """ä¸ºåœé¡¿ç‚¹è·å–MDDï¼ˆéŸ³ä¹åŠ¨æ€å¯†åº¦ï¼‰è¯„åˆ†
        
        Args:
            pause: åœé¡¿ä¿¡æ¯å­—å…¸
            
        Returns:
            MDDè¯„åˆ† (0-1, è¶Šé«˜è¡¨ç¤ºéŸ³ä¹è¶Šæ¿€çƒˆï¼Œè¶Šä¸åº”è¯¥åˆ‡å‰²)
        """
        if not self.adaptive_enhancer or not hasattr(self.adaptive_enhancer, 'last_analyzed_segments'):
            return 0.5  # é»˜è®¤ä¸­ç­‰å¯†åº¦
        
        # è®¡ç®—åœé¡¿çš„ä¸­å¿ƒæ—¶é—´ç‚¹
        pause_center_time = ((pause['start'] + pause['end']) / 2.0) / self.sample_rate
        
        # åœ¨åˆ†æçš„ç‰‡æ®µä¸­æ‰¾åˆ°å¯¹åº”çš„MDDè¯„åˆ†
        for segment in self.adaptive_enhancer.last_analyzed_segments:
            if segment.start_time <= pause_center_time < segment.end_time:
                return segment.dynamic_density_score
        
        # å¦‚æœæ²¡æ‰¾åˆ°å¯¹åº”ç‰‡æ®µï¼Œä½¿ç”¨ç›¸é‚»ç‰‡æ®µçš„å¹³å‡å€¼
        segments = self.adaptive_enhancer.last_analyzed_segments
        if not segments:
            return 0.5
        
        # æ‰¾åˆ°æœ€è¿‘çš„ç‰‡æ®µ
        closest_segment = min(segments, key=lambda s: min(
            abs(s.start_time - pause_center_time),
            abs(s.end_time - pause_center_time)
        ))
        
        return closest_segment.dynamic_density_score
    
    def _filter_simple_pauses(self, pause_segments: List[Dict]) -> List[Dict]:
        """ç®€å•çš„é™æ€é˜ˆå€¼è¿‡æ»¤ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        min_pause_duration = self.current_adaptive_params.min_pause_duration if self.current_adaptive_params else get_config('vocal_pause_splitting.min_pause_duration', 1.0)
        min_pause_samples = int(min_pause_duration * self.sample_rate)
        
        valid_pauses = []
        for pause in pause_segments:
            duration_samples = pause['end'] - pause['start']
            if duration_samples >= min_pause_samples:
                pause['duration'] = duration_samples / self.sample_rate
                valid_pauses.append(pause)
        
        logger.info(f"ç®€å•è¿‡æ»¤ï¼šä¿ç•™ {len(valid_pauses)} ä¸ªåœé¡¿ (æœ€å° > {min_pause_duration:.2f}s)")
        return valid_pauses
    
    def _is_chorus_section(self, pause: Dict) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºå‰¯æ­Œéƒ¨åˆ†ï¼ˆåŸºäºèƒ½é‡å’Œé¢‘è°±ç‰¹å¾ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªå ä½å®ç°ï¼Œå¯ä»¥åç»­å¢å¼º
        # å¯ä»¥é€šè¿‡åˆ†æpauseå‰åçš„éŸ³é¢‘èƒ½é‡ã€é¢‘è°±å¤æ‚åº¦ç­‰åˆ¤æ–­
        return False
        
    def _classify_pause_positions(self, valid_pauses: List[Dict], audio_length: int) -> List[VocalPause]:
        """åˆ†ç±»åœé¡¿ä½ç½®ï¼ˆå¤´éƒ¨/ä¸­é—´/å°¾éƒ¨ï¼‰"""
        vocal_pauses = []

        for pause in valid_pauses:
            start_time = pause['start'] / self.sample_rate
            end_time = pause['end'] / self.sample_rate
            duration = pause['duration']

            # åˆ¤æ–­åœé¡¿ä½ç½®ç±»å‹
            if pause['start'] == 0:
                # å¤´éƒ¨åœé¡¿
                position_type = 'head'
            elif pause['end'] == audio_length:
                # å°¾éƒ¨åœé¡¿
                position_type = 'tail'
            else:
                # ä¸­é—´åœé¡¿
                position_type = 'middle'

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºåœé¡¿æ—¶é•¿ï¼Œè¶Šé•¿ç½®ä¿¡åº¦è¶Šé«˜ï¼‰
            min_pause_duration = self.current_adaptive_params.min_pause_duration if self.current_adaptive_params else 1.0
            confidence = min(1.0, duration / (min_pause_duration * 2))

            vocal_pause = VocalPause(
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                position_type=position_type,
                confidence=confidence,
                cut_point=0.0  # ç¨åè®¡ç®—
            )
            vocal_pauses.append(vocal_pause)

        return vocal_pauses

    def _calculate_cut_points(self, vocal_pauses: List[VocalPause], bpm_features: Optional['BPMFeatures'] = None, waveform: Optional[np.ndarray] = None) -> List[VocalPause]:
        """è®¡ç®—æœ€ç»ˆåˆ‡ç‚¹ï¼Œé›†æˆèƒ½é‡è°·å’ŒèŠ‚æ‹å¯¹é½"""
        logger.info(f"è®¡ç®— {len(vocal_pauses)} ä¸ªåœé¡¿çš„åˆ‡å‰²ç‚¹ (èƒ½é‡è°·æœ€ä¼˜ + BPMæ™ºèƒ½èåˆæ¨¡å¼)...")

        for i, pause in enumerate(vocal_pauses):
            # 1. ä¸ºèƒ½é‡è°·æœç´¢å®šä¹‰ä¸€ä¸ªå®‰å…¨çš„èŒƒå›´
            search_start, search_end = self._define_search_range(pause)
            
            logger.debug(f"åœé¡¿ {i+1} ({pause.position_type}): åŸå§‹èŒƒå›´ [{pause.start_time:.3f}s, {pause.end_time:.3f}s], "
                        f"èƒ½é‡è°·æœç´¢èŒƒå›´ [{search_start:.3f}s, {search_end:.3f}s]")

            # 2. å¼ºåˆ¶å¯»æ‰¾ç‰©ç†ä¸Šçš„èƒ½é‡æœ€ä½ç‚¹ä½œä¸ºåŸºå‡†
            valley_point_s = self._find_energy_valley(waveform, search_start, search_end)
            if valley_point_s is None:
                # å¦‚æœæ‰¾ä¸åˆ°èƒ½é‡è°·ï¼Œä½¿ç”¨åœé¡¿ä¸­å¿ƒä½œä¸ºå…œåº•
                valley_point_s = (pause.start_time + pause.end_time) / 2
                logger.warning(f"  -> æœªæ‰¾åˆ°èƒ½é‡è°·ï¼Œå›é€€åˆ°ä¸­å¿ƒç‚¹: {valley_point_s:.3f}s")

            # 3. å¦‚æœBPMä¿¡æ¯å¯ç”¨ï¼Œè¿›è¡Œæ™ºèƒ½å¯¹é½ï¼ˆä»¥èƒ½é‡è°·ä¸ºåŸºç¡€ï¼‰
            final_cut_point_s = valley_point_s
            if bpm_features and self.current_adaptive_params:
                final_cut_point_s = self._smart_beat_align(
                    waveform, valley_point_s, bpm_features, search_start, search_end
                )

            # 4. æ›´æ–°æœ€ç»ˆåˆ‡ç‚¹
            pause.cut_point = final_cut_point_s
            logger.info(f"åœé¡¿ {i+1} ({pause.position_type}): æœ€ç»ˆåˆ‡ç‚¹ @ {pause.cut_point:.3f}s")

        return vocal_pauses

    def _define_search_range(self, pause: VocalPause) -> Tuple[float, float]:
        """ä¸ºèƒ½é‡è°·æœç´¢å®šä¹‰ä¸€ä¸ªå®‰å…¨çš„èŒƒå›´ï¼Œå·§å¦™åˆ©ç”¨offsetå‚æ•°"""
        search_start = pause.start_time
        search_end = pause.end_time
        
        # åº”ç”¨åç§»é‡æ¥æŒ‡å¯¼æœç´¢èŒƒå›´ï¼Œè€Œä¸æ˜¯ç›´æ¥å†³å®šåˆ‡ç‚¹
        if pause.position_type == 'head':
            search_start = max(search_start, pause.end_time + self.head_offset - 0.5)
            search_end = min(search_end, pause.end_time + self.head_offset + 0.5)
        elif pause.position_type == 'tail':
            search_start = max(search_start, pause.start_time + self.tail_offset - 0.5)
            search_end = min(search_end, pause.start_time + self.tail_offset + 0.5)
        
        return (search_start, search_end) if search_end > search_start else (pause.start_time, pause.end_time)

    def _find_energy_valley(self, waveform: Optional[np.ndarray], start_s: float, end_s: float) -> Optional[float]:
        """åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…å¯»æ‰¾èƒ½é‡æœ€ä½ç‚¹ï¼Œå¹¶åº”ç”¨å®‰å…¨å®ˆå«"""
        if waveform is None or len(waveform) == 0:
            return None

        # ä»é…ç½®ä¸­è·å–èƒ½é‡è°·æ£€æµ‹çš„ç²¾ç»†å‚æ•°
        local_rms_ms = int(get_config('vocal_pause_splitting.local_rms_window_ms', 25))
        guard_ms = int(get_config('vocal_pause_splitting.lookahead_guard_ms', 120))
        floor_pct = float(get_config('vocal_pause_splitting.silence_floor_percentile', 5))

        l_idx = max(0, int(start_s * self.sample_rate))
        r_idx = min(len(waveform), int(end_s * self.sample_rate))

        if r_idx > l_idx:
            # è°ƒç”¨åº•å±‚çš„èƒ½é‡è°·æœç´¢å‡½æ•°
            valley_idx = self._select_valley_cut_point(
                waveform, l_idx, r_idx, self.sample_rate,
                local_rms_ms, guard_ms, floor_pct
            )
            return valley_idx / self.sample_rate if valley_idx is not None else None
        return None

    def _smart_beat_align(self, waveform: np.ndarray, valley_point_s: float, bpm_features: 'BPMFeatures', search_start_s: float, search_end_s: float) -> float:
        """æ™ºèƒ½èŠ‚æ‹å¯¹é½ï¼šåœ¨èƒ½é‡è°·åˆ’å®šçš„å®‰é™åŒºå†…å¯»æ‰¾èŠ‚æ‹ç‚¹
        
        æ ¸å¿ƒåŸåˆ™ï¼šèƒ½é‡è°·æœ€ä¼˜ï¼ŒBPMä»…ä¸ºè¾…åŠ©ï¼Œç»ä¸å…è®¸åˆ‡åœ¨äººå£°ä¸Š
        """
        beat_interval = 60.0 / float(bpm_features.main_bpm)
        nearest_beat_s = round(valley_point_s / beat_interval) * beat_interval

        # å®‰å…¨æ£€æŸ¥1ï¼šèŠ‚æ‹ç‚¹å¿…é¡»åœ¨æœç´¢èŒƒå›´å†…
        if not (search_start_s <= nearest_beat_s <= search_end_s):
            logger.debug(f"  èŠ‚æ‹ç‚¹ {nearest_beat_s:.3f}s è¶…å‡ºæœç´¢èŒƒå›´ï¼Œåšå®ˆèƒ½é‡è°·ç‚¹ {valley_point_s:.3f}s")
            return valley_point_s

        # å®‰å…¨æ£€æŸ¥2ï¼šä¸¥æ ¼èƒ½é‡æ ¡éªŒï¼Œç»ä¸å…è®¸åˆ‡åœ¨äººå£°ä¸Š
        valley_idx = int(valley_point_s * self.sample_rate)
        beat_idx = int(nearest_beat_s * self.sample_rate)
        
        win_size = int(0.05 * self.sample_rate) # 50msèƒ½é‡æ¯”è¾ƒçª—å£
        
        valley_energy = np.mean(waveform[max(0, valley_idx - win_size//2) : valley_idx + win_size//2]**2)
        beat_energy = np.mean(waveform[max(0, beat_idx - win_size//2) : beat_idx + win_size//2]**2)

        # å…³é”®ä¿®å¤ï¼šä¸¥æ ¼çš„èƒ½é‡å®¹å¿åº¦ï¼Œä¼˜å…ˆç‰©ç†é™éŸ³
        energy_tolerance_ratio = 1.3  # é™ä½å®¹å¿åº¦ï¼Œæ›´ä¸¥æ ¼

        if beat_energy <= valley_energy * energy_tolerance_ratio:
            logger.debug(f"  æ™ºèƒ½å¯¹é½ï¼šèŠ‚æ‹ç‚¹ {nearest_beat_s:.3f}s èƒ½é‡éªŒè¯é€šè¿‡ (Beat={beat_energy:.2e} â‰¤ Valley*{energy_tolerance_ratio}={valley_energy*energy_tolerance_ratio:.2e})")
            return nearest_beat_s
        else:
            logger.debug(f"  æ™ºèƒ½å¯¹é½æ‹’ç»ï¼šèŠ‚æ‹ç‚¹èƒ½é‡è¿‡é«˜ (Beat={beat_energy:.2e} > Valley*{energy_tolerance_ratio}={valley_energy*energy_tolerance_ratio:.2e})ï¼Œåšå®ˆèƒ½é‡è°·")
            return valley_point_s

    def _select_valley_cut_point(self, waveform: np.ndarray, left_idx: int, right_idx: int,
                                 sample_rate: int, local_rms_ms: int, guard_ms: int, floor_percentile: float) -> Optional[int]:
        """åœ¨[left_idx, right_idx]å†…é€‰æ‹©RMSè°·å€¼åˆ‡ç‚¹ï¼Œå¸¦æœªæ¥é™é»˜å®ˆå«ï¼›è‹¥å¤±è´¥è¿”å›Noneã€‚"""
        left_idx = max(0, int(left_idx)); right_idx = min(len(waveform), int(right_idx))
        if right_idx - left_idx <= 8:
            return None
        
        win_samples = max(1, int(local_rms_ms / 1000.0 * sample_rate))
        guard_samples = max(1, int(guard_ms / 1000.0 * sample_rate)) if guard_ms and guard_ms > 0 else 0
        segment = waveform[left_idx:right_idx]
        rms = self._compute_rms_envelope(segment, win_samples)
        floor_val = np.percentile(np.abs(segment), floor_percentile)
        
        # å€™é€‰æŒ‰RMSå‡åºå°è¯•ï¼Œä¼˜å…ˆæ›´"é™"çš„ç‚¹
        order = np.argsort(rms)
        max_try = min(200, len(order))
        # è¾¹ç•Œä¿æŠ¤ï¼šè‡³å°‘è·å·¦å³è¾¹ç•Œå„20msï¼Œé¿å…è´´è¾¹åˆ‡
        margin_samples = max(1, int(0.02 * sample_rate))
        
        for k in range(max_try):
            j = int(order[k])
            if j < margin_samples or j > (len(rms) - margin_samples):
                continue
                
            if guard_samples > 0:
                if self._future_silence_guard(rms, j, guard_samples, floor_val):
                    return left_idx + j
            else:
                return left_idx + j
                
        # å…œåº•ï¼šé€‰æ‹©æ»¡è¶³è¾¹ç•Œä¿æŠ¤çš„RMSæœ€å°ç‚¹
        if order.size > 0:
            for jj in order:
                j = int(jj)
                if j >= margin_samples and j <= (len(rms) - margin_samples):
                    return left_idx + j
            j = int(order[0])
            j = max(margin_samples, min(len(rms) - margin_samples, j))
            return left_idx + j
        return None

    def _compute_rms_envelope(self, waveform: np.ndarray, win_samples: int) -> np.ndarray:
        """è®¡ç®—ç®€æ˜“æ»‘åŠ¨RMSåŒ…ç»œï¼ˆsameå¯¹é½ï¼‰ã€‚"""
        if win_samples <= 1:
            return np.abs(waveform).astype(np.float32)
        kernel = np.ones(int(win_samples), dtype=np.float32) / float(max(1, int(win_samples)))
        return np.sqrt(np.convolve((waveform.astype(np.float32) ** 2), kernel, mode='same'))

    def _future_silence_guard(self, rms: np.ndarray, start_idx: int, guard_samples: int, floor_val: float,
                               allowance: float = 1.2, ratio: float = 0.7) -> bool:
        """æœªæ¥é™é»˜å®ˆå«ï¼šåœ¨ [start_idx, start_idx+guard] å†…å¤šæ•°æ ·æœ¬ä½äº floorÃ—allowanceã€‚"""
        end_idx = min(start_idx + max(0, int(guard_samples)), len(rms))
        if end_idx <= start_idx:
            return False
        window = rms[start_idx:end_idx]
        if window.size == 0:
            return False
        below = np.sum(window <= (floor_val * allowance))
        return (below / float(window.size)) >= ratio