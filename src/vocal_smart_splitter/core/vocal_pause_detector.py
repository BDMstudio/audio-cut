#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# vocal_smart_splitter/core/vocal_pause_detector.py
# AI-SUMMARY: äººå£°åœé¡¿æ£€æµ‹å™¨ - ä½¿ç”¨Silero VADç›´æ¥åœ¨åŸå§‹éŸ³é¢‘ä¸Šæ£€æµ‹äººå£°åœé¡¿

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

from ..utils.config_manager import get_config
from ..utils.adaptive_parameter_calculator import create_adaptive_calculator, AdaptiveParameters

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥è‡ªé€‚åº”å¢å¼ºå™¨
try:
    from .adaptive_vad_enhancer import AdaptiveVADEnhancer
    ADAPTIVE_VAD_AVAILABLE = True
    logger.info("è‡ªé€‚åº”VADå¢å¼ºå™¨å¯ç”¨")
except ImportError as e:
    logger.warning(f"è‡ªé€‚åº”VADå¢å¼ºå™¨ä¸å¯ç”¨: {e}")
    ADAPTIVE_VAD_AVAILABLE = False

@dataclass
class VocalPause:
    """äººå£°åœé¡¿æ•°æ®ç»“æ„"""
    start_time: float        # åœé¡¿å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end_time: float          # åœé¡¿ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    duration: float          # åœé¡¿æ—¶é•¿ï¼ˆç§’ï¼‰
    position_type: str       # ä½ç½®ç±»å‹ï¼š'head', 'middle', 'tail'
    confidence: float        # ç½®ä¿¡åº¦ (0-1)
    cut_point: float         # åˆ‡å‰²ç‚¹æ—¶é—´ï¼ˆç§’ï¼‰

class VocalPauseDetectorV2:
    """æ”¹è¿›çš„äººå£°åœé¡¿æ£€æµ‹å™¨ - ç›´æ¥åœ¨åŸå§‹éŸ³é¢‘ä¸Šä½¿ç”¨Silero VAD"""

    def __init__(self, sample_rate: int = 44100):
        """åˆå§‹åŒ–äººå£°åœé¡¿æ£€æµ‹å™¨ (v1.2.0 - BPMè‡ªé€‚åº”å¢å¼º)

        Args:
            sample_rate: é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate

        # ğŸ†• v1.2.0: BPMè‡ªé€‚åº”å‚æ•°è®¡ç®—å™¨
        self.adaptive_calculator = create_adaptive_calculator()

        # ğŸ”„ åŠ¨æ€å‚æ•°ï¼ˆå°†è¢«AdaptiveParameterCalculatorè¦†ç›–ï¼‰
        self.current_adaptive_params: Optional[AdaptiveParameters] = None

        # é™æ€é…ç½®å‚æ•°ï¼ˆä¸å—BPMå½±å“ï¼‰
        self.min_confidence = get_config('vocal_pause_splitting.min_confidence', 0.5)
        self.head_offset = get_config('vocal_pause_splitting.head_offset', -0.5)
        self.tail_offset = get_config('vocal_pause_splitting.tail_offset', 0.5)

        # âŒ ä»¥ä¸‹å‚æ•°å·²è¿ç§»åˆ°åŠ¨æ€è®¡ç®—ï¼ˆä¿ç•™ä½œä¸ºfallbackï¼‰
        self.fallback_min_pause_duration = get_config('vocal_pause_splitting.min_pause_duration', 1.0)
        self.fallback_voice_threshold = get_config('vocal_pause_splitting.voice_threshold', 0.3)

        # ğŸ”„ åˆå§‹åŒ–æ—¶ä½¿ç”¨fallbackå€¼ï¼ˆå°†åœ¨æ£€æµ‹æ—¶è¢«åŠ¨æ€å‚æ•°è¦†ç›–ï¼‰
        self.min_pause_duration = self.fallback_min_pause_duration
        self.voice_threshold = self.fallback_voice_threshold

        # BPMæ„ŸçŸ¥è‡ªé€‚åº”å¢å¼ºå™¨
        self.enable_bpm_adaptation = get_config('vocal_pause_splitting.enable_bpm_adaptation', True)
        self.adaptive_enhancer = None

        if self.enable_bpm_adaptation and ADAPTIVE_VAD_AVAILABLE:
            try:
                self.adaptive_enhancer = AdaptiveVADEnhancer(sample_rate)
                logger.info("BPMè‡ªé€‚åº”å¢å¼ºå™¨å·²å¯ç”¨")
            except Exception as e:
                logger.warning(f"BPMè‡ªé€‚åº”å¢å¼ºå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_bpm_adaptation = False
        else:
            logger.info("ä½¿ç”¨å›ºå®šé˜ˆå€¼VADæ¨¡å¼")

        # åˆå§‹åŒ–Silero VAD
        self._init_silero_vad()

        logger.info(f"äººå£°åœé¡¿æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ (é‡‡æ ·ç‡: {sample_rate}, BPMè‡ªé€‚åº”: {'å¼€å¯' if self.enable_bpm_adaptation else 'å…³é—­'})")

    def apply_adaptive_parameters(self, bpm: float, complexity: float, instrument_count: int):
        """åº”ç”¨BPMè‡ªé€‚åº”å‚æ•° (v1.2.0)

        Args:
            bpm: æ£€æµ‹åˆ°çš„BPMå€¼
            complexity: ç¼–æ›²å¤æ‚åº¦ (0-1)
            instrument_count: ä¹å™¨æ•°é‡
        """
        try:
            # ä½¿ç”¨AdaptiveParameterCalculatorè®¡ç®—å‚æ•°
            self.current_adaptive_params = self.adaptive_calculator.calculate_all_parameters(
                bpm, complexity, instrument_count
            )

            # åº”ç”¨åŠ¨æ€å‚æ•°åˆ°é…ç½®ç³»ç»Ÿ
            override_params = self.adaptive_calculator.get_static_override_parameters(
                self.current_adaptive_params
            )
            self.adaptive_calculator.apply_dynamic_parameters(
                self.current_adaptive_params, override_params
            )

            # æ›´æ–°å®ä¾‹å˜é‡ï¼ˆç”¨äºç›´æ¥è®¿é—®ï¼‰
            self.min_pause_duration = self.current_adaptive_params.min_pause_duration
            self.voice_threshold = self.current_adaptive_params.vad_threshold

            logger.info("=== BPMè‡ªé€‚åº”å‚æ•°å·²åº”ç”¨ ===")
            logger.info(f"BPM: {self.current_adaptive_params.bpm_value} ({self.current_adaptive_params.category})")
            logger.info(f"åœé¡¿æ—¶é•¿: {self.current_adaptive_params.min_pause_duration:.3f}s")
            logger.info(f"VADé˜ˆå€¼: {self.current_adaptive_params.vad_threshold:.3f}")
            logger.info(f"è¡¥å¿ç³»æ•°: {self.current_adaptive_params.compensation_factor:.3f}")

            return True

        except Exception as e:
            logger.error(f"åº”ç”¨BPMè‡ªé€‚åº”å‚æ•°å¤±è´¥: {e}")
            # ä½¿ç”¨fallbackå‚æ•°
            self.min_pause_duration = self.fallback_min_pause_duration
            self.voice_threshold = self.fallback_voice_threshold
            return False

    def get_current_parameters_info(self) -> Dict:
        """è·å–å½“å‰å‚æ•°ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰"""
        if self.current_adaptive_params:
            return {
                'mode': 'adaptive',
                'bpm': self.current_adaptive_params.bpm_value,
                'category': self.current_adaptive_params.category,
                'min_pause_duration': self.current_adaptive_params.min_pause_duration,
                'vad_threshold': self.current_adaptive_params.vad_threshold,
                'compensation_factor': self.current_adaptive_params.compensation_factor,
                'complexity_score': self.current_adaptive_params.complexity_score,
                'instrument_count': self.current_adaptive_params.instrument_count
            }
        else:
            return {
                'mode': 'fallback',
                'min_pause_duration': self.fallback_min_pause_duration,
                'vad_threshold': self.fallback_voice_threshold
            }

    def _init_silero_vad(self):
        """åˆå§‹åŒ–Silero VAD"""
        try:
            import torch
            torch.set_num_threads(1)

            # ä¸‹è½½å¹¶åŠ è½½Silero VADæ¨¡å‹
            model, utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False
            )

            self.vad_model = model
            self.vad_utils = utils
            (self.get_speech_timestamps,
             self.save_audio, self.read_audio,
             self.VADIterator, self.collect_chunks) = utils

            logger.info("Silero VADæ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            logger.error(f"Silero VADåˆå§‹åŒ–å¤±è´¥: {e}")
            self.vad_model = None

    def detect_vocal_pauses(self, original_audio: np.ndarray) -> List[VocalPause]:
        """æ£€æµ‹äººå£°åœé¡¿ï¼ˆé›†æˆBPMæ„ŸçŸ¥è‡ªé€‚åº”å¢å¼ºï¼‰

        Args:
            original_audio: åŸå§‹éŸ³é¢‘ï¼ˆåŒ…å«èƒŒæ™¯éŸ³ä¹ï¼‰

        Returns:
            æ£€æµ‹åˆ°çš„äººå£°åœé¡¿åˆ—è¡¨
        """
        logger.info("å¼€å§‹BPMæ„ŸçŸ¥çš„äººå£°åœé¡¿æ£€æµ‹...")

        try:
            if self.vad_model is None:
                logger.error("Silero VADæ¨¡å‹æœªåŠ è½½")
                return []

            # å­˜å‚¨åˆ†æç»“æœç”¨äºè‡ªé€‚åº”è°ƒæ•´
            complexity_segments = None
            bpm_features = None

            # 1. BPMæ„ŸçŸ¥å¤æ‚åº¦åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_bpm_adaptation and self.adaptive_enhancer:
                logger.info("æ‰§è¡ŒBPMæ„ŸçŸ¥çš„ç¼–æ›²å¤æ‚åº¦åˆ†æ...")
                complexity_segments, bpm_features = self.adaptive_enhancer.analyze_arrangement_complexity(original_audio)

                if complexity_segments and bpm_features:
                    logger.info(f"ğŸµ éŸ³é¢‘åˆ†æå®Œæˆ: {float(bpm_features.main_bpm):.1f} BPM ({bpm_features.bpm_category})")
                    # ğŸ†• å­˜å‚¨ä¹å™¨å¤æ‚åº¦åˆ†æç»“æœç”¨äºå¤šä¹å™¨ç¯å¢ƒä¼˜åŒ–
                    if hasattr(self.adaptive_enhancer, 'last_instrument_analysis'):
                        self.last_complexity_analysis = self.adaptive_enhancer.last_instrument_analysis
                else:
                    logger.warning("å¤æ‚åº¦åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼æ¨¡å¼")
                    self.enable_bpm_adaptation = False

            # 2. è‡ªé€‚åº”VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³
            speech_timestamps = self._detect_adaptive_speech_timestamps(
                original_audio, complexity_segments, bpm_features
            )

            # 3. è®¡ç®—åœé¡¿åŒºåŸŸï¼ˆè¯­éŸ³ç‰‡æ®µä¹‹é—´çš„é—´éš™ï¼‰
            pause_segments = self._calculate_pause_segments(speech_timestamps, len(original_audio))

            # 4. è‡ªé€‚åº”è¿‡æ»¤æœ‰æ•ˆåœé¡¿
            valid_pauses = self._filter_adaptive_pauses(pause_segments, complexity_segments, bpm_features)

            # 4.b å¯é€‰ï¼šåœ¨â€œé•¿è¯­éŸ³æ®µâ€å†…æŒ‰ç½‘æ ¼æ‰«æ valley ç”Ÿæˆåˆæˆåœé¡¿ï¼ˆé»˜è®¤å…³é—­ï¼Œé›¶ç ´åï¼‰
            try:
                enable_voiced_valley = bool(get_config('vocal_pause_splitting.voiced_valley_fallback.enable', False))
            except Exception:
                enable_voiced_valley = False
            if enable_voiced_valley and original_audio is not None and len(original_audio) > 0 and speech_timestamps:
                try:
                    sr = self.sample_rate
                    local_rms_ms = int(get_config('vocal_pause_splitting.local_rms_window_ms', 25))
                    floor_pct = float(get_config('vocal_pause_splitting.silence_floor_percentile', 5))
                    min_gap_s = float(get_config('vocal_pause_splitting.voiced_valley_fallback.min_gap_s', 6.0))
                    synth_window_s = float(get_config('vocal_pause_splitting.voiced_valley_fallback.window_s', 0.30))
                    half_win = max(0.05, synth_window_s / 2.0)
                    added = 0
                    for ts in speech_timestamps:
                        seg_start_s = ts['start'] / sr
                        seg_end_s = ts['end'] / sr
                        seg_dur = seg_end_s - seg_start_s
                        if seg_dur >= max(min_gap_s * 1.2, min_gap_s + 1.0):
                            # ç½‘æ ¼ä¸­å¿ƒä» half-step å¼€å§‹ï¼Œè¦†ç›–å·¦ä¾§åŠä¸ªé—´éš”ï¼Œé¿å…æ¼æ‰æ®µé¦–çš„æ˜æ˜¾è°·
                            step = float(min_gap_s)
                            center = seg_start_s + step * 0.5
                            while center <= (seg_end_s - step * 0.5):
                                l_idx = max(ts['start'], int((center - 0.50) * sr))
                                r_idx = min(ts['end'],   int((center + 0.50) * sr))
                                if r_idx - l_idx <= int(0.10 * sr):
                                    center += step
                                    continue
                                v_idx = self._select_valley_cut_point(original_audio, l_idx, r_idx, sr, local_rms_ms, -1, floor_pct)
                                if v_idx is not None:
                                    cp = v_idx / float(sr)
                                    s = max(seg_start_s, cp - half_win)
                                    e = min(seg_end_s,   cp + half_win)
                                    if (e - s) >= 0.08:
                                        # æ ‡è®°ä¸ºå¼ºåˆ¶ valleyï¼Œä»¥åœ¨åˆ‡ç‚¹é˜¶æ®µè·³è¿‡â€œä¸­å¿ƒå³å+å®½åŠå¾„é›¶äº¤å‰â€çš„æ¼‚ç§»
                                        valid_pauses.append({'start': int(s * sr), 'end': int(e * sr), 'duration': (e - s), 'force_valley': True})
                                        added += 1
                                center += step
                    if added > 0:
                        logger.info(f"voiced_valley_fallback: [32m+{added}[0m synthetic pauses added within voiced segments")
                except Exception as _e:
                    logger.warning(f"voiced_valley_fallback failed: {_e}")

            # 5. åˆ†ç±»åœé¡¿ä½ç½®ï¼ˆå¤´éƒ¨/ä¸­é—´/å°¾éƒ¨ï¼‰
            vocal_pauses = self._classify_pause_positions(valid_pauses, speech_timestamps, len(original_audio))

            # 6. è®¡ç®—åˆ‡å‰²ç‚¹ï¼ˆé™éŸ³å¹³å°ä¸­å¿ƒ+å³å+é›¶äº¤å‰å¸é™„ï¼‰
            vocal_pauses = self._calculate_cut_points(vocal_pauses, bpm_features, original_audio)

            # 7. BPMæ„ŸçŸ¥çš„åœé¡¿ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_bpm_adaptation and bpm_features:
                vocal_pauses = self._optimize_pauses_with_bpm(vocal_pauses, bpm_features)

            logger.info(f"æ£€æµ‹åˆ° {len(vocal_pauses)} ä¸ªæœ‰æ•ˆäººå£°åœé¡¿")
            if self.enable_bpm_adaptation and bpm_features:
                logger.info(f"ğŸµ BPMè‡ªé€‚åº”ä¼˜åŒ–å®Œæˆ ({bpm_features.bpm_category}éŸ³ä¹)")

            return vocal_pauses

        except Exception as e:
            logger.error(f"BPMæ„ŸçŸ¥äººå£°åœé¡¿æ£€æµ‹å¤±è´¥: {e}")
            return []

    def _detect_speech_timestamps(self, audio: np.ndarray) -> List[Dict]:
        """ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³

        Args:
            audio: éŸ³é¢‘æ•°æ®

        Returns:
            è¯­éŸ³æ—¶é—´æˆ³åˆ—è¡¨ [{'start': int, 'end': int}] (æ ·æœ¬ç´¢å¼•)
        """
        try:
            import torch
            import librosa

            # Silero VADåªæ”¯æŒ16000Hzï¼Œéœ€è¦é‡é‡‡æ ·
            target_sr = 16000
            if self.sample_rate != target_sr:
                audio_resampled = librosa.resample(audio, orig_sr=self.sample_rate, target_sr=target_sr)
            else:
                audio_resampled = audio

            # è½¬æ¢ä¸ºtorch tensor
            audio_tensor = torch.from_numpy(audio_resampled).float()

            # ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³
            # âœ… --- æ ¸å¿ƒä¿®å¤ï¼šä»configå®æ—¶è¯»å–æ‰€æœ‰VADå‚æ•° ---
            logger.debug("å®æ—¶ä»configåŠ è½½VADå‚æ•°...")
            
            # ä» 'advanced_vad' éƒ¨åˆ†è¯»å–ï¼Œè¿™æ˜¯æˆ‘ä»¬æ–°çš„â€œé»„é‡‘å‚æ•°â€å­˜æ”¾åœ°
            vad_threshold = get_config('advanced_vad.silero_prob_threshold_down', 0.35)
            min_speech_ms = get_config('advanced_vad.silero_min_speech_ms', 250)
            min_silence_ms = get_config('advanced_vad.silero_min_silence_ms', 700)
            window_size = get_config('advanced_vad.silero_window_size_samples', 512)
            pad_ms = get_config('advanced_vad.silero_speech_pad_ms', 150)

            logger.info(f"åº”ç”¨VADå‚æ•°: threshold={vad_threshold}, min_speech={min_speech_ms}ms, min_silence={min_silence_ms}ms")
            
            # ä½¿ç”¨Silero VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³
            speech_timestamps = self.get_speech_timestamps(
                audio_tensor, 
                self.vad_model,
                sampling_rate=target_sr,
                threshold=vad_threshold,           # âœ… ä½¿ç”¨å®æ—¶è¯»å–çš„é˜ˆå€¼
                min_speech_duration_ms=min_speech_ms, # âœ… ä½¿ç”¨å®æ—¶è¯»å–çš„å‚æ•°
                min_silence_duration_ms=min_silence_ms, # âœ… ä½¿ç”¨å®æ—¶è¯»å–çš„å‚æ•°
                window_size_samples=window_size,     # âœ… ä½¿ç”¨å®æ—¶è¯»å–çš„å‚æ•°
                speech_pad_ms=pad_ms                 # âœ… ä½¿ç”¨å®æ—¶è¯»å–çš„å‚æ•°
            )

            # å°†æ—¶é—´æˆ³æ˜ å°„å›åŸå§‹é‡‡æ ·ç‡ï¼ˆä½¿ç”¨æ­£ç¡®çš„è·¨åŸŸæ˜ å°„ï¼‰
            # å°†æ—¶é—´æˆ³æ˜ å°„å›åŸå§‹é‡‡æ ·ç‡
            if self.sample_rate != target_sr:
                scale_factor = self.sample_rate / target_sr
                for ts in speech_timestamps:
                    ts['start'] = int(ts['start'] * scale_factor)
                    ts['end'] = int(ts['end'] * scale_factor)
            
            logger.info(f"Silero VADæ£€æµ‹ç»“æœ: {len(speech_timestamps)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            
            # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            for i, ts in enumerate(speech_timestamps[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                start_sec = ts['start'] / self.sample_rate
                end_sec = ts['end'] / self.sample_rate
                duration = end_sec - start_sec
                logger.info(f"  è¯­éŸ³ç‰‡æ®µ{i+1}: {start_sec:.2f}s - {end_sec:.2f}s (æ—¶é•¿: {duration:.2f}s)")
            
            if len(speech_timestamps) > 10:
                logger.info(f"  ... è¿˜æœ‰ {len(speech_timestamps)-10} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            
            return speech_timestamps
            
        except Exception as e:
            logger.error(f"Silero VADæ£€æµ‹å¤±è´¥: {e}")
            return []

    def _detect_adaptive_speech_timestamps(
        self, audio: np.ndarray, complexity_segments=None, bpm_features=None
    ) -> List[Dict]:
        """è‡ªé€‚åº”VADæ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³ï¼ˆé›†æˆBPMæ„ŸçŸ¥ï¼‰

        Args:
            audio: éŸ³é¢‘æ•°æ®
            complexity_segments: ç¼–æ›²å¤æ‚åº¦ç‰‡æ®µï¼ˆå¯é€‰ï¼‰
            bpm_features: BPMç‰¹å¾ï¼ˆå¯é€‰ï¼‰

        Returns:
            è¯­éŸ³æ—¶é—´æˆ³åˆ—è¡¨
        """
        if not self.enable_bpm_adaptation or not complexity_segments or not bpm_features:
            # ä½¿ç”¨å›ºå®šé˜ˆå€¼çš„åŸå§‹æ–¹æ³•
            return self._detect_speech_timestamps(audio)

        try:
            import torch
            import librosa

            # é‡é‡‡æ ·åˆ°Silero VADæ”¯æŒçš„é‡‡æ ·ç‡
            target_sr = 16000
            if self.sample_rate != target_sr:
                audio_resampled = librosa.resample(audio, orig_sr=self.sample_rate, target_sr=target_sr)
            else:
                audio_resampled = audio

            # è½¬æ¢ä¸ºtorch tensor
            audio_tensor = torch.from_numpy(audio_resampled).float()

            # ä½¿ç”¨åˆ†æ®µè‡ªé€‚åº”æ£€æµ‹
            all_speech_timestamps = []

            # æŒ‰å¤æ‚åº¦ç‰‡æ®µè¿›è¡Œåˆ†æ®µæ£€æµ‹
            for segment in complexity_segments:
                # è®¡ç®—å½“å‰ç‰‡æ®µçš„æ ·æœ¬èŒƒå›´
                start_sample = int(segment.start_time * target_sr)
                end_sample = int(min(segment.end_time * target_sr, len(audio_resampled)))

                if end_sample <= start_sample:
                    continue

                segment_audio = audio_tensor[start_sample:end_sample]

                # è·å–å½“å‰ç‰‡æ®µçš„è‡ªé€‚åº”å‚æ•°
                adaptive_params = self.adaptive_enhancer.get_enhanced_adaptive_vad_params(
                    complexity_segments, bpm_features, (segment.start_time + segment.end_time) / 2
                )

                # ä½¿ç”¨è‡ªé€‚åº”å‚æ•°è¿›è¡ŒVADæ£€æµ‹
                segment_timestamps = self.get_speech_timestamps(
                    segment_audio,
                    self.vad_model,
                    sampling_rate=target_sr,
                    threshold=adaptive_params['voice_threshold'],
                    min_speech_duration_ms=adaptive_params['min_speech_duration_ms'],
                    min_silence_duration_ms=adaptive_params['min_silence_duration_ms'],
                    window_size_samples=512,
                    speech_pad_ms=30
                )

                # å°†ç‰‡æ®µæ—¶é—´æˆ³æ˜ å°„å›å…¨å±€æ—¶é—´
                for ts in segment_timestamps:
                    ts['start'] += start_sample
                    ts['end'] += start_sample

                all_speech_timestamps.extend(segment_timestamps)

            # åˆå¹¶é‡å çš„æ—¶é—´æˆ³
            all_speech_timestamps = self._merge_overlapping_timestamps(all_speech_timestamps)

            # æ˜ å°„å›åŸå§‹é‡‡æ ·ç‡ï¼ˆä½¿ç”¨æ­£ç¡®çš„è·¨åŸŸæ˜ å°„ï¼‰
            if self.sample_rate != target_sr:
                from ..utils.audio_processor import map_time_between_domains
                # è·å–é‡é‡‡æ ·å»¶è¿Ÿï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰
                latency_samples = int(get_config('time_mapping.latency_samples', 0))
                
                for ts in all_speech_timestamps:
                    # è½¬æ¢ä¸ºç§’
                    start_sec = ts['start'] / target_sr
                    end_sec = ts['end'] / target_sr
                    
                    # æ˜ å°„åˆ°åŸå§‹é‡‡æ ·ç‡åŸŸ
                    start_sec_mapped = map_time_between_domains(
                        start_sec, target_sr, self.sample_rate, latency_samples
                    )
                    end_sec_mapped = map_time_between_domains(
                        end_sec, target_sr, self.sample_rate, latency_samples
                    )
                    
                    # è½¬æ¢å›æ ·æœ¬
                    ts['start'] = int(start_sec_mapped * self.sample_rate)
                    ts['end'] = int(end_sec_mapped * self.sample_rate)

            logger.info(f"ğŸµ è‡ªé€‚åº”VADæ£€æµ‹å®Œæˆ: {len(all_speech_timestamps)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            return all_speech_timestamps

        except Exception as e:
            logger.error(f"è‡ªé€‚åº”VADæ£€æµ‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°å›ºå®šé˜ˆå€¼æ¨¡å¼")
            return self._detect_speech_timestamps(audio)

    def _merge_overlapping_timestamps(self, timestamps: List[Dict]) -> List[Dict]:
        """åˆå¹¶é‡å çš„æ—¶é—´æˆ³"""
        if not timestamps:
            return []

        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        timestamps = sorted(timestamps, key=lambda x: x['start'])
        merged = [timestamps[0]]

        for current in timestamps[1:]:
            last = merged[-1]

            # å¦‚æœå½“å‰ç‰‡æ®µä¸ä¸Šä¸€ä¸ªç‰‡æ®µé‡å æˆ–ç›¸é‚»ï¼Œåˆ™åˆå¹¶
            if current['start'] <= last['end'] + 1000:  # 1000æ ·æœ¬çš„å®¹å¿åº¦
                last['end'] = max(last['end'], current['end'])
            else:
                merged.append(current)

        return merged

    def _calculate_pause_segments(self, speech_timestamps: List[Dict], audio_length: int) -> List[Dict]:
        """è®¡ç®—åœé¡¿åŒºåŸŸ

        Args:
            speech_timestamps: è¯­éŸ³æ—¶é—´æˆ³
            audio_length: éŸ³é¢‘æ€»é•¿åº¦ï¼ˆæ ·æœ¬æ•°ï¼‰

        Returns:
            åœé¡¿åŒºåŸŸåˆ—è¡¨ [{'start': int, 'end': int}]
        """
        pause_segments = []

        if not speech_timestamps:
            # æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ•´ä¸ªéŸ³é¢‘éƒ½æ˜¯åœé¡¿
            pause_segments.append({
                'start': 0,
                'end': audio_length
            })
            return pause_segments

        # å¤´éƒ¨åœé¡¿ï¼ˆéŸ³é¢‘å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªè¯­éŸ³ç‰‡æ®µï¼‰
        if speech_timestamps[0]['start'] > 0:
            pause_segments.append({
                'start': 0,
                'end': speech_timestamps[0]['start']
            })

        # ä¸­é—´åœé¡¿ï¼ˆè¯­éŸ³ç‰‡æ®µä¹‹é—´ï¼‰
        for i in range(len(speech_timestamps) - 1):
            current_end = speech_timestamps[i]['end']
            next_start = speech_timestamps[i + 1]['start']

            if next_start > current_end:
                pause_segments.append({
                    'start': current_end,
                    'end': next_start
                })

        # å°¾éƒ¨åœé¡¿ï¼ˆæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µåˆ°éŸ³é¢‘ç»“æŸï¼‰
        if speech_timestamps[-1]['end'] < audio_length:
            pause_segments.append({
                'start': speech_timestamps[-1]['end'],
                'end': audio_length
            })

        return pause_segments

    def _filter_valid_pauses(self, pause_segments: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤æœ‰æ•ˆåœé¡¿

        Args:
            pause_segments: åœé¡¿åŒºåŸŸåˆ—è¡¨

        Returns:
            æœ‰æ•ˆåœé¡¿åˆ—è¡¨
        """
        valid_pauses = []
        min_pause_samples = int(self.min_pause_duration * self.sample_rate)

        for pause in pause_segments:
            duration_samples = pause['end'] - pause['start']
            duration_seconds = duration_samples / self.sample_rate

            if duration_samples >= min_pause_samples:
                valid_pauses.append({
                    **pause,
                    'duration': duration_seconds
                })

        logger.debug(f"è¿‡æ»¤åä¿ç•™ {len(valid_pauses)} ä¸ªæœ‰æ•ˆåœé¡¿")
        return valid_pauses

    def _classify_pause_positions(self, pause_segments: List[Dict],
                                speech_timestamps: List[Dict],
                                audio_length: int) -> List[VocalPause]:
        """åˆ†ç±»åœé¡¿ä½ç½®

        Args:
            pause_segments: æœ‰æ•ˆåœé¡¿åˆ—è¡¨
            speech_timestamps: è¯­éŸ³æ—¶é—´æˆ³
            audio_length: éŸ³é¢‘æ€»é•¿åº¦

        Returns:
            åˆ†ç±»åçš„äººå£°åœé¡¿åˆ—è¡¨
        """
        vocal_pauses = []

        for pause in pause_segments:
            start_time = pause['start'] / self.sample_rate
            end_time = pause['end'] / self.sample_rate
            duration = pause['duration']

            # åˆ¤æ–­åœé¡¿ä½ç½®ç±»å‹
            if pause['start'] == 0:
                # å¤´éƒ¨åœé¡¿
                position_type = 'head'
            elif pause['end'] == audio_length:
                # å°¾éƒ¨åœé¡¿
                position_type = 'tail'
            else:
                # ä¸­é—´åœé¡¿
                position_type = 'middle'

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºåœé¡¿æ—¶é•¿ï¼Œè¶Šé•¿ç½®ä¿¡åº¦è¶Šé«˜ï¼‰
            confidence = min(1.0, duration / (self.min_pause_duration * 2))

            vocal_pause = VocalPause(
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                position_type=position_type,
                confidence=confidence,
                cut_point=0.0  # ç¨åè®¡ç®—
            )
            # é€ä¼ åˆæˆåœé¡¿çš„â€œå¼ºåˆ¶ valleyâ€æ ‡è®°ï¼ˆä¸ä¿®æ”¹æ•°æ®ç»“æ„ï¼ŒåŠ¨æ€å±æ€§å³å¯ï¼‰
            try:
                if isinstance(pause, dict) and pause.get('force_valley', False):
                    setattr(vocal_pause, 'force_valley', True)
            except Exception:
                pass

            vocal_pauses.append(vocal_pause)

        return vocal_pauses

    def _compute_rms_envelope(self, waveform: np.ndarray, win_samples: int) -> np.ndarray:
        """è®¡ç®—ç®€æ˜“æ»‘åŠ¨RMSåŒ…ç»œï¼ˆsameå¯¹é½ï¼‰ã€‚"""
        if win_samples <= 1:
            return np.abs(waveform).astype(np.float32)
        kernel = np.ones(int(win_samples), dtype=np.float32) / float(max(1, int(win_samples)))
        return np.sqrt(np.convolve((waveform.astype(np.float32) ** 2), kernel, mode='same'))

    def _future_silence_guard(self, rms: np.ndarray, start_idx: int, guard_samples: int, floor_val: float,
                               allowance: float = 1.2, ratio: float = 0.7) -> bool:
        """æœªæ¥é™é»˜å®ˆå«ï¼šåœ¨ [start_idx, start_idx+guard] å†…å¤šæ•°æ ·æœ¬ä½äº floorÃ—allowanceã€‚"""
        end_idx = min(start_idx + max(0, int(guard_samples)), len(rms))
        if end_idx <= start_idx:
            return False
        window = rms[start_idx:end_idx]
        if window.size == 0:
            return False
        below = np.sum(window <= (floor_val * allowance))
        return (below / float(window.size)) >= ratio

    def _select_valley_cut_point(self, waveform: np.ndarray, left_idx: int, right_idx: int,
                                 sample_rate: int, local_rms_ms: int, guard_ms: int, floor_percentile: float) -> Optional[int]:
        """åœ¨[left_idx, right_idx]å†…é€‰æ‹©RMSè°·å€¼åˆ‡ç‚¹ï¼Œå¸¦æœªæ¥é™é»˜å®ˆå«ï¼›è‹¥å¤±è´¥è¿”å›Noneã€‚"""
        left_idx = max(0, int(left_idx)); right_idx = min(len(waveform), int(right_idx))
        if right_idx - left_idx <= 8:
            return None
        win_samples = max(1, int(local_rms_ms / 1000.0 * sample_rate))
        guard_samples = max(1, int(guard_ms / 1000.0 * sample_rate)) if guard_ms and guard_ms > 0 else 0
        segment = waveform[left_idx:right_idx]
        rms = self._compute_rms_envelope(segment, win_samples)
        floor_val = np.percentile(np.abs(segment), floor_percentile)
        # å€™é€‰æŒ‰RMSå‡åºå°è¯•ï¼Œä¼˜å…ˆæ›´â€œé™â€çš„ç‚¹
        order = np.argsort(rms)
        max_try = min(200, len(order))
        # è¾¹ç•Œä¿æŠ¤ï¼šè‡³å°‘è·å·¦å³è¾¹ç•Œå„20msï¼Œé¿å…è´´è¾¹åˆ‡
        margin_samples = max(1, int(0.02 * sample_rate))
        # è°·å®½/å¡åº¦çº¦æŸå‚æ•°
        try:
            min_valley_ms = int(get_config('vocal_pause_splitting.min_valley_width_ms', 120))
        except Exception:
            min_valley_ms = 120
        valley_half = max(1, int((min_valley_ms / 1000.0) * sample_rate / 2))
        edge_band = max(1, int(0.01 * sample_rate))  # 10ms è¾¹å¸¦ç”¨äºè¯„ä¼°ä¸¤ä¾§ä¸Šå¡
        slope_ratio = 1.15  # ä¸¤ä¾§åº”å½“æ˜æ˜¾é«˜äºè°·åº•

        candidates = []
        for k in range(max_try):
            j = int(order[k])
            if j < margin_samples or j > (len(rms) - margin_samples):
                continue
            # è°·å®½ä¸å¡åº¦çº¦æŸï¼ˆä»…å¯¹ valley è·¯å¾„ï¼‰ï¼šç¡®ä¿ j å‘¨å›´ min_valley_ms èŒƒå›´å†…ä¸ºâ€œçœŸè°·â€
            left_edge = max(0, j - valley_half)
            right_edge = min(len(rms) - 1, j + valley_half)
            if right_edge - left_edge < 3:
                continue
            window = rms[left_edge:right_edge + 1]
            wmin = float(np.min(window))
            if float(rms[j]) > (wmin + 1e-12):
                # j å¿…é¡»æ˜¯è¯¥çª—å£å†…çš„ï¼ˆæˆ–è¿‘ä¼¼ï¼‰æœ€å°ç‚¹
                continue
            # ä¸¤ä¾§ä¸Šå¡ï¼šä¸¤ä¾§è¾¹å¸¦å¹³å‡åº”æ˜¾è‘—é«˜äºè°·åº•
            left_band_vals = rms[left_edge:min(len(rms), left_edge + edge_band)]
            right_band_vals = rms[max(0, right_edge - edge_band):right_edge]
            if left_band_vals.size == 0 or right_band_vals.size == 0:
                continue
            left_mean = float(np.mean(left_band_vals))
            right_mean = float(np.mean(right_band_vals))
            if not (left_mean >= slope_ratio * float(rms[j]) and right_mean >= slope_ratio * float(rms[j])):
                continue

            if guard_samples > 0:
                if self._future_silence_guard(rms, j, guard_samples, floor_val):
                    return left_idx + j
            else:
                # åœ¨ valley å¼ºåˆ¶æ¨¡å¼ä¸‹æš‚ä¸å¯ç”¨æœªæ¥å®ˆå«ï¼Œå…ˆæ”¶é›†å€™é€‰è°·
                candidates.append(j)
        if guard_samples == 0 and len(candidates) > 0:
            # ä½¿ç”¨ç®€å•è°±ç‰¹å¾ä¸ºè°·æ‰“åˆ†ï¼šflatness + 0.3*centroid_norm + 0.3*unvoiced
            # æ°”å£°/æ‘©æ“¦éŸ³ï¼šå¹³å¦åº¦é«˜ã€è´¨å¿ƒåé«˜ã€æ— åŸºéŸ³ï¼ˆè‡ªç›¸å…³å³°ä½ï¼‰
            def _score(j_idx: int) -> float:
                half = max(1, int(0.02 * sample_rate))  # 20ms åŠçª—
                s0 = max(0, j_idx - half)
                s1 = min(len(segment), j_idx + half)
                w = segment[s0:s1]
                if w.size < 8:
                    return -1e9
                win = np.hanning(w.size)
                spec = np.abs(np.fft.rfft(w * win)) + 1e-12
                geo = np.exp(np.mean(np.log(spec)))
                arith = np.mean(spec) + 1e-12
                flat = float(geo / arith)
                freqs = np.fft.rfftfreq(w.size, d=1.0 / float(sample_rate))
                centroid = float(np.sum(freqs * spec) / (np.sum(spec) + 1e-12))
                centroid_norm = centroid / (0.5 * float(sample_rate) + 1e-12)
                # ç®€åŒ– voicingï¼šè‡ªç›¸å…³æ³•ï¼Œæ’é™¤0æ»åï¼Œå–â‰¤20msèŒƒå›´å†…çš„å³°
                w_zm = w - float(np.mean(w))
                ac = np.correlate(w_zm, w_zm, mode='full')
                ac = ac[ac.size // 2:]
                if ac.size > 1 and ac[0] > 0:
                    maxlag = min(len(ac) - 1, int(0.02 * sample_rate))
                    peak = float(np.max(ac[1:maxlag + 1] / (ac[0] + 1e-12))) if maxlag >= 1 else 0.0
                else:
                    peak = 0.0
                unvoiced = 1.0 - max(0.0, min(1.0, peak))
                return float(flat + 0.3 * centroid_norm + 0.3 * unvoiced)
            best_j = max(candidates, key=_score)
            return left_idx + int(best_j)
        # å…œåº•ï¼šé€‰æ‹©æ»¡è¶³è¾¹ç•Œä¿æŠ¤çš„RMSæœ€å°ç‚¹ï¼›è‹¥å‡ä¸æ»¡è¶³åˆ™å¤¹ç´§åˆ°è¾¹ç•Œå†…
        if order.size > 0:
            for jj in order:
                j = int(jj)
                if j >= margin_samples and j <= (len(rms) - margin_samples):
                    return left_idx + j
            j = int(order[0])
            j = max(margin_samples, min(len(rms) - margin_samples, j))
            return left_idx + j
        return None

    def _calculate_cut_points(self, vocal_pauses: List[VocalPause], bpm_features: Optional['BPMFeatures'] = None, waveform: Optional[np.ndarray] = None) -> List[VocalPause]:
            """
            è®¡ç®—ç²¾ç¡®çš„åˆ‡å‰²ç‚¹ä½ç½®ï¼ˆå¼ºåˆ¶èƒ½é‡è°·æ£€æµ‹ï¼‰
            """
            # è¯»å–åˆ‡ç‚¹ç²¾ä¿®é…ç½®
            max_shift_s = float(get_config('vocal_pause_splitting.max_shift_from_silence_center', 0.08))
            backoff_ms = int(get_config('vocal_pause_splitting.boundary_backoff_ms', 180))
            backoff_s = backoff_ms / 1000.0
            local_rms_ms = int(get_config('vocal_pause_splitting.local_rms_window_ms', 25))
            floor_pct = float(get_config('vocal_pause_splitting.silence_floor_percentile', 5))
            guard_ms = int(get_config('vocal_pause_splitting.lookahead_guard_ms', 120))

            logger.info(f"è®¡ç®— {len(vocal_pauses)} ä¸ªåœé¡¿çš„åˆ‡å‰²ç‚¹ (å¼ºåˆ¶èƒ½é‡è°·æ£€æµ‹æ¨¡å¼)...")

            for i, pause in enumerate(vocal_pauses):
                # âœ… --- å…³é”®ä¿®å¤ï¼šæ¢å¤ left å’Œ right å˜é‡çš„å®šä¹‰ ---
                # é»˜è®¤æœç´¢èŒƒå›´æ˜¯æ•´ä¸ªåœé¡¿åŒºåŸŸï¼Œå¹¶å‘å†…æ”¶ç¼©ä¸€ä¸ªè¾¹ç•Œç¼“å†²
                left = pause.start_time + backoff_s
                right = pause.end_time - backoff_s
                # å¦‚æœæ”¶ç¼©åèŒƒå›´æ— æ•ˆï¼Œåˆ™ä½¿ç”¨åŸå§‹åœé¡¿èŒƒå›´
                if right <= left:
                    left, right = pause.start_time, pause.end_time
                # âœ… --- ä¿®å¤ç»“æŸ ---

                selected_idx: Optional[int] = None

                # å…¨é¢é‡‡ç”¨èƒ½é‡è°·æ£€æµ‹é€»è¾‘
                if waveform is not None and len(waveform) > 0:
                    l_idx = max(0, int(left * self.sample_rate))
                    r_idx = min(len(waveform), int(right * self.sample_rate))

                    if r_idx > l_idx:
                        # å¼ºåˆ¶ä½¿ç”¨èƒ½é‡è°·æ£€æµ‹
                        valley_idx = self._select_valley_cut_point(
                            waveform, l_idx, r_idx, self.sample_rate,
                            local_rms_ms, guard_ms, floor_pct
                        )

                        if valley_idx is not None:
                            selected_idx = valley_idx
                            logger.debug(f"åœé¡¿ {i+1}: å¼ºåˆ¶ä½¿ç”¨ valley åˆ‡ç‚¹ idx={selected_idx}")
                        else:
                            # å¦‚æœæ‰¾ä¸åˆ°èƒ½é‡è°·ï¼ˆæå°‘è§ï¼‰ï¼Œå›é€€åˆ°åœé¡¿ä¸­å¿ƒ
                            selected_idx = int((pause.start_time + pause.end_time) / 2 * self.sample_rate)
                            logger.warning(f"åœé¡¿ {i+1}: æœªæ‰¾åˆ°èƒ½é‡è°·ï¼Œå›é€€åˆ°ä¸­å¿ƒç‚¹")
                    else:
                        # å¦‚æœæœç´¢èŒƒå›´æ— æ•ˆï¼Œä¹Ÿå›é€€åˆ°ä¸­å¿ƒç‚¹
                        selected_idx = int((pause.start_time + pause.end_time) / 2 * self.sample_rate)
                else:
                    # å¦‚æœæ²¡æœ‰æ³¢å½¢æ•°æ®ï¼ŒåŒæ ·å›é€€åˆ°ä¸­å¿ƒç‚¹
                    selected_idx = int((pause.start_time + pause.end_time) / 2 * self.sample_rate)

                # å°†æœ€ç»ˆé€‰æ‹©çš„æ ·æœ¬ç´¢å¼•è½¬æ¢ä¸ºæ—¶é—´
                pause.cut_point = selected_idx / self.sample_rate
                logger.info(f"åœé¡¿ {i+1} ({pause.position_type}): {pause.start_time:.2f}s-{pause.end_time:.2f}s â†’ åˆ‡ç‚¹: {pause.cut_point:.2f}s")

            return vocal_pauses
    
    # Removed dead code that was unreachable after return statement
    
    def _filter_adaptive_pauses(self, pause_segments: List[Dict],
                              complexity_segments: List,
                              bpm_features: 'BPMFeatures') -> List[Dict]:
        """åŸºäºBPMç‰¹å¾è‡ªé€‚åº”è¿‡æ»¤åœé¡¿

        Args:
            pause_segments: åœé¡¿åŒºåŸŸåˆ—è¡¨
            bpm_features: BPMåˆ†æç‰¹å¾

        Returns:
            è‡ªé€‚åº”è¿‡æ»¤åçš„åœé¡¿åˆ—è¡¨
        """
        # å…³é”®ä¿®å¤ï¼šå½“æœªå¯ç”¨BPMè‡ªé€‚åº”ã€æœªåˆå§‹åŒ–å¢å¼ºå™¨ï¼Œæˆ–bpmç‰¹å¾ä¸å¯ç”¨æ—¶ï¼Œå›é€€åˆ°å›ºå®šé˜ˆå€¼è¿‡æ»¤
        if (not getattr(self, 'enable_bpm_adaptation', False)) or (not hasattr(self, 'adaptive_enhancer')) or (not self.adaptive_enhancer) or (bpm_features is None):
            return self._filter_valid_pauses(pause_segments)

        valid_pauses = []

        # åŸºäºBPMå’Œä¹å™¨å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æœ€å°åœé¡¿æ—¶é•¿
        if bpm_features.bpm_category == 'slow':
            # æ…¢æ­Œï¼šå…è®¸æ›´çŸ­çš„åœé¡¿ï¼ˆæ­Œæ‰‹æœ‰æ›´å¤šæ—¶é—´æ¢æ°”ï¼‰
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.slow_song_multiplier', 1.5)
            min_pause_duration = max(0.6, self.min_pause_duration * multiplier)
        elif bpm_features.bpm_category == 'fast':
            # å¿«æ­Œï¼šéœ€è¦æ›´é•¿çš„åœé¡¿æ‰è®¤ä¸ºæ˜¯çœŸæ­£çš„åœé¡¿
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.fast_song_multiplier', 0.7)
            min_pause_duration = self.min_pause_duration * multiplier
        else:
            # ä¸­ç­‰é€Ÿåº¦ï¼šä½¿ç”¨å¯é…ç½®çš„æ ‡å‡†ä¹˜æ•°
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.pause_duration_multipliers.medium_song_multiplier', 1.0)
            min_pause_duration = self.min_pause_duration * multiplier

        # ğŸ†• å¤šä¹å™¨ç¯å¢ƒå¢å¼ºï¼šæ ¹æ®ä¹å™¨æ•°é‡å’Œå¤æ‚åº¦è¿›ä¸€æ­¥è°ƒæ•´
        if hasattr(self, 'last_complexity_analysis') and self.last_complexity_analysis:
            complexity = self.last_complexity_analysis.get('total_complexity', 0.0)
            instrument_count = self.last_complexity_analysis.get('instrument_count', 1)

            # ä¹å™¨è¶Šå¤šï¼Œéœ€è¦æ›´é•¿çš„åœé¡¿æ¥ç¡®ä¿æ˜¯çœŸå®çš„äººå£°åœé¡¿
            if instrument_count >= 4:  # 4ç§ä»¥ä¸Šä¹å™¨
                base_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_4_plus_base', 1.4)
                step_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_4_plus_step', 0.1)
                instrument_factor = base_factor + (instrument_count - 4) * step_factor
            elif instrument_count >= 3:  # 3ç§ä¹å™¨
                base_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_3_base', 1.2)
                complexity_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_3_complexity_factor', 0.4)
                instrument_factor = base_factor + (complexity - 0.5) * complexity_factor
            elif instrument_count >= 2:  # 2ç§ä¹å™¨
                base_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_2_base', 1.1)
                complexity_factor = get_config('vocal_pause_splitting.bpm_adaptive_settings.complexity_multipliers.instrument_2_complexity_factor', 0.2)
                instrument_factor = base_factor + (complexity - 0.3) * complexity_factor
            else:
                instrument_factor = 1.0

            min_pause_duration = min_pause_duration * instrument_factor
            logger.info(f"ğŸ¸ å¤šä¹å™¨è°ƒæ•´: {instrument_count}ç§ä¹å™¨, å¤æ‚åº¦{float(complexity):.3f}, ç³»æ•°Ã—{instrument_factor:.2f}")

        # ç¡®ä¿ä¸ä¼šè¿‡åº¦è°ƒæ•´
        min_pause_duration = np.clip(min_pause_duration, 0.5, 3.0)

        min_pause_samples = int(min_pause_duration * self.sample_rate)

        # ğŸ†• ç¬¬ä¸€éï¼šè®¡ç®—æ‰€æœ‰ä¸­é—´åœé¡¿çš„å¹³å‡æ—¶é•¿ï¼ˆæ’é™¤å¤´å°¾åœé¡¿ï¼‰
        middle_pause_durations = []
        # ä»æœ€åä¸€ä¸ªåœé¡¿æ¨æ–­éŸ³é¢‘æ€»é•¿åº¦
        total_audio_length = pause_segments[-1]['end'] if pause_segments else 0

        for i, pause in enumerate(pause_segments):
            duration_samples = pause['end'] - pause['start']
            duration_seconds = duration_samples / self.sample_rate

            # åªç»Ÿè®¡ä¸­é—´åœé¡¿ï¼Œæ’é™¤å¤´å°¾åœé¡¿
            is_head = (i == 0 and pause['start'] == 0)
            is_tail = (i == len(pause_segments) - 1 and pause['end'] >= total_audio_length * 0.95)  # å…è®¸5%çš„è¯¯å·®

            if duration_samples >= min_pause_samples and not is_head and not is_tail:
                middle_pause_durations.append(duration_seconds)

        logger.info(f"ä¸­é—´åœé¡¿ç»Ÿè®¡: æ€»åœé¡¿{len(pause_segments)}ä¸ª, ä¸­é—´åœé¡¿{len(middle_pause_durations)}ä¸ª")

        if not middle_pause_durations:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆè¦æ±‚çš„ä¸­é—´åœé¡¿ï¼Œå›é€€åˆ°æ‰€æœ‰åœé¡¿")
            # å›é€€ç­–ç•¥ï¼šä½¿ç”¨æ‰€æœ‰ç¬¦åˆæœ€å°æ—¶é•¿çš„åœé¡¿
            all_pause_durations = []
            for pause in pause_segments:
                duration_samples = pause['end'] - pause['start']
                duration_seconds = duration_samples / self.sample_rate
                if duration_samples >= min_pause_samples:
                    all_pause_durations.append(duration_seconds)
            if not all_pause_durations:
                return []
            middle_pause_durations = all_pause_durations

        # è®¡ç®—ä¸­é—´åœé¡¿æ—¶é•¿ç»Ÿè®¡
        average_pause_duration = np.mean(middle_pause_durations)
        median_pause_duration = np.median(middle_pause_durations)
        std_pause_duration = np.std(middle_pause_durations)

        logger.info(f"åœé¡¿æ—¶é•¿ç»Ÿè®¡: å¹³å‡={average_pause_duration:.3f}s, ä¸­ä½={median_pause_duration:.3f}s, æ ‡å‡†å·®={std_pause_duration:.3f}s")

        # åŠ¨æ€é˜ˆå€¼ï¼šä½¿ç”¨å¹³å‡å€¼å’Œä¸­ä½æ•°çš„è¾ƒå¤§è€…ä½œä¸ºåŸºå‡†
        duration_threshold = max(average_pause_duration, median_pause_duration)

        # å¯¹äºå˜åŒ–è¾ƒå¤§çš„åœé¡¿åˆ†å¸ƒï¼Œé€‚å½“é™ä½é˜ˆå€¼
        if std_pause_duration > average_pause_duration * 0.5:
            duration_threshold = average_pause_duration * 0.8  # é™ä½20%
            logger.info(f"æ£€æµ‹åˆ°é«˜å˜å¼‚æ€§åœé¡¿åˆ†å¸ƒï¼Œé™ä½é˜ˆå€¼è‡³ {duration_threshold:.3f}s")

        # ğŸ†• ç¬¬äºŒéï¼šåŸºäºå¹³å‡å€¼ç­›é€‰åˆ†å‰²ç‚¹
        valid_pauses = []

        for pause in pause_segments:
            duration_samples = pause['end'] - pause['start']
            duration_seconds = duration_samples / self.sample_rate

            # åŸºç¡€æ—¶é•¿æ£€æŸ¥
            if duration_samples < min_pause_samples:
                continue

            # è¾¹ç•Œåœé¡¿æ”¾å®½ï¼šå¤´/å°¾ä»…éœ€æ»¡è¶³æœ€å°åœé¡¿æ—¶é•¿ï¼›ä¸­é—´åœé¡¿éœ€ â‰¥ åŠ¨æ€é˜ˆå€¼
            is_head = (pause.get('start', 0) == 0)
            is_tail = (pause.get('end', 0) >= total_audio_length * 0.95)

            if is_head or is_tail:
                duration_ratio = duration_seconds / max(average_pause_duration, 1e-6)
                valid_pauses.append({
                    **pause,
                    'duration': duration_seconds,
                    'confidence': 0.75,  # è¾¹ç•Œåœé¡¿åŸºç¡€ç½®ä¿¡åº¦
                    'bpm_aligned': False,
                    'duration_ratio': duration_ratio
                })
                logger.debug(f"è¾¹ç•Œåœé¡¿ä¿ç•™: {duration_seconds:.3f}s (head={is_head}, tail={is_tail})")
                continue

            # ğŸ¯ ä¸­é—´åœé¡¿ï¼šåªé€‰æ‹©æ—¶é•¿â‰¥é˜ˆå€¼çš„åœé¡¿ä½œä¸ºåˆ†å‰²ç‚¹
            if duration_seconds >= duration_threshold:
                # æ ¹æ®èŠ‚æ‹å¼ºåº¦è°ƒæ•´ç½®ä¿¡åº¦
                confidence = 0.8  # åŸºç¡€ç½®ä¿¡åº¦
                beat_duration = 60.0 / bpm_features.main_bpm if (bpm_features and getattr(bpm_features, 'main_bpm', 0) > 0) else 1.0
                if abs(duration_seconds % beat_duration) < 0.1 or abs(duration_seconds % (beat_duration * 2)) < 0.1:
                    confidence += 0.1

                duration_ratio = duration_seconds / max(average_pause_duration, 1e-6)
                if duration_ratio >= 1.5:
                    confidence += 0.1

                valid_pauses.append({
                    **pause,
                    'duration': duration_seconds,
                    'confidence': confidence,
                    'bpm_aligned': abs(duration_seconds % beat_duration) < 0.1,
                    'duration_ratio': duration_ratio
                })
                logger.debug(f"é€‰æ‹©åœé¡¿: {duration_seconds:.3f}s (æ¯”ä¾‹: {duration_ratio:.2f}x)")
            else:
                logger.debug(f"è·³è¿‡çŸ­åœé¡¿: {duration_seconds:.3f}s < {duration_threshold:.3f}s")

        logger.info(f"å¹³å‡å€¼ç­›é€‰å®Œæˆ: {len(middle_pause_durations)}ä¸ªå€™é€‰ â†’ {len(valid_pauses)}ä¸ªåˆ†å‰²ç‚¹ (é˜ˆå€¼: {duration_threshold:.3f}s)")
        return valid_pauses

    def _get_adaptive_offsets(self, bpm_features: 'BPMFeatures') -> Tuple[float, float]:
        """æ ¹æ®BPMè·å–åŠ¨æ€åç§»ä¹˜æ•°

        Args:
            bpm_features: BPMåˆ†æç‰¹å¾

        Returns:
            Tuple[head_offset, tail_offset]: è°ƒæ•´åçš„åç§»å€¼
        """
        if bpm_features.bpm_category == 'slow':
            # æ…¢æ­Œï¼šä½¿ç”¨æ›´é•¿çš„åç§»ï¼Œç»™æ­Œæ‰‹æ›´å¤šçš„åœé¡¿ç¼“å†²æ—¶é—´
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.offset_multipliers.slow_song_offset_multiplier', 1.6)
        elif bpm_features.bpm_category == 'fast':
            # å¿«æ­Œï¼šä½¿ç”¨æ›´çŸ­çš„åç§»ï¼Œä¿æŒç´§å‡‘çš„èŠ‚å¥æ„Ÿ
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.offset_multipliers.fast_song_offset_multiplier', 0.6)
        else:
            # ä¸­é€Ÿæ­Œï¼šä½¿ç”¨æ ‡å‡†åç§»
            multiplier = get_config('vocal_pause_splitting.bpm_adaptive_settings.offset_multipliers.medium_song_offset_multiplier', 1.0)

        adaptive_head_offset = self.head_offset * multiplier
        adaptive_tail_offset = self.tail_offset * multiplier

        logger.debug(f"BPMè‡ªé€‚åº”åç§»: {bpm_features.bpm_category}æ­Œ, ä¹˜æ•°Ã—{multiplier:.1f}, åç§»({adaptive_head_offset:.2f}s, +{adaptive_tail_offset:.2f}s)")

        return adaptive_head_offset, adaptive_tail_offset

    def _optimize_pauses_with_bpm(self, vocal_pauses: List[VocalPause],
                                 bpm_features: 'BPMFeatures') -> List[VocalPause]:
        """ä½¿ç”¨BPMä¿¡æ¯ä¼˜åŒ–åœé¡¿åˆ‡ç‚¹

        Args:
            vocal_pauses: äººå£°åœé¡¿åˆ—è¡¨
            bpm_features: BPMåˆ†æç‰¹å¾

        Returns:
            BPMä¼˜åŒ–åçš„åœé¡¿åˆ—è¡¨
        """
        if not hasattr(self, 'adaptive_enhancer') or not self.adaptive_enhancer:
            return vocal_pauses

        beat_duration = 60.0 / bpm_features.main_bpm if bpm_features.main_bpm > 0 else 1.0

        for pause in vocal_pauses:
            original_cut_point = pause.cut_point

            # å°è¯•å°†åˆ‡ç‚¹å¯¹é½åˆ°æœ€è¿‘çš„èŠ‚æ‹ç‚¹
            if bpm_features.beat_strength > 0.6:  # èŠ‚æ‹è¾ƒå¼ºæ—¶æ‰å¯¹é½
                # æ‰¾åˆ°æœ€è¿‘çš„èŠ‚æ‹ç‚¹
                beat_times = []
                current_beat = 0
                while current_beat < pause.end_time + 2:  # æœç´¢èŒƒå›´æ‰©å±•
                    beat_times.append(current_beat)
                    current_beat += beat_duration

                # æ‰¾åˆ°æœ€æ¥è¿‘å½“å‰åˆ‡ç‚¹çš„èŠ‚æ‹ç‚¹
                if beat_times:
                    closest_beat = min(beat_times, key=lambda x: abs(x - original_cut_point))

                    # å¦‚æœèŠ‚æ‹ç‚¹åœ¨åœé¡¿èŒƒå›´å†…ä¸”è·ç¦»ä¸å¤ªè¿œï¼Œä½¿ç”¨èŠ‚æ‹ç‚¹
                    if (pause.start_time <= closest_beat <= pause.end_time and
                        abs(closest_beat - original_cut_point) < 0.3):
                        pause.cut_point = closest_beat
                        pause.confidence += 0.05  # èŠ‚æ‹å¯¹é½æé«˜ç½®ä¿¡åº¦
                        logger.debug(f"åœé¡¿åˆ‡ç‚¹å¯¹é½åˆ°èŠ‚æ‹: {original_cut_point:.2f}s -> {closest_beat:.2f}s")

        return vocal_pauses

    def generate_pause_report(self, vocal_pauses: List[VocalPause]) -> Dict:
        """ç”Ÿæˆåœé¡¿æ£€æµ‹æŠ¥å‘Š

        Args:
            vocal_pauses: äººå£°åœé¡¿åˆ—è¡¨

        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        if not vocal_pauses:
            return {
                'total_pauses': 0,
                'avg_confidence': 0.0,
                'total_pause_duration': 0.0,
                'pause_types': {'head': 0, 'middle': 0, 'tail': 0}
            }

        # ç»Ÿè®¡åœé¡¿ç±»å‹
        pause_types = {'head': 0, 'middle': 0, 'tail': 0}
        total_duration = 0.0
        total_confidence = 0.0

        for pause in vocal_pauses:
            pause_types[pause.position_type] += 1
            total_duration += pause.duration
            total_confidence += pause.confidence

        return {
            'total_pauses': len(vocal_pauses),
            'avg_confidence': total_confidence / len(vocal_pauses),
            'total_pause_duration': total_duration,
            'pause_types': pause_types
        }