#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyTorch 2.8.0 å®‰è£…éªŒè¯å’ŒCUDAå…¼å®¹æ€§æ£€æŸ¥è„šæœ¬
"""

import sys
import torch
import time

def print_separator(title):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print("=" * 60)
    print(f" {title}")
    print("=" * 60)

def check_pytorch_version():
    """æ£€æŸ¥PyTorchç‰ˆæœ¬ä¿¡æ¯"""
    print_separator("PyTorchç‰ˆæœ¬ä¿¡æ¯")
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"Pythonè·¯å¾„: {sys.executable}")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨è™šæ‹Ÿç¯å¢ƒä¸­
    in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)
    print(f"è™šæ‹Ÿç¯å¢ƒ: {'æ˜¯' if in_venv else 'å¦'}")
    
    if in_venv and 'audio_env' in sys.executable:
        print("âœ… æ­£åœ¨ä½¿ç”¨audio_envè™šæ‹Ÿç¯å¢ƒ")
    else:
        print("âš ï¸  æœªä½¿ç”¨é¢„æœŸçš„è™šæ‹Ÿç¯å¢ƒ")

def check_cuda_support():
    """æ£€æŸ¥CUDAæ”¯æŒ"""
    print_separator("CUDAæ”¯æŒæ£€æŸ¥")
    
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"âœ… CUDAæ”¯æŒå·²å¯ç”¨")
        print(f"PyTorchç¼–è¯‘çš„CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  æ€»å†…å­˜: {props.total_memory / 1024**3:.1f}GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"  å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")
        
        # æ£€æŸ¥æ”¯æŒçš„æ¶æ„
        if hasattr(torch.cuda, 'get_arch_list'):
            archs = torch.cuda.get_arch_list()
            print(f"æ”¯æŒçš„CUDAæ¶æ„: {archs}")
            
            # æ£€æŸ¥RTX 5060 Tiçš„sm_120æ¶æ„
            if 'sm_120' in archs:
                print("âœ… æ”¯æŒRTX 5060 Ti (sm_120æ¶æ„)")
            else:
                print("âš ï¸  å¯èƒ½ä¸å®Œå…¨æ”¯æŒRTX 5060 Ti")
    else:
        print("âŒ CUDAæ”¯æŒæœªå¯ç”¨")
        return False
    
    return True

def test_gpu_computation():
    """æµ‹è¯•GPUè®¡ç®—åŠŸèƒ½"""
    print_separator("GPUè®¡ç®—æµ‹è¯•")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
        return False
    
    try:
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        print("åˆ›å»ºæµ‹è¯•å¼ é‡...")
        device = torch.device('cuda:0')
        
        # æµ‹è¯•çŸ©é˜µè¿ç®—
        size = 1000
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        print(f"æ‰§è¡Œ {size}x{size} çŸ©é˜µä¹˜æ³•...")
        start_time = time.time()
        c = torch.matmul(a, b)
        torch.cuda.synchronize()  # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
        end_time = time.time()
        
        print(f"âœ… GPUçŸ©é˜µè¿ç®—æˆåŠŸ")
        print(f"è®¡ç®—æ—¶é—´: {(end_time - start_time)*1000:.2f}ms")
        print(f"ç»“æœå½¢çŠ¶: {c.shape}")
        print(f"ç»“æœæ•°æ®ç±»å‹: {c.dtype}")
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
        memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
        print(f"GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f}MB (å·²åˆ†é…) / {memory_reserved:.1f}MB (å·²é¢„ç•™)")
        
        # æ¸…ç†å†…å­˜
        del a, b, c
        torch.cuda.empty_cache()
        print("GPUå†…å­˜å·²æ¸…ç†")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False

def check_version_compatibility():
    """æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§"""
    print_separator("ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥")
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬
    pytorch_version = torch.__version__
    if pytorch_version.startswith('2.8.0'):
        print("âœ… PyTorchç‰ˆæœ¬æ­£ç¡®: 2.8.0")
    else:
        print(f"âš ï¸  PyTorchç‰ˆæœ¬ä¸åŒ¹é…: {pytorch_version} (æœŸæœ›: 2.8.0)")
    
    # æ£€æŸ¥CUDAç‰ˆæœ¬
    if torch.cuda.is_available():
        cuda_version = torch.version.cuda
        if cuda_version == '12.9':
            print("âœ… CUDAç‰ˆæœ¬å®Œå…¨åŒ¹é…: 12.9")
        elif cuda_version and cuda_version.startswith('12.'):
            print(f"âœ… CUDAç‰ˆæœ¬å…¼å®¹: {cuda_version} (æ”¯æŒ12.9)")
        else:
            print(f"âš ï¸  CUDAç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹: {cuda_version}")
    
    # æ£€æŸ¥cuDNN
    if torch.backends.cudnn.is_available():
        print("âœ… cuDNNå¯ç”¨")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    else:
        print("âš ï¸  cuDNNä¸å¯ç”¨")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” PyTorch 2.8.0 å®‰è£…éªŒè¯å’ŒCUDAå…¼å®¹æ€§æ£€æŸ¥")
    print(f"æ£€æŸ¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. æ£€æŸ¥PyTorchç‰ˆæœ¬
    check_pytorch_version()
    print()
    
    # 2. æ£€æŸ¥CUDAæ”¯æŒ
    cuda_available = check_cuda_support()
    print()
    
    # 3. æµ‹è¯•GPUè®¡ç®—
    if cuda_available:
        gpu_test_passed = test_gpu_computation()
        print()
    else:
        gpu_test_passed = False
    
    # 4. æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
    check_version_compatibility()
    print()
    
    # 5. æ€»ç»“
    print_separator("æ€»ç»“")
    
    if torch.__version__.startswith('2.8.0') and cuda_available and gpu_test_passed:
        print("ğŸ‰ å®Œç¾ï¼PyTorch 2.8.0+cu129å®‰è£…æˆåŠŸï¼ŒGPUåŠ é€Ÿå®Œå…¨å¯ç”¨")
        print("âœ… ç‰ˆæœ¬å…¼å®¹æ€§: å®Œå…¨å…¼å®¹")
        print("âœ… GPUæ”¯æŒ: å®Œå…¨æ”¯æŒ")
        print("âœ… è®¡ç®—æµ‹è¯•: å…¨éƒ¨é€šè¿‡")
        print()
        print("ğŸš€ å¯ä»¥å¼€å§‹ä½¿ç”¨GPUåŠ é€Ÿçš„æ·±åº¦å­¦ä¹ åŠŸèƒ½ï¼")
    else:
        print("âš ï¸  å®‰è£…å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä»¥ä¸Šè¾“å‡º")
        
        if not torch.__version__.startswith('2.8.0'):
            print("- PyTorchç‰ˆæœ¬ä¸æ­£ç¡®")
        if not cuda_available:
            print("- CUDAæ”¯æŒä¸å¯ç”¨")
        if not gpu_test_passed:
            print("- GPUè®¡ç®—æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()
